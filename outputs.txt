Epoch: 0
Negative Examples
[]

Positive Examples
['aaaa', 'aaaab', 'aaab', 'aaabaab', 'aabaabb', 'aabab', 'abaab', 'abaabaa', 'abab', 'ababaab', 'baaa', 'baaaaa', 'bbaabaab', 'bbbaa']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaa', 'aabaabb', 'abaabaa', 'baaa', 'baaaaa', 'bbaabaab', 'bbbaa']
Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7016876935958862
Train Epoch 1, Loss 0.628387987613678
Train Epoch 2, Loss 0.5597656965255737
Accuracy at epoch 0: 0.53125, total training samples: 7
Epoch: 1
Negative Examples
['aaaaaaab', 'aabaaaab', 'aabab', 'aababaaa', 'abaab', 'abaabab', 'abab', 'baaaaaa', 'baaaaaba', 'baaabaaa', 'baaabb', 'bbab', 'bbbb', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaaaaab', 'aabaaaab', 'aabab', 'abaab', 'abaabab', 'abab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.9339044094085693
Train Epoch 1, Loss 0.8285796046257019
Train Epoch 2, Loss 0.7437766194343567
Accuracy at epoch 1: 0.908203125, total training samples: 13
Epoch: 2
Negative Examples
['aaaaaabb', 'aaabb', 'abbbbaa', 'babaaba', 'bbaabb', 'bbbabb', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaab', 'aaabaab', 'aaabab', 'aabaaaab', 'aabab', 'abaab', 'ababab', 'abbbbaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbbbaab']
Neg
Train Epoch 0, Loss 0.7071191668510437
Train Epoch 1, Loss 0.6380107402801514
Train Epoch 2, Loss 0.5723179578781128
Accuracy at epoch 2: 0.455078125, total training samples: 14
Epoch: 3
Negative Examples
['aaaaaaab', 'aaaab', 'aaab', 'aaabaab', 'aaababab', 'aabab', 'abaaaaab', 'abab', 'ababaab', 'abbba', 'abbbab', 'baaaaaa', 'bbab', 'bbabbaaa', 'bbbaabab', 'bbbbbaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaaaaab', 'aaaab', 'aaab', 'aaabaab', 'aaababab', 'aabab', 'abaaaaab', 'abab', 'ababaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.9068495631217957
Train Epoch 1, Loss 0.8157519698143005
Train Epoch 2, Loss 0.7400923371315002
Accuracy at epoch 3: 0.833984375, total training samples: 23
Epoch: 4
Negative Examples
['aaaa', 'abaab', 'abbbbabb', 'baaaaa', 'babbaabb', 'bbababb', 'bbabbabb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aaab', 'aaababab', 'aababab', 'ababab', 'ababbb', 'bbaaaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'ababbb', 'bbaaaab']
Pos Neg Neg
Train Epoch 0, Loss 0.7010695338249207
Train Epoch 1, Loss 0.6883342862129211
Train Epoch 2, Loss 0.6769952774047852
Accuracy at epoch 4: 0.732421875, total training samples: 26
Epoch: 5
Negative Examples
['aaaabab', 'aaababab', 'abaaaa', 'abaaab', 'ababab', 'abbaba', 'abbb', 'baabba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaab', 'abaab', 'abaabab', 'ababaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaabab', 'aaababab', 'abaaab', 'ababab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.7034010887145996
Train Epoch 1, Loss 0.6346859335899353
Train Epoch 2, Loss 0.5706691145896912
Accuracy at epoch 5: 0.482421875, total training samples: 30
Epoch: 6
Negative Examples
[]

Positive Examples
['aaaab', 'aaaabaab', 'aaab', 'aabaab', 'aabaaba', 'aabab', 'aababab', 'abaabaab', 'abab', 'ababaab', 'abbabaa', 'abbabba', 'bbaba', 'bbbabbb', 'bbbba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabaaba', 'abbabaa', 'abbabba', 'bbaba', 'bbbabbb', 'bbbba']
Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7859218716621399
Train Epoch 1, Loss 0.7145410180091858
Train Epoch 2, Loss 0.6536234617233276
Accuracy at epoch 6: 0.91015625, total training samples: 36
Epoch: 7
Negative Examples
['aaaabbaa', 'aabba', 'abbaaaaa', 'baaba', 'babb']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aabaab', 'abaab', 'abaabab', 'abab', 'ababab', 'abababab', 'baabab', 'babbbbab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baabab', 'babbbbab']
Neg Neg
Train Epoch 0, Loss 0.7726746797561646
Train Epoch 1, Loss 0.6983456015586853
Train Epoch 2, Loss 0.6299545764923096
Accuracy at epoch 7: 0.4765625, total training samples: 38
Epoch: 8
Negative Examples
['aaaaab', 'aaaaabab', 'aaababab', 'aabaab', 'aabab', 'aabbbba', 'abaaaab', 'abbaab', 'abbbbbab', 'baaab', 'baabba', 'baabbb', 'babaab', 'bbaabba', 'bbbaaabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaaab', 'aaaaabab', 'aaababab', 'aabaab', 'aabab', 'abaaaab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.821094810962677
Train Epoch 1, Loss 0.7463703155517578
Train Epoch 2, Loss 0.6827252507209778
Accuracy at epoch 8: 0.900390625, total training samples: 44
Epoch: 9
Negative Examples
['aaaaa', 'aaabaa', 'abaaa', 'abaabb', 'abbabb', 'baaa', 'babababa', 'bbaba', 'bbabaaba', 'bbbabaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aababab', 'abaaaab', 'abaabab', 'bbabaab', 'bbbabab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['bbabaab', 'bbbabab']
Neg Neg
Train Epoch 0, Loss 0.7645596861839294
Train Epoch 1, Loss 0.702070951461792
Train Epoch 2, Loss 0.6426384449005127
Accuracy at epoch 9: 0.515625, total training samples: 46
Epoch: 10
Negative Examples
['aaaab', 'aaaabab', 'aaaabbaa', 'aaabab', 'aaababab', 'aabab', 'aababab', 'abaabab', 'ababaab', 'abababab', 'ababb', 'abbaba', 'abbbaa', 'baabb', 'bbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaab', 'aaaabab', 'aaabab', 'aaababab', 'aabab', 'aababab', 'abaabab', 'ababaab', 'abababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.8193339109420776
Train Epoch 1, Loss 0.7487401962280273
Train Epoch 2, Loss 0.6868342757225037
Accuracy at epoch 10: 0.875, total training samples: 55
Epoch: 11
Negative Examples
['abaaa', 'ababa', 'baaa', 'baabaaaa', 'baba', 'babbbaa', 'bbbabbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'abaabaab', 'abaabab', 'abab', 'ababaaab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 11: No counterexamples found, skipped.
Accuracy at epoch 11: 0.927734375, total training samples: 55
Epoch: 12
Negative Examples
['aaaababb', 'aabbbaa', 'aabbbb', 'abaaaaaa', 'abbba', 'baaa', 'baaaaaa', 'bbbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aabaaab', 'aabab', 'abaaab', 'abaab', 'abaabab', 'abbbaab', 'bbaaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbbaab', 'bbaaab']
Neg Neg
Train Epoch 0, Loss 0.7453540563583374
Train Epoch 1, Loss 0.6894640922546387
Train Epoch 2, Loss 0.635723352432251
Accuracy at epoch 12: 0.515625, total training samples: 57
Epoch: 13
Negative Examples
['aaaab', 'aaabaab', 'aaabbb', 'aaabbbab', 'aabaab', 'aabab', 'abaaaab', 'abaaab', 'abaab', 'abaabab', 'ababaab', 'baaaab', 'baabb', 'bbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaab', 'aaabaab', 'aabaab', 'aabab', 'abaaaab', 'abaaab', 'abaab', 'abaabab', 'ababaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7980186343193054
Train Epoch 1, Loss 0.73482346534729
Train Epoch 2, Loss 0.6795719861984253
Accuracy at epoch 13: 0.892578125, total training samples: 66
Epoch: 14
Negative Examples
['aaaaaa', 'aaabbba', 'aaabbbba', 'abaa', 'abba']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aabab', 'aababab', 'abaaab', 'abaab', 'abaabaab', 'abab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 14: No counterexamples found, skipped.
Accuracy at epoch 14: 0.892578125, total training samples: 66
Epoch: 15
Negative Examples
['aaabb', 'aabb', 'abaabb', 'abbaaaa', 'babba', 'bbaabaaa', 'bbbbba', 'bbbbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaabaab', 'aaab', 'abaaabab', 'abaabab', 'bbbaabab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['bbbaabab']
Neg
Train Epoch 0, Loss 0.7682191133499146
Train Epoch 1, Loss 0.7042550444602966
Train Epoch 2, Loss 0.6454322934150696
Accuracy at epoch 15: 0.4765625, total training samples: 67
Epoch: 16
Negative Examples
['aaaab', 'aaab', 'aaababab', 'aaabbb', 'aabbb', 'abaaab', 'abaaabab', 'abaabbab', 'abab', 'abbb', 'baaabbab', 'babbba', 'bbaabbaa', 'bbbb', 'bbbbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaab', 'aaab', 'aaababab', 'abaaab', 'abaaabab', 'abab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.789927065372467
Train Epoch 1, Loss 0.7300974726676941
Train Epoch 2, Loss 0.6766588091850281
Accuracy at epoch 16: 0.837890625, total training samples: 73
Epoch: 17
Negative Examples
['ababba', 'baaba', 'bbbabb', 'bbbbabaa']
Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabaaab', 'aabaabab', 'aabb', 'aabbb', 'abab', 'ababab', 'baaabb', 'babb', 'babbbbab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabb', 'aabbb', 'baaabb', 'babb', 'babbbbab']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7084051966667175
Train Epoch 1, Loss 0.6604079008102417
Train Epoch 2, Loss 0.6143527626991272
Accuracy at epoch 17: 0.490234375, total training samples: 78
Epoch: 18
Negative Examples
['aaaaabab', 'aaaaba', 'aaaabab', 'aabaabb', 'aabab', 'aabba', 'abaa', 'abaaaa', 'abaaabb', 'abaab', 'ababab', 'abbabb', 'baabba', 'bababab', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaaabab', 'aaaabab', 'aabab', 'abaab', 'ababab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7456233501434326
Train Epoch 1, Loss 0.6830839514732361
Train Epoch 2, Loss 0.6263416409492493
Accuracy at epoch 18: 0.84765625, total training samples: 83
Epoch: 19
Negative Examples
['abba', 'baaa', 'baaabbbb', 'bbaa', 'bbaaaabb']
Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aaabaa', 'aabaaab', 'aabab', 'abaab', 'abab', 'abababaa', 'ababbb']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabaa', 'abababaa', 'ababbb']
Neg Neg Neg
Train Epoch 0, Loss 0.7233179211616516
Train Epoch 1, Loss 0.6618869304656982
Train Epoch 2, Loss 0.6091749668121338
Accuracy at epoch 19: 0.5390625, total training samples: 86
Epoch: 20
Negative Examples
['aaaaaab', 'aaab', 'aaabaaab', 'aaababbb', 'aabaab', 'abaaa', 'abaaab', 'abaaba', 'abababaa', 'abbaaab', 'bbbaab', 'bbbb', 'bbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abaab', 'abaabaab']
Pos Pos
Counterexamples
['aaaaaab', 'aaab', 'aaabaaab', 'aabaab', 'abaaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7212966084480286
Train Epoch 1, Loss 0.6668127179145813
Train Epoch 2, Loss 0.6162060499191284
Accuracy at epoch 20: 0.853515625, total training samples: 91
Epoch: 21
Negative Examples
['aabbbaa', 'abbbbb', 'bbabbba']
Neg Neg Neg
Positive Examples
['aaaaabab', 'aaaab', 'aaaabaab', 'aaab', 'aabab', 'abaab', 'abab', 'bababb', 'bbaaaabb', 'bbabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['bababb', 'bbaaaabb', 'bbabab']
Neg Neg Neg
Train Epoch 0, Loss 0.7232668995857239
Train Epoch 1, Loss 0.6758143305778503
Train Epoch 2, Loss 0.6296406388282776
Accuracy at epoch 21: 0.58203125, total training samples: 94
Epoch: 22
Negative Examples
['aaaaa', 'aaaab', 'aaab', 'aaabab', 'abaabab', 'ababaab', 'abbbaa', 'baaa', 'baaaaabb', 'bbaaa', 'bbabbbbb', 'bbbaaaab', 'bbbba', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abaaaaab']
Pos
Counterexamples
['aaaab', 'aaab', 'aaabab', 'abaabab', 'ababaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7080866694450378
Train Epoch 1, Loss 0.6513091921806335
Train Epoch 2, Loss 0.6002815961837769
Accuracy at epoch 22: 0.8515625, total training samples: 99
Epoch: 23
Negative Examples
['aaaabb', 'aaaba', 'aabbb', 'baabaa', 'babbabaa', 'bbbabba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaab', 'aabaaaab', 'aabaaab', 'abaabaab', 'abaabab', 'ababaab', 'abbabbab', 'bbbabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbabbab', 'bbbabab']
Neg Neg
Train Epoch 0, Loss 0.8784736394882202
Train Epoch 1, Loss 0.794974684715271
Train Epoch 2, Loss 0.7214219570159912
Accuracy at epoch 23: 0.759765625, total training samples: 101
Epoch: 24
Negative Examples
['aaaa', 'aaaabbb', 'aaaba', 'aaabaaa', 'aabbaaa', 'aabbb', 'abaabab', 'abaabbba', 'abab', 'abbbbb', 'baba', 'bbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'abaaaab', 'abaaab', 'baab']
Pos Pos Pos Pos
Counterexamples
['abaabab', 'abab', 'baab']
Pos Pos Neg
Train Epoch 0, Loss 0.733020007610321
Train Epoch 1, Loss 0.7054042220115662
Train Epoch 2, Loss 0.6806797981262207
Accuracy at epoch 24: 0.798828125, total training samples: 104
Epoch: 25
Negative Examples
['aaab', 'aaba', 'aabbbb', 'abaabaab', 'ababaab', 'abbaaaa', 'baaaab', 'baabb', 'bbbaab', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaabab', 'aabaaab', 'abaaaaab', 'bbbab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aaab', 'abaabaab', 'ababaab', 'bbbab']
Pos Pos Pos Neg
Train Epoch 0, Loss 0.7123326063156128
Train Epoch 1, Loss 0.6866781711578369
Train Epoch 2, Loss 0.6626797914505005
Accuracy at epoch 25: 0.87109375, total training samples: 108
Epoch: 26
Negative Examples
['abaabb', 'abab', 'ababa', 'ababab', 'abbba', 'bbba', 'bbbbaba', 'bbbbbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabab', 'aababaab', 'abaab', 'abaabaab', 'abaabab', 'abbbaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abab', 'ababab', 'abbbaaab']
Pos Pos Neg
Train Epoch 0, Loss 0.717417299747467
Train Epoch 1, Loss 0.6933846473693848
Train Epoch 2, Loss 0.6717238426208496
Accuracy at epoch 26: 0.884765625, total training samples: 111
Epoch: 27
Negative Examples
['aaaba', 'aaabaaaa', 'aabaa', 'baaa', 'babba', 'babbb', 'bbababa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaabaab', 'aaab', 'aabaaaab', 'aabaabab', 'abab', 'baaab', 'babab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaab', 'babab']
Neg Neg
Train Epoch 0, Loss 0.7111213207244873
Train Epoch 1, Loss 0.6404980421066284
Train Epoch 2, Loss 0.573012113571167
Accuracy at epoch 27: 0.521484375, total training samples: 113
Epoch: 28
Negative Examples
['aaaa', 'aaaaaaab', 'aaaaab', 'aaab', 'aabab', 'aabba', 'aabbab', 'abaab', 'abbba', 'bababbb', 'babbaaaa', 'babbaaab', 'babbbbbb', 'bbbaabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaaaaab', 'aaaaab', 'aaab', 'aabab', 'abaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.8513928651809692
Train Epoch 1, Loss 0.7661200761795044
Train Epoch 2, Loss 0.6986052989959717
Accuracy at epoch 28: 0.91796875, total training samples: 118
Epoch: 29
Negative Examples
['ababab', 'abba', 'abbaa', 'baababba', 'bbaa', 'bbaabba', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaaabab', 'aaaabaab', 'aaab', 'aabaab', 'abaab', 'abab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababab', 'bbab']
Pos Neg
Train Epoch 0, Loss 0.694595992565155
Train Epoch 1, Loss 0.6711286902427673
Train Epoch 2, Loss 0.6455656290054321
Accuracy at epoch 29: 0.841796875, total training samples: 120
Epoch: 30
Negative Examples
['aaabbaba', 'abbb', 'baab', 'baabba', 'babb', 'babbbabb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaaabab', 'aaabab', 'aabaaab', 'abaaaab', 'abaaab', 'abab', 'abbaaab', 'babab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbaaab', 'babab']
Neg Neg
Train Epoch 0, Loss 0.7881452441215515
Train Epoch 1, Loss 0.69071364402771
Train Epoch 2, Loss 0.6110318899154663
Accuracy at epoch 30: 0.537109375, total training samples: 122
Epoch: 31
Negative Examples
['aaabaaab', 'aaabaab', 'aaabab', 'aaabbaa', 'abaaab', 'abab', 'abababab', 'baabab', 'baba', 'babbbaab', 'bbaaaa', 'bbaab', 'bbab', 'bbabb', 'bbbaabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaabaaab', 'aaabaab', 'aaabab', 'abaaab', 'abab', 'abababab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7883594632148743
Train Epoch 1, Loss 0.7036094069480896
Train Epoch 2, Loss 0.6300241947174072
Accuracy at epoch 31: 0.845703125, total training samples: 128
Epoch: 32
Negative Examples
['aababbaa', 'ababba', 'abba', 'bbaabba', 'bbbaaaa']
Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabab', 'aabaaaab', 'aabaab', 'abaaaaab', 'abab', 'bbaaaabb']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['bbaaaabb']
Neg
Train Epoch 0, Loss 0.7759878039360046
Train Epoch 1, Loss 0.6726750135421753
Train Epoch 2, Loss 0.5928770303726196
Accuracy at epoch 32: 0.689453125, total training samples: 129
Epoch: 33
Negative Examples
['aaaaab', 'aaab', 'aababaab', 'abaab', 'abba', 'baaaa', 'baabbaa', 'baabbaaa', 'babaaa', 'bbaaa', 'bbaabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaabab', 'aabab', 'abab']
Pos Pos Pos Pos
Counterexamples
['aaaaab', 'aaab', 'aababaab', 'abaab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.7535911798477173
Train Epoch 1, Loss 0.6918349266052246
Train Epoch 2, Loss 0.6341927647590637
Accuracy at epoch 33: 0.857421875, total training samples: 133
Epoch: 34
Negative Examples
['abaaa', 'abaaba', 'ababaa', 'baaaa', 'baaabba', 'babbbbb', 'bbaabbbb', 'bbbbaab', 'bbbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aabaaaab', 'aabaab', 'abaaaab', 'ababab', 'baaaaab', 'baaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaaaab', 'baaab']
Neg Neg
Train Epoch 0, Loss 0.8159134984016418
Train Epoch 1, Loss 0.7449309825897217
Train Epoch 2, Loss 0.6871457695960999
Accuracy at epoch 34: 0.626953125, total training samples: 135
Epoch: 35
Negative Examples
['aaaa', 'aaaaab', 'aaab', 'aabb', 'abaab', 'ababab', 'abababab', 'baaba', 'baabbaab', 'babbbbbb', 'bbabb', 'bbbbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabab', 'aabab', 'abab', 'bbab']
Pos Pos Pos Pos
Counterexamples
['aaaaab', 'aaab', 'abaab', 'ababab', 'abababab', 'bbab']
Pos Pos Pos Pos Pos Neg
Train Epoch 0, Loss 0.7094367146492004
Train Epoch 1, Loss 0.6875832080841064
Train Epoch 2, Loss 0.6650462746620178
Accuracy at epoch 35: 0.892578125, total training samples: 141
Epoch: 36
Negative Examples
['ababaab', 'abbbaab', 'baaa', 'bbaaa', 'bbbab', 'bbbbbaab', 'bbbbbbbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabaaab', 'aaabaab', 'aabaab', 'aabaabab', 'aabab', 'abaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababaab']
Pos
Train Epoch 0, Loss 0.6932089328765869
Train Epoch 1, Loss 0.6084917783737183
Train Epoch 2, Loss 0.5294946432113647
Accuracy at epoch 36: 0.623046875, total training samples: 142
Epoch: 37
Negative Examples
['bbbbbabb']
Neg
Positive Examples
['aaaab', 'aabaaaa', 'aabaaab', 'aabab', 'aababbb', 'abaaaab', 'abaaabab', 'abaabaab', 'abab', 'abbbaaba', 'baaaab', 'baab', 'baabbab', 'babbbbab', 'bbaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabaaaa', 'aababbb', 'abbbaaba', 'baaaab', 'baab', 'baabbab', 'babbbbab', 'bbaabab']
Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.8811193108558655
Train Epoch 1, Loss 0.7795946598052979
Train Epoch 2, Loss 0.701486349105835
Accuracy at epoch 37: 0.87109375, total training samples: 150
Epoch: 38
Negative Examples
['aabaaba', 'abaa', 'abab', 'ababa', 'ababab', 'abbbb', 'baababa', 'baabbb', 'babbaab', 'bbbabbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'abaaaaab', 'abaaab', 'ababaaab']
Pos Pos Pos Pos Pos
Counterexamples
['abab', 'ababab']
Pos Pos
Train Epoch 0, Loss 0.7071605920791626
Train Epoch 1, Loss 0.6425259709358215
Train Epoch 2, Loss 0.5805709958076477
Accuracy at epoch 38: 0.67578125, total training samples: 152
Epoch: 39
Negative Examples
['aaba', 'baabbbbb', 'bbbaaaba', 'bbbbaabb']
Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabab', 'aabbabb', 'abaaaab', 'abaaab', 'abaab', 'abaabaab', 'abab', 'ababaab', 'abbaaa', 'baabbbb', 'bbbabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbabb', 'abbaaa', 'baabbbb', 'bbbabab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7581899166107178
Train Epoch 1, Loss 0.687915027141571
Train Epoch 2, Loss 0.6320258378982544
Accuracy at epoch 39: 0.955078125, total training samples: 156
Epoch: 40
Negative Examples
['abaabb', 'abbaab', 'babb', 'bbaab', 'bbabaa', 'bbabbba', 'bbbbaaba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaabaab', 'aaabab', 'aabaaab', 'aabab', 'abaaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 40: No counterexamples found, skipped.
Accuracy at epoch 40: 0.96875, total training samples: 156
Epoch: 41
Negative Examples
['aaaaaba', 'abaa', 'bbaa', 'bbaaaaaa', 'bbbab']
Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'aaab', 'aaabaaab', 'aaabab', 'aabaaaab', 'aabab', 'abaaabab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 41: No counterexamples found, skipped.
Accuracy at epoch 41: 0.96484375, total training samples: 156
Epoch: 42
Negative Examples
['aaaa', 'aaaabbbb', 'aaababa', 'baaaba', 'babaab', 'bababaa', 'babbaaaa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaa', 'aaaabab', 'aaab', 'aaabab', 'aabab', 'abaaaab', 'abaab', 'abab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaaaa']
Neg
Train Epoch 0, Loss 0.7221079468727112
Train Epoch 1, Loss 0.6516740918159485
Train Epoch 2, Loss 0.5978947281837463
Accuracy at epoch 42: 0.619140625, total training samples: 157
Epoch: 43
Negative Examples
['aaaab', 'aaabaab', 'aabaaab', 'aabb', 'abaaaab', 'abaaab', 'abaababa', 'abbb', 'abbba', 'baaaab', 'bababbbb', 'bbaabaa', 'bbabbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aabab']
Pos Pos
Counterexamples
['aaaab', 'aaabaab', 'aabaaab', 'abaaaab', 'abaaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7296711206436157
Train Epoch 1, Loss 0.6791232824325562
Train Epoch 2, Loss 0.6315097212791443
Accuracy at epoch 43: 0.861328125, total training samples: 162
Epoch: 44
Negative Examples
['aaaa', 'aaababa', 'aaabbbaa', 'babaaa']
Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaaab', 'aaaabaab', 'aaab', 'aabaabab', 'aabab', 'abaab', 'abaabab', 'ababaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 44: No counterexamples found, skipped.
Accuracy at epoch 44: 0.861328125, total training samples: 162
Epoch: 45
Negative Examples
['aaabb', 'aaba', 'aabbaa', 'bbbabaa']
Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaab', 'aaabaab', 'aabaaab', 'aabab', 'abaaaab', 'ababbbbb', 'baabab', 'bbbbbab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababbbbb', 'baabab', 'bbbbbab']
Neg Neg Neg
Train Epoch 0, Loss 0.8055499196052551
Train Epoch 1, Loss 0.7230284810066223
Train Epoch 2, Loss 0.6551921963691711
Accuracy at epoch 45: 0.80859375, total training samples: 165
Epoch: 46
Negative Examples
['aaaaaaa', 'aaabbbb', 'aabb', 'abaabbbb', 'abab', 'abbaa', 'abbb', 'bababba', 'babba', 'bbaaaa', 'bbabbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabaab', 'aaabab', 'abaaaab', 'abaab']
Pos Pos Pos Pos
Counterexamples
['abab']
Pos
Train Epoch 0, Loss 0.6974322199821472
Train Epoch 1, Loss 0.6294823288917542
Train Epoch 2, Loss 0.5668028593063354
Accuracy at epoch 46: 0.8515625, total training samples: 166
Epoch: 47
Negative Examples
['aaaaaabb', 'aaaba', 'ababb', 'baaa', 'baaabb', 'bbabba', 'bbbbaa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abaaabab', 'abaab', 'abab', 'ababaaab', 'ababab', 'abababab', 'abbabbab', 'babbb']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbabbab', 'babbb']
Neg Neg
Train Epoch 0, Loss 0.7416602373123169
Train Epoch 1, Loss 0.6713496446609497
Train Epoch 2, Loss 0.602881908416748
Accuracy at epoch 47: 0.7265625, total training samples: 168
Epoch: 48
Negative Examples
['aaaabaab', 'aabab', 'abaaa', 'abaab', 'ababa', 'ababaaab', 'ababab', 'abababab', 'abbaaabb', 'abbb', 'babaaab', 'babbabb', 'bbaa', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaabaab', 'aabab', 'abaab', 'ababaaab', 'ababab', 'abababab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7348745465278625
Train Epoch 1, Loss 0.6596872210502625
Train Epoch 2, Loss 0.6009359955787659
Accuracy at epoch 48: 0.880859375, total training samples: 174
Epoch: 49
Negative Examples
['aaaaa', 'aaaabba', 'babbabba']
Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaabaaab', 'aaabaab', 'aabaaaab', 'aabaabab', 'abaaabab', 'abab', 'ababab', 'ababbab', 'abbbab', 'bbaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababbab', 'abbbab', 'bbaab']
Neg Neg Neg
Train Epoch 0, Loss 0.7920845150947571
Train Epoch 1, Loss 0.7196304798126221
Train Epoch 2, Loss 0.6487337350845337
Accuracy at epoch 49: 0.822265625, total training samples: 177
Epoch: 50
Negative Examples
['aaabaab', 'aababab', 'abaab', 'ababab', 'abba', 'abbabb', 'baab', 'babaabb', 'bbabbbaa', 'bbbbaaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaabab', 'aabab', 'abaaaab']
Pos Pos Pos Pos
Counterexamples
['aaabaab', 'aababab', 'abaab', 'ababab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.7271044254302979
Train Epoch 1, Loss 0.6544791460037231
Train Epoch 2, Loss 0.596904993057251
Accuracy at epoch 50: 0.8515625, total training samples: 181
Epoch: 51
Negative Examples
['abbaaa', 'babbabba', 'babbb', 'bbaaba', 'bbbaabaa', 'bbbb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaaabb', 'aaab', 'aaabaaab', 'aabaaaa', 'aabaaaab', 'aabbbab', 'abaaabab', 'abaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaabb', 'aabaaaa', 'aabbbab']
Neg Neg Neg
Train Epoch 0, Loss 0.7512151598930359
Train Epoch 1, Loss 0.6960265636444092
Train Epoch 2, Loss 0.6494154930114746
Accuracy at epoch 51: 0.853515625, total training samples: 184
Epoch: 52
Negative Examples
['abaaa', 'abaab', 'abaabaab', 'ababaab', 'abbbab', 'babaaba', 'bbaa', 'bbababb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aabaaaab', 'aababab', 'abaaab', 'ababaaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'abaabaab', 'ababaab']
Pos Pos Pos
Train Epoch 0, Loss 0.7083210945129395
Train Epoch 1, Loss 0.6456907391548157
Train Epoch 2, Loss 0.5873157382011414
Accuracy at epoch 52: 0.9140625, total training samples: 187
Epoch: 53
Negative Examples
['ababaaba', 'baabb', 'babbbaa', 'babbbba', 'bbba']
Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabab', 'aababab', 'abab', 'ababab', 'abbaabab', 'babb', 'bbbabaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbaabab', 'babb', 'bbbabaab']
Neg Neg Neg
Train Epoch 0, Loss 0.7938695549964905
Train Epoch 1, Loss 0.7365714907646179
Train Epoch 2, Loss 0.6830812096595764
Accuracy at epoch 53: 0.876953125, total training samples: 190
Epoch: 54
Negative Examples
['aaaababb', 'aababba', 'aabbbb', 'abaa', 'ababb', 'abba', 'abbab', 'abbbaba', 'baababab', 'bbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'aabaab', 'abaaaaab', 'abab']
Pos Pos Pos Pos Pos
Counterexamples
[]

Round 54: No counterexamples found, skipped.
Accuracy at epoch 54: 0.880859375, total training samples: 190
Epoch: 55
Negative Examples
['aabaabab', 'aabbabb', 'aabbbbbb', 'abaaaa', 'abbba', 'baabbabb', 'bbaab', 'bbaabbaa', 'bbba', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aabab']
Pos Pos Pos
Counterexamples
['aabaabab']
Pos
Train Epoch 0, Loss 0.7084131836891174
Train Epoch 1, Loss 0.6390976905822754
Train Epoch 2, Loss 0.578768789768219
Accuracy at epoch 55: 0.875, total training samples: 191
Epoch: 56
Negative Examples
['aaaa', 'ababa', 'baaaaa', 'babaaba', 'babbaaba', 'bbbb', 'bbbbbbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaabaab', 'aaabab', 'aababab', 'aabbab', 'abab', 'baaab', 'bbabab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbab', 'baaab', 'bbabab']
Neg Neg Neg
Train Epoch 0, Loss 0.8255036473274231
Train Epoch 1, Loss 0.7634153366088867
Train Epoch 2, Loss 0.7076204419136047
Accuracy at epoch 56: 0.810546875, total training samples: 194
Epoch: 57
Negative Examples
['aaabbbbb', 'aabaab', 'aababb', 'abaa', 'abaab', 'ababab', 'abbbbbba', 'baba', 'babaaabb', 'babba', 'bbbbaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'abaaaaab', 'abaabab']
Pos Pos Pos
Counterexamples
['aabaab', 'abaab', 'ababab']
Pos Pos Pos
Train Epoch 0, Loss 0.7106370329856873
Train Epoch 1, Loss 0.6593870520591736
Train Epoch 2, Loss 0.6113606691360474
Accuracy at epoch 57: 0.89453125, total training samples: 197
Epoch: 58
Negative Examples
['aabaabbb', 'aabbbaaa', 'abaaba', 'bbaaba']
Neg Neg Neg Neg
Positive Examples
['aaaaaaa', 'aaab', 'aabaaab', 'aabab', 'aabbbaab', 'abaaaaab', 'abaabab', 'abab', 'ababaaab', 'baaab', 'bbbabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaaaa', 'aabbbaab', 'baaab', 'bbbabab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7500907182693481
Train Epoch 1, Loss 0.6997700929641724
Train Epoch 2, Loss 0.6582474708557129
Accuracy at epoch 58: 0.892578125, total training samples: 201
Epoch: 59
Negative Examples
['aaaaba', 'aaabbbaa', 'aabaa', 'aababab', 'abbaa', 'abbb', 'baabbaa', 'babab', 'babbbbb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaaab', 'aaabaab']
Pos Pos Pos Pos
Counterexamples
['aababab']
Pos
Train Epoch 0, Loss 0.7154618501663208
Train Epoch 1, Loss 0.6550971269607544
Train Epoch 2, Loss 0.5973805785179138
Accuracy at epoch 59: 0.845703125, total training samples: 202
Epoch: 60
Negative Examples
['aaaa', 'aabb', 'bbaaa']
Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aaabab', 'aabab', 'abaab', 'abab', 'abbabbab', 'baab', 'baabab', 'bbaaab', 'bbabbbab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbabbab', 'baab', 'baabab', 'bbaaab', 'bbabbbab']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.8028193712234497
Train Epoch 1, Loss 0.7339028120040894
Train Epoch 2, Loss 0.6805453896522522
Accuracy at epoch 60: 0.923828125, total training samples: 207
Epoch: 61
Negative Examples
['aabaaba', 'abaabbba', 'ababaaab', 'abba', 'baaab', 'bababb', 'bbaa', 'bbaba', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaaabab', 'aaababbb', 'ababab']
Pos Pos Pos Pos Pos
Counterexamples
['ababaaab', 'aaababbb']
Pos Neg
Train Epoch 0, Loss 0.7019109725952148
Train Epoch 1, Loss 0.6582622528076172
Train Epoch 2, Loss 0.6167332530021667
Accuracy at epoch 61: 0.91796875, total training samples: 209
Epoch: 62
Negative Examples
['aaaabbb', 'abaa', 'abbab', 'abbbaabb', 'baaa', 'bababb', 'bbabab', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaa', 'aaaaabab', 'aaaab', 'aaabab', 'aabaab', 'abaaab', 'abab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaa']
Neg
Train Epoch 0, Loss 0.7152142524719238
Train Epoch 1, Loss 0.6380544900894165
Train Epoch 2, Loss 0.575753927230835
Accuracy at epoch 62: 0.576171875, total training samples: 210
Epoch: 63
Negative Examples
['aaaa', 'aaab', 'aaababab', 'aababaab', 'abaab', 'abab', 'ababaaab', 'abba', 'bbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaab', 'aaababab', 'aababaab', 'abaab', 'abab', 'ababaaab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7281925082206726
Train Epoch 1, Loss 0.674633800983429
Train Epoch 2, Loss 0.6275200843811035
Accuracy at epoch 63: 0.927734375, total training samples: 216
Epoch: 64
Negative Examples
['aaaaabbb', 'baaa', 'babaa', 'bbaba', 'bbbba', 'bbbbabb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaabab', 'aabaaab', 'aabaab', 'abaab', 'abbabaab', 'baab', 'baabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbabaab', 'baab', 'baabab']
Neg Neg Neg
Train Epoch 0, Loss 0.8035860061645508
Train Epoch 1, Loss 0.7432889938354492
Train Epoch 2, Loss 0.6885704398155212
Accuracy at epoch 64: 0.728515625, total training samples: 219
Epoch: 65
Negative Examples
['aaba', 'aabab', 'aababaab', 'abaa', 'abaab', 'abbaa', 'abbab', 'baaa', 'baaaaba', 'baaabbb', 'baaba', 'babbbabb', 'bbaa', 'bbbaba', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab']
Pos
Counterexamples
['aabab', 'aababaab', 'abaab']
Pos Pos Pos
Train Epoch 0, Loss 0.7367969155311584
Train Epoch 1, Loss 0.6839771270751953
Train Epoch 2, Loss 0.6342247724533081
Accuracy at epoch 65: 0.904296875, total training samples: 222
Epoch: 66
Negative Examples
['aaabb', 'aabbaabb', 'aabbabb', 'aabbb', 'abaaaaba', 'abbaaba', 'abbbaa', 'babaaaaa']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabab', 'aababab', 'abaab', 'abaabbab', 'abab', 'ababbaab', 'bbaaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaabbab', 'ababbaab', 'bbaaaab']
Neg Neg Neg
Train Epoch 0, Loss 0.7927525639533997
Train Epoch 1, Loss 0.731533944606781
Train Epoch 2, Loss 0.674297034740448
Accuracy at epoch 66: 0.810546875, total training samples: 225
Epoch: 67
Negative Examples
['aabb', 'aabbba', 'abaaab', 'abaaabab', 'ababaab', 'ababab', 'abababab', 'ababbaba', 'abbbba', 'baab', 'babb', 'babbb', 'bbabb', 'bbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aabab']
Pos Pos
Counterexamples
['abaaab', 'abaaabab', 'ababaab', 'ababab', 'abababab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7321581840515137
Train Epoch 1, Loss 0.6712066531181335
Train Epoch 2, Loss 0.6209286451339722
Accuracy at epoch 67: 0.892578125, total training samples: 230
Epoch: 68
Negative Examples
['abaa', 'baabaaa', 'bababa']
Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabab', 'aabaaaab', 'aabaabab', 'abaaaab', 'abaaab', 'abaaabab', 'abab', 'ababaab', 'bbbbabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['bbbbabab']
Neg
Train Epoch 0, Loss 0.7639426589012146
Train Epoch 1, Loss 0.6770310997962952
Train Epoch 2, Loss 0.6018553972244263
Accuracy at epoch 68: 0.96875, total training samples: 231
Epoch: 69
Negative Examples
['aabbbaba', 'abababab', 'baaa', 'baaba', 'bababb', 'babbbba', 'bbbb', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaab', 'aabaab', 'aabab', 'ababaaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abababab']
Pos
Train Epoch 0, Loss 0.7165395021438599
Train Epoch 1, Loss 0.6323747634887695
Train Epoch 2, Loss 0.5659422874450684
Accuracy at epoch 69: 0.876953125, total training samples: 232
Epoch: 70
Negative Examples
['aabba', 'abbbbaab', 'baabbabb', 'baba', 'babba', 'bbaa', 'bbaaaba', 'bbaab', 'bbabba', 'bbbaaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabaab', 'aabbbbb', 'abaaaaab', 'abaaaab', 'abaab', 'ababab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbbbb']
Neg
Train Epoch 0, Loss 0.723577618598938
Train Epoch 1, Loss 0.6367981433868408
Train Epoch 2, Loss 0.5602303147315979
Accuracy at epoch 70: 0.951171875, total training samples: 233
Epoch: 71
Negative Examples
['aaabbabb', 'ababbaab', 'abbab', 'bbbabab', 'bbbbba']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaa', 'aaaab', 'aaabab', 'aabab', 'aababab', 'abaaaab', 'abaab', 'bbaaaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaaa', 'bbaaaab']
Neg Neg
Train Epoch 0, Loss 0.7462932467460632
Train Epoch 1, Loss 0.6654435396194458
Train Epoch 2, Loss 0.6011264324188232
Accuracy at epoch 71: 0.603515625, total training samples: 235
Epoch: 72
Negative Examples
['aaaab', 'aaaabaab', 'aaab', 'aaaba', 'aaabaaab', 'aabaab', 'aabb', 'abaabab', 'abba', 'baba', 'bbaa', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabab', 'abab']
Pos Pos
Counterexamples
['aaaab', 'aaaabaab', 'aaab', 'aaabaaab', 'aabaab', 'abaabab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7280640602111816
Train Epoch 1, Loss 0.673614501953125
Train Epoch 2, Loss 0.6245970726013184
Accuracy at epoch 72: 0.916015625, total training samples: 241
Epoch: 73
Negative Examples
['abaa', 'abbabbab', 'babaaaba', 'babaabb', 'bbaaaa', 'bbbbaaab']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaabab', 'aaab', 'aaabaab', 'aaabbab', 'aabab', 'aababab', 'abaab', 'ababab', 'baaaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabbab', 'baaaaab']
Neg Neg
Train Epoch 0, Loss 0.8179506063461304
Train Epoch 1, Loss 0.7509219646453857
Train Epoch 2, Loss 0.6908566951751709
Accuracy at epoch 73: 0.66796875, total training samples: 243
Epoch: 74
Negative Examples
['aaaaa', 'aaaabab', 'aaabab', 'aabab', 'abaaaaab', 'abaaab', 'abaabbb', 'abbababb', 'abbb', 'baba', 'bbaab', 'bbabaaa', 'bbabaaab', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaaab']
Pos Pos
Counterexamples
['aaaabab', 'aaabab', 'aabab', 'abaaaaab', 'abaaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7094866633415222
Train Epoch 1, Loss 0.6473929286003113
Train Epoch 2, Loss 0.5932617783546448
Accuracy at epoch 74: 0.87109375, total training samples: 248
Epoch: 75
Negative Examples
['aaabb', 'aababaab', 'aabba', 'bbbb', 'bbbbba']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaaaabab', 'aaabaaab', 'aabaab', 'aabab', 'abaaabab', 'abab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aababaab', 'bbab']
Pos Neg
Train Epoch 0, Loss 0.752373456954956
Train Epoch 1, Loss 0.7087607979774475
Train Epoch 2, Loss 0.6670975089073181
Accuracy at epoch 75: 0.93359375, total training samples: 250
Epoch: 76
Negative Examples
['aaba', 'baabbab', 'bbaba', 'bbabaaa', 'bbbaa']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaaaabab', 'aaaab', 'aaaabab', 'aaab', 'aaabab', 'abaaab', 'abaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 76: No counterexamples found, skipped.
Accuracy at epoch 76: 0.947265625, total training samples: 250
Epoch: 77
Negative Examples
['aaaba', 'aabaaaa', 'ababaa', 'ababba', 'baabbbb', 'babba', 'bbabb', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaaab', 'aaab', 'aabaab', 'ababaaab', 'ababab', 'abababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 77: No counterexamples found, skipped.
Accuracy at epoch 77: 0.93359375, total training samples: 250
Epoch: 78
Negative Examples
['aaaba', 'aaba', 'baaaba', 'baababb', 'bbbbaba', 'bbbbbbbb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaab', 'aaabaaab', 'aaabab', 'aabaaab', 'aabaab', 'aabaabab', 'abaaaaab', 'abaabab', 'ababaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 78: No counterexamples found, skipped.
Accuracy at epoch 78: 0.943359375, total training samples: 250
Epoch: 79
Negative Examples
['abbabaaa', 'abbb', 'abbbb', 'bababbaa', 'babbbbaa', 'bbbaab']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaababab', 'aabab', 'aababaab', 'abaabab', 'ababaaab', 'ababaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 79: No counterexamples found, skipped.
Accuracy at epoch 79: 0.943359375, total training samples: 250
Epoch: 80
Negative Examples
['aaaaaaaa', 'abbbb', 'bbab', 'bbbbbbb']
Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aaabaab', 'aaabab', 'aabaaab', 'aabab', 'abaaab', 'abaabab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 80: No counterexamples found, skipped.
Accuracy at epoch 80: 0.935546875, total training samples: 250
Epoch: 81
Negative Examples
['aaabaaa', 'aaba', 'abaaaa', 'abaaaaba', 'abbabbb', 'babaaa', 'babaabb', 'bbaabaaa', 'bbbaaa', 'bbbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aabab', 'aabbab', 'abaaaab', 'abaaab', 'ababaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbab']
Neg
Train Epoch 0, Loss 0.7156221866607666
Train Epoch 1, Loss 0.6278524398803711
Train Epoch 2, Loss 0.5458109974861145
Accuracy at epoch 81: 0.666015625, total training samples: 251
Epoch: 82
Negative Examples
['aaaba', 'aaabaab', 'aaabab', 'aabaa', 'abbaa', 'abbbb', 'baaaabb', 'babaaaa', 'babbbb', 'bbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaab', 'aaabaaab', 'aabaaab']
Pos Pos Pos Pos
Counterexamples
['aaabaab', 'aaabab']
Pos Pos
Train Epoch 0, Loss 0.783444881439209
Train Epoch 1, Loss 0.6898053884506226
Train Epoch 2, Loss 0.6151399612426758
Accuracy at epoch 82: 0.92578125, total training samples: 253
Epoch: 83
Negative Examples
['ababbaaa', 'bbaaa', 'bbaaabb', 'bbababab']
Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'aaabaaab', 'aaabaab', 'aabaaab', 'aabaabab', 'aabab', 'abaaab', 'abaab', 'abab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 83: No counterexamples found, skipped.
Accuracy at epoch 83: 0.908203125, total training samples: 253
Epoch: 84
Negative Examples
['aaaa', 'abaaaba', 'babaaa', 'bbba', 'bbbabaaa']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaaabab', 'aaaab', 'aaab', 'aabab', 'abaaaabb', 'abaaab', 'abaab', 'abab', 'ababaab', 'abababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaaaabb']
Neg
Train Epoch 0, Loss 0.8164173364639282
Train Epoch 1, Loss 0.7158533930778503
Train Epoch 2, Loss 0.6298353672027588
Accuracy at epoch 84: 0.8828125, total training samples: 254
Epoch: 85
Negative Examples
['abaab', 'ababab', 'abba', 'abbbbaaa', 'abbbbab', 'babbbbab', 'bbaaabb', 'bbbba', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaaabaab', 'aaabaab', 'aabaab', 'abaaaaab', 'abab', 'baaaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'ababab', 'baaaaab']
Pos Pos Neg
Train Epoch 0, Loss 0.7154831886291504
Train Epoch 1, Loss 0.6918594241142273
Train Epoch 2, Loss 0.6709219813346863
Accuracy at epoch 85: 0.794921875, total training samples: 257
Epoch: 86
Negative Examples
['aaaaaab', 'aaaab', 'ababaab', 'abbbaa', 'abbbb', 'baaaba', 'bbaabb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabaab', 'aaabbab', 'aabaaab', 'aabab', 'aababab', 'abaabab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaaab', 'aaaab', 'ababaab', 'aaabbab']
Pos Pos Pos Neg
Train Epoch 0, Loss 0.7411469221115112
Train Epoch 1, Loss 0.7040321826934814
Train Epoch 2, Loss 0.6695986986160278
Accuracy at epoch 86: 0.6796875, total training samples: 261
Epoch: 87
Negative Examples
['aaabb', 'aabaab', 'aabab', 'abaaaaab', 'abaabbaa', 'abab', 'ababaab', 'ababab', 'abbabb', 'baabbabb', 'baabbbbb', 'bababbbb', 'babb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aabbaaab']
Pos Pos
Counterexamples
['aabaab', 'aabab', 'abaaaaab', 'abab', 'ababaab', 'ababab', 'aabbaaab']
Pos Pos Pos Pos Pos Pos Neg
Train Epoch 0, Loss 0.7206624150276184
Train Epoch 1, Loss 0.6957878470420837
Train Epoch 2, Loss 0.6785373687744141
Accuracy at epoch 87: 0.734375, total training samples: 268
Epoch: 88
Negative Examples
['aaaabaab', 'aaabaaab', 'abaaabab', 'abaab', 'abaabaab', 'abaabab', 'abba', 'abbba', 'abbbbba', 'baaaa', 'baabab', 'baababb', 'bbabaab', 'bbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aabab']
Pos Pos
Counterexamples
['aaaabaab', 'aaabaaab', 'abaaabab', 'abaab', 'abaabaab', 'abaabab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7624579071998596
Train Epoch 1, Loss 0.6824372410774231
Train Epoch 2, Loss 0.6233053207397461
Accuracy at epoch 88: 0.859375, total training samples: 274
Epoch: 89
Negative Examples
['aababb', 'abaaa', 'abbbaa', 'abbbbbaa', 'abbbbbba', 'baabbbbb', 'babb', 'bbababa', 'bbbba', 'bbbbabbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaabaaab', 'aabbaaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos
Counterexamples
['aabbaaab']
Neg
Train Epoch 0, Loss 0.7862046957015991
Train Epoch 1, Loss 0.7181434035301208
Train Epoch 2, Loss 0.6582320928573608
Accuracy at epoch 89: 0.806640625, total training samples: 275
Epoch: 90
Negative Examples
['aababab', 'aabba', 'ababaab', 'abababab', 'abbb', 'abbbabba', 'baababab', 'babaabb', 'babababa', 'bbaaa', 'bbbaaab', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aabab', 'abab']
Pos Pos Pos
Counterexamples
['aababab', 'ababaab', 'abababab']
Pos Pos Pos
Train Epoch 0, Loss 0.7528038024902344
Train Epoch 1, Loss 0.6848475337028503
Train Epoch 2, Loss 0.6298864483833313
Accuracy at epoch 90: 0.892578125, total training samples: 278
Epoch: 91
Negative Examples
['aaaaaabb', 'aabb', 'ababb', 'abba', 'baaabbaa', 'babb', 'bbaaba', 'bbabaa', 'bbabbaaa', 'bbbbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaab', 'aababaab', 'abaab', 'babbbb', 'bbab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['babbbb', 'bbab']
Neg Neg
Train Epoch 0, Loss 0.7856873273849487
Train Epoch 1, Loss 0.7034194469451904
Train Epoch 2, Loss 0.6377009153366089
Accuracy at epoch 91: 0.90234375, total training samples: 280
Epoch: 92
Negative Examples
['abaab', 'abaaba', 'ababba', 'abbba', 'abbbab', 'bababb', 'babbba', 'babbbaba', 'bbababaa', 'bbabba', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aabaab', 'aabab', 'aabbbab']
Pos Pos Pos Pos
Counterexamples
['abaab', 'aabbbab']
Pos Neg
Train Epoch 0, Loss 0.6978757977485657
Train Epoch 1, Loss 0.6572151184082031
Train Epoch 2, Loss 0.6215465068817139
Accuracy at epoch 92: 0.859375, total training samples: 282
Epoch: 93
Negative Examples
['aaaabaaa', 'aaabb', 'ababab', 'abbaaaba', 'abbb', 'abbbbbb', 'babbaba', 'bbaa', 'bbab', 'bbabab', 'bbba', 'bbbab', 'bbbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaab', 'aabaab']
Pos Pos Pos
Counterexamples
['ababab']
Pos
Train Epoch 0, Loss 0.7830676436424255
Train Epoch 1, Loss 0.689200222492218
Train Epoch 2, Loss 0.6120069026947021
Accuracy at epoch 93: 0.875, total training samples: 283
Epoch: 94
Negative Examples
['aaaaabbb', 'aabb', 'abaa', 'baaabaaa', 'babbbaa', 'bbabb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaa', 'aaab', 'aabaaab', 'aabaabab', 'aabab', 'aabbabab', 'abaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaa', 'aabbabab']
Neg Neg
Train Epoch 0, Loss 0.7683792114257812
Train Epoch 1, Loss 0.6952711343765259
Train Epoch 2, Loss 0.6356673836708069
Accuracy at epoch 94: 0.80078125, total training samples: 285
Epoch: 95
Negative Examples
['aaabb', 'aabbba', 'ababaaab', 'ababaab', 'baaababa', 'bbaab', 'bbaba', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaab', 'aaabab', 'aabaab', 'aabab', 'abaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababaaab', 'ababaab']
Pos Pos
Train Epoch 0, Loss 0.7033888101577759
Train Epoch 1, Loss 0.6434054970741272
Train Epoch 2, Loss 0.5862584114074707
Accuracy at epoch 95: 0.896484375, total training samples: 287
Epoch: 96
Negative Examples
['abaaabb', 'abaabaaa', 'abbbaabb', 'baaa', 'baaabbb', 'baba', 'babb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aabaaab', 'aabab', 'abab', 'baaabab', 'baab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaabab', 'baab', 'bbab']
Neg Neg Neg
Train Epoch 0, Loss 0.8005568385124207
Train Epoch 1, Loss 0.7308656573295593
Train Epoch 2, Loss 0.6697172522544861
Accuracy at epoch 96: 0.7734375, total training samples: 290
Epoch: 97
Negative Examples
['aaaaabab', 'aaabb', 'aabab', 'aababba', 'aabbbba', 'abab', 'abbabaa', 'baab', 'baabaa', 'baba', 'bbaaabba', 'bbbaabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'ababaab']
Pos Pos
Counterexamples
['aaaaabab', 'aabab', 'abab']
Pos Pos Pos
Train Epoch 0, Loss 0.7550144791603088
Train Epoch 1, Loss 0.6880950927734375
Train Epoch 2, Loss 0.6249573826789856
Accuracy at epoch 97: 0.890625, total training samples: 293
Epoch: 98
Negative Examples
['abaabbba', 'abbabba', 'baba', 'babbab', 'bbbbbbaa']
Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'aaab', 'aaabaab', 'aaabb', 'abaab', 'abab', 'baaabb', 'baab', 'bbbaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabb', 'baaabb', 'baab', 'bbbaabab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7718766927719116
Train Epoch 1, Loss 0.704565703868866
Train Epoch 2, Loss 0.6512348651885986
Accuracy at epoch 98: 0.814453125, total training samples: 297
Epoch: 99
Negative Examples
['abaa', 'abaab', 'abab', 'baaab', 'bbaaaab', 'bbaabaa', 'bbab']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaaabab', 'abaaab', 'ababab']
Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'abab']
Pos Pos
Train Epoch 0, Loss 0.7035800218582153
Train Epoch 1, Loss 0.6483295559883118
Train Epoch 2, Loss 0.5967256426811218
Accuracy at epoch 99: 0.90234375, total training samples: 299
Epoch: 100
Negative Examples
['aababaaa', 'ababaa', 'abababa', 'baaaabb', 'babb', 'babbbb', 'bbaaaa', 'bbabbaa', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabaab', 'aabaab', 'baaab', 'bbabab', 'bbbaabab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['baaab', 'bbabab', 'bbbaabab']
Neg Neg Neg
Train Epoch 0, Loss 0.7665338516235352
Train Epoch 1, Loss 0.6937792301177979
Train Epoch 2, Loss 0.6252394318580627
Accuracy at epoch 100: 0.802734375, total training samples: 302
Epoch: 101
Negative Examples
['aababab', 'abaaaab', 'abaaaba', 'abab', 'baaa', 'baaabbab', 'babaaaba', 'bbaabab', 'bbaba', 'bbabaab', 'bbabbb', 'bbbaaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'abaab']
Pos Pos Pos
Counterexamples
['aababab', 'abaaaab', 'abab']
Pos Pos Pos
Train Epoch 0, Loss 0.7341524958610535
Train Epoch 1, Loss 0.6470987796783447
Train Epoch 2, Loss 0.5856804847717285
Accuracy at epoch 101: 0.91796875, total training samples: 305
Epoch: 102
Negative Examples
['aaabba', 'abaa', 'abaabbbb', 'baababa', 'baabb', 'babaaba', 'babb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaaaabb', 'aaabaab', 'abaab', 'baabab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaabb', 'baabab', 'bbab']
Neg Neg Neg
Train Epoch 0, Loss 0.7797686457633972
Train Epoch 1, Loss 0.7243692874908447
Train Epoch 2, Loss 0.6765842437744141
Accuracy at epoch 102: 0.869140625, total training samples: 308
Epoch: 103
Negative Examples
['aaaa', 'aababaa', 'ababab', 'baaaba', 'baabba', 'babbabb', 'bbaabbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaaabab', 'aaab', 'aaabaaab', 'aabab', 'abaaab', 'abaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababab']
Pos
Train Epoch 0, Loss 0.7150484323501587
Train Epoch 1, Loss 0.6435706615447998
Train Epoch 2, Loss 0.5833215713500977
Accuracy at epoch 103: 0.90234375, total training samples: 309
Epoch: 104
Negative Examples
['aaabbba', 'aababba', 'abba', 'abbaabb', 'abbb', 'baabb', 'bbaba', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'abababab', 'abbbaaab', 'bbab']
Pos Pos Pos Pos
Counterexamples
['abbbaaab', 'bbab']
Neg Neg
Train Epoch 0, Loss 0.7436442971229553
Train Epoch 1, Loss 0.6776912212371826
Train Epoch 2, Loss 0.6165527105331421
Accuracy at epoch 104: 0.90234375, total training samples: 311
Epoch: 105
Negative Examples
['aaba', 'abaaba', 'abbbab', 'abbbbbab', 'baaaab', 'baba', 'bbabaaaa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aabaaaab', 'aabab', 'aababab', 'abaaaab', 'abaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 105: No counterexamples found, skipped.
Accuracy at epoch 105: 0.9140625, total training samples: 311
Epoch: 106
Negative Examples
['aababbab', 'aabb', 'abab', 'baaaa', 'babab', 'babbabab', 'babbb', 'babbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaab', 'aabaaaab', 'aabaab', 'aabab', 'abaab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abab']
Pos
Train Epoch 0, Loss 0.6972383856773376
Train Epoch 1, Loss 0.626251220703125
Train Epoch 2, Loss 0.5583006143569946
Accuracy at epoch 106: 0.873046875, total training samples: 312
Epoch: 107
Negative Examples
['abbaabb', 'babaa', 'bbbaa', 'bbbab']
Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaaabaab', 'aaab', 'aabaab', 'abaa', 'abab', 'ababaab', 'ababab', 'abbb', 'baab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaa', 'abbb', 'baab']
Neg Neg Neg
Train Epoch 0, Loss 0.7079527378082275
Train Epoch 1, Loss 0.650193989276886
Train Epoch 2, Loss 0.6003751158714294
Accuracy at epoch 107: 0.931640625, total training samples: 315
Epoch: 108
Negative Examples
['aaaabb', 'aabaabaa', 'abbababa', 'abbba', 'baab', 'bbaba', 'bbbaba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaab', 'aaabaab', 'aabaaaab', 'aabaab', 'aabab', 'abaabab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 108: No counterexamples found, skipped.
Accuracy at epoch 108: 0.9453125, total training samples: 315
Epoch: 109
Negative Examples
['aaabbba', 'aababaab', 'abbb', 'abbba', 'abbbbab', 'bbababb', 'bbbaab', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaab', 'aaababab', 'aabaaab', 'aabbab', 'abaaaab', 'abaabaab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aababaab', 'aabbab']
Pos Neg
Train Epoch 0, Loss 0.7205028533935547
Train Epoch 1, Loss 0.6809511184692383
Train Epoch 2, Loss 0.6503279805183411
Accuracy at epoch 109: 0.875, total training samples: 317
Epoch: 110
Negative Examples
['aaabbaa', 'abaabaa', 'abab', 'ababbb', 'abba', 'abbaa', 'abbbaba', 'bababb', 'babbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaab', 'aaabaa', 'aabab', 'abaab']
Pos Pos Pos Pos Pos
Counterexamples
['abab', 'aaabaa']
Pos Neg
Train Epoch 0, Loss 0.7500511407852173
Train Epoch 1, Loss 0.700252115726471
Train Epoch 2, Loss 0.6577797532081604
Accuracy at epoch 110: 0.947265625, total training samples: 319
Epoch: 111
Negative Examples
['aaaaa', 'aaaaaaba', 'aaabbaa', 'aabbba', 'aabbbaaa', 'baaaba', 'baaabaaa', 'baabaaa', 'bbabb', 'bbabbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabaaab', 'abaaaab', 'abaab', 'abab']
Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 111: No counterexamples found, skipped.
Accuracy at epoch 111: 0.927734375, total training samples: 319
Epoch: 112
Negative Examples
['aabbabb', 'abaaa', 'abbbaaa', 'abbbab', 'abbbb', 'baaa', 'baabb', 'babababa', 'babb', 'bbaba', 'bbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aabbbaab', 'abaaabab', 'abab']
Pos Pos Pos Pos
Counterexamples
['aabbbaab']
Neg
Train Epoch 0, Loss 0.6984413862228394
Train Epoch 1, Loss 0.5923960208892822
Train Epoch 2, Loss 0.5104816555976868
Accuracy at epoch 112: 0.77734375, total training samples: 320
Epoch: 113
Negative Examples
['aabaa', 'abaab', 'abaabba', 'abab', 'ababbaba', 'abbbabaa', 'abbbbab', 'babbbba', 'bbaa', 'bbaaaab', 'bbaaba', 'bbbb', 'bbbbaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaabb', 'abaaaab']
Pos Pos Pos
Counterexamples
['abaab', 'abab', 'aaabb']
Pos Pos Neg
Train Epoch 0, Loss 0.7311690449714661
Train Epoch 1, Loss 0.6925086379051208
Train Epoch 2, Loss 0.6575811505317688
Accuracy at epoch 113: 0.912109375, total training samples: 323
Epoch: 114
Negative Examples
['aaaaba', 'abaab', 'abaabaa', 'abbbaaa', 'abbbba', 'abbbbab', 'bbaaa', 'bbaaaa', 'bbabaa', 'bbbaaaa', 'bbbababa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aaababab', 'aabaaab', 'aabab']
Pos Pos Pos Pos Pos
Counterexamples
['abaab']
Pos
Train Epoch 0, Loss 0.7123159766197205
Train Epoch 1, Loss 0.6376999616622925
Train Epoch 2, Loss 0.570033609867096
Accuracy at epoch 114: 0.91015625, total training samples: 324
Epoch: 115
Negative Examples
['aaabaabb', 'abbb', 'baababb', 'babba']
Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aabaaaab', 'aabaab', 'aababaab', 'abaaabab', 'abaab', 'abab', 'babaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['babaabab']
Neg
Train Epoch 0, Loss 0.8080725073814392
Train Epoch 1, Loss 0.6972691416740417
Train Epoch 2, Loss 0.6032922863960266
Accuracy at epoch 115: 0.76953125, total training samples: 325
Epoch: 116
Negative Examples
['aaabab', 'aabaaaba', 'abaa', 'abbab', 'abbabbb', 'abbb', 'abbba', 'baabba', 'babbb', 'bbab', 'bbbaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aabaab', 'abaaaaab']
Pos Pos Pos Pos
Counterexamples
['aaabab']
Pos
Train Epoch 0, Loss 0.7225189208984375
Train Epoch 1, Loss 0.6239792704582214
Train Epoch 2, Loss 0.5533891320228577
Accuracy at epoch 116: 0.931640625, total training samples: 326
Epoch: 117
Negative Examples
['ababaaa', 'abbba', 'babbbaa']
Neg Neg Neg
Positive Examples
['aaaabaab', 'aaab', 'aaabaaab', 'aaabaab', 'aaabbab', 'aabaaab', 'aabbab', 'abaaab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabbab', 'aabbab']
Neg Neg
Train Epoch 0, Loss 0.9241312742233276
Train Epoch 1, Loss 0.7873883247375488
Train Epoch 2, Loss 0.6639459133148193
Accuracy at epoch 117: 0.798828125, total training samples: 328
Epoch: 118
Negative Examples
['aaaa', 'aaaaaa', 'aabaabab', 'aababaab', 'abaabaab', 'abab', 'ababaab', 'ababbabb', 'babaa', 'bbabbbaa', 'bbabbbba', 'bbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aabab', 'abaaaaab']
Pos Pos Pos Pos
Counterexamples
['aabaabab', 'aababaab', 'abaabaab', 'abab', 'ababaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7886039018630981
Train Epoch 1, Loss 0.6936937570571899
Train Epoch 2, Loss 0.6210277676582336
Accuracy at epoch 118: 0.908203125, total training samples: 333
Epoch: 119
Negative Examples
['aaabbb', 'aaba', 'abbaabaa', 'abbabbaa', 'baaabbb', 'bbaabaa', 'bbabaa', 'bbbaaba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'aabaabab', 'aabab', 'aababaab', 'babbaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['babbaab']
Neg
Train Epoch 0, Loss 0.7918866872787476
Train Epoch 1, Loss 0.6944445371627808
Train Epoch 2, Loss 0.6128493547439575
Accuracy at epoch 119: 0.90625, total training samples: 334
Epoch: 120
Negative Examples
['abaabb', 'abbab', 'baaaa', 'baaababa', 'baababa', 'baababaa', 'bbaaaa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaab', 'aaabab', 'aabab', 'abaaaaab', 'abaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 120: No counterexamples found, skipped.
Accuracy at epoch 120: 0.89453125, total training samples: 334
Epoch: 121
Negative Examples
['aaabbbab', 'aababab', 'aabba', 'baabbb', 'babab', 'bbbbbbab']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabb', 'aaaab', 'aabaaaab', 'abaaaab', 'abaab', 'abab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aababab', 'aaaaabb']
Pos Neg
Train Epoch 0, Loss 0.7085200548171997
Train Epoch 1, Loss 0.6474679708480835
Train Epoch 2, Loss 0.5947664976119995
Accuracy at epoch 121: 0.91796875, total training samples: 336
Epoch: 122
Negative Examples
['aaaaaba', 'abaaaaaa', 'abaabbba', 'ababaab', 'babbb', 'babbbaab', 'bbabbbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaabaaab', 'aaabaab', 'aabaab', 'abaab', 'abaabaab', 'abab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababaab', 'bbab']
Pos Neg
Train Epoch 0, Loss 0.729765772819519
Train Epoch 1, Loss 0.680724024772644
Train Epoch 2, Loss 0.6371036767959595
Accuracy at epoch 122: 0.916015625, total training samples: 338
Epoch: 123
Negative Examples
['aabbaa', 'aabbb', 'bababaa', 'bababba', 'bbabbb', 'bbbab']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaab', 'aaab', 'aaabab', 'abaaaaab', 'abaabab', 'baaaab', 'baabab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaaab', 'baabab']
Neg Neg
Train Epoch 0, Loss 0.8021592497825623
Train Epoch 1, Loss 0.7187755703926086
Train Epoch 2, Loss 0.646668553352356
Accuracy at epoch 123: 0.484375, total training samples: 340
Epoch: 124
Negative Examples
['aaaaaaab', 'aaaaab', 'aaaabab', 'aaab', 'aaabab', 'aabaaab', 'aababab', 'aabbaa', 'aabbabb', 'abaab', 'abbaaa', 'bbaabba', 'bbab', 'bbabb', 'bbbaabba', 'bbbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaaaaab', 'aaaaab', 'aaaabab', 'aaab', 'aaabab', 'aabaaab', 'aababab', 'abaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7454226016998291
Train Epoch 1, Loss 0.6737963557243347
Train Epoch 2, Loss 0.6173681020736694
Accuracy at epoch 124: 0.96484375, total training samples: 348
Epoch: 125
Negative Examples
['aaaa', 'aaaaa', 'abbaaaa', 'abbab', 'baaabaaa', 'baabbbb', 'bbba', 'bbbab', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'abaaaab', 'ababab']
Pos Pos Pos Pos
Counterexamples
[]

Round 125: No counterexamples found, skipped.
Accuracy at epoch 125: 0.951171875, total training samples: 348
Epoch: 126
Negative Examples
['aabba', 'abababab', 'abbbb', 'baababab', 'babababb', 'babbb', 'babbbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaab', 'aaabaab', 'aabab', 'aababab', 'abaab', 'abab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abababab']
Pos
Train Epoch 0, Loss 0.7551848888397217
Train Epoch 1, Loss 0.6212867498397827
Train Epoch 2, Loss 0.5298810005187988
Accuracy at epoch 126: 0.904296875, total training samples: 349
Epoch: 127
Negative Examples
['aaaaa', 'aaba', 'abaaabaa', 'baaabb', 'bbaba', 'bbabb', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaab', 'aaabab', 'aababbab', 'ababaab', 'baabaab', 'bbabab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aababbab', 'baabaab', 'bbabab']
Neg Neg Neg
Train Epoch 0, Loss 0.8069241642951965
Train Epoch 1, Loss 0.7424681186676025
Train Epoch 2, Loss 0.6864144206047058
Accuracy at epoch 127: 0.56640625, total training samples: 352
Epoch: 128
Negative Examples
['aaaab', 'aaabaab', 'aaababb', 'aababaab', 'aabbb', 'abaaaaab', 'abaaab', 'abaaabab', 'ababaab', 'baabbbb', 'bababa', 'babb', 'babbaaaa', 'bbab', 'bbbabbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abababab']
Pos
Counterexamples
['aaaab', 'aaabaab', 'aababaab', 'abaaaaab', 'abaaab', 'abaaabab', 'ababaab']
Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7710277438163757
Train Epoch 1, Loss 0.6829386949539185
Train Epoch 2, Loss 0.618279755115509
Accuracy at epoch 128: 0.94140625, total training samples: 359
Epoch: 129
Negative Examples
['ababb', 'abba', 'abbba', 'abbbbaaa', 'baabaa', 'baabbbb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aabaab', 'abaaaaab', 'abaabaab', 'abab']
Pos Pos Pos Pos Pos
Counterexamples
[]

Round 129: No counterexamples found, skipped.
Accuracy at epoch 129: 0.92578125, total training samples: 359
Epoch: 130
Negative Examples
['aaaaba', 'aaaababa', 'aaabb', 'abbbba', 'babaaba', 'bbba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'aaab', 'aaabbab', 'aabaaaab', 'aabab', 'abaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabbab']
Neg
Train Epoch 0, Loss 0.8180921077728271
Train Epoch 1, Loss 0.7080143094062805
Train Epoch 2, Loss 0.6079099774360657
Accuracy at epoch 130: 0.646484375, total training samples: 360
Epoch: 131
Negative Examples
['aababaab', 'aabb', 'aabbba', 'abaaaba', 'abaabab', 'abab', 'baaaba', 'babbb', 'bbaabab', 'bbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abaaaab', 'abaaab']
Pos Pos
Counterexamples
['aababaab', 'abaabab', 'abab']
Pos Pos Pos
Train Epoch 0, Loss 0.7898351550102234
Train Epoch 1, Loss 0.7106781005859375
Train Epoch 2, Loss 0.6499025225639343
Accuracy at epoch 131: 0.919921875, total training samples: 363
Epoch: 132
Negative Examples
['aaaaaba', 'abaaabbb', 'abbbbbba', 'bbaabbb', 'bbbab']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaab', 'aababaab', 'abaaab', 'abaabab', 'abab', 'abbaab', 'baaab', 'baab', 'baabb']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbaab', 'baaab', 'baab', 'baabb']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.8014079928398132
Train Epoch 1, Loss 0.7347990274429321
Train Epoch 2, Loss 0.6784339547157288
Accuracy at epoch 132: 0.791015625, total training samples: 367
Epoch: 133
Negative Examples
['aaaba', 'aababb', 'aabb', 'abaaaab', 'abab', 'bbaa', 'bbaaa', 'bbaab', 'bbba', 'bbbabb', 'bbbbbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaaab', 'aaaaab', 'abaaaaab', 'abababab']
Pos Pos Pos Pos Pos
Counterexamples
['abaaaab', 'abab']
Pos Pos
Train Epoch 0, Loss 0.6995638608932495
Train Epoch 1, Loss 0.6425833702087402
Train Epoch 2, Loss 0.5918505191802979
Accuracy at epoch 133: 0.923828125, total training samples: 369
Epoch: 134
Negative Examples
['aababb', 'abbaa', 'abbbba', 'bbabaa']
Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaab', 'aaabaaab', 'aabab', 'abaaaaab', 'abaab', 'abab', 'abbaaaab', 'baabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbaaaab', 'baabab']
Neg Neg
Train Epoch 0, Loss 0.8261306285858154
Train Epoch 1, Loss 0.7438586354255676
Train Epoch 2, Loss 0.6699839234352112
Accuracy at epoch 134: 0.810546875, total training samples: 371
Epoch: 135
Negative Examples
['aaaaaa', 'aaba', 'aabbbbba', 'abaaabab', 'abaab', 'abbb', 'abbbab', 'baba', 'bbabaa', 'bbbbaaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aabaaab', 'abaaab', 'abab']
Pos Pos Pos Pos
Counterexamples
['abaaabab', 'abaab']
Pos Pos
Train Epoch 0, Loss 0.7436893582344055
Train Epoch 1, Loss 0.6670674681663513
Train Epoch 2, Loss 0.6125519275665283
Accuracy at epoch 135: 0.869140625, total training samples: 373
Epoch: 136
Negative Examples
['aaaaabb', 'aaabaaa', 'abbbba', 'bababb', 'bbabaa', 'bbba', 'bbbabbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaabaab', 'aaabbab', 'aabaabab', 'aabab', 'abab', 'baab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabbab', 'baab', 'bbab']
Neg Neg Neg
Train Epoch 0, Loss 0.7646906971931458
Train Epoch 1, Loss 0.6995806694030762
Train Epoch 2, Loss 0.6433125138282776
Accuracy at epoch 136: 0.884765625, total training samples: 376
Epoch: 137
Negative Examples
['aaaabb', 'aaabaab', 'aabaab', 'aabababa', 'abaab', 'bbbbaaa']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'aaab', 'aaabaaab', 'aabab', 'aababab', 'abaabab', 'abab', 'baaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabaab', 'aabaab', 'abaab', 'baaabab']
Pos Pos Pos Neg
Train Epoch 0, Loss 0.7848458290100098
Train Epoch 1, Loss 0.7330959439277649
Train Epoch 2, Loss 0.6933634877204895
Accuracy at epoch 137: 0.75390625, total training samples: 380
Epoch: 138
Negative Examples
['aaaa', 'aaaaa', 'aabab', 'aababbba', 'abaab', 'ababab', 'abbabba', 'baaaab', 'babbba', 'bbaaab', 'bbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaab', 'abaaab']
Pos Pos Pos
Counterexamples
['aabab', 'abaab', 'ababab']
Pos Pos Pos
Train Epoch 0, Loss 0.7494192719459534
Train Epoch 1, Loss 0.6798133850097656
Train Epoch 2, Loss 0.6242086887359619
Accuracy at epoch 138: 0.927734375, total training samples: 383
Epoch: 139
Negative Examples
['aaabbbba', 'aabbbaab', 'abbba', 'bbbbbaab']
Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aaab', 'aaabab', 'aaababab', 'aabab', 'abaaab', 'abaaabab', 'ababab', 'babbaaab', 'bbaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['babbaaab', 'bbaaab']
Neg Neg
Train Epoch 0, Loss 0.7330924272537231
Train Epoch 1, Loss 0.6450421214103699
Train Epoch 2, Loss 0.5873318910598755
Accuracy at epoch 139: 0.74609375, total training samples: 385
Epoch: 140
Negative Examples
['aabaab', 'aabababa', 'abaabaab', 'ababaaab', 'ababbba', 'abbaa', 'baaaba', 'bbaabb', 'bbabbabb', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabab', 'aabaabab', 'abaaaab', 'ababab']
Pos Pos Pos Pos Pos
Counterexamples
['aabaab', 'abaabaab', 'ababaaab']
Pos Pos Pos
Train Epoch 0, Loss 0.7270253300666809
Train Epoch 1, Loss 0.6489794850349426
Train Epoch 2, Loss 0.5883174538612366
Accuracy at epoch 140: 0.89453125, total training samples: 388
Epoch: 141
Negative Examples
['aaaa', 'aabababa', 'aabbaaa', 'baabaa', 'baabbaa', 'babbaaa', 'bbaa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabaab', 'aabaa', 'aababab', 'abaab', 'abab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aabaa']
Neg
Train Epoch 0, Loss 0.7290493249893188
Train Epoch 1, Loss 0.6405904293060303
Train Epoch 2, Loss 0.5603041648864746
Accuracy at epoch 141: 0.587890625, total training samples: 389
Epoch: 142
Negative Examples
['aaaaa', 'aaaaabab', 'aaaabab', 'aaab', 'aaabab', 'aabaaab', 'aabab', 'abab', 'ababaab', 'abba', 'abbbbb', 'bbab', 'bbbab', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaaabab', 'aaaabab', 'aaab', 'aaabab', 'aabaaab', 'aabab', 'abab', 'ababaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7302337288856506
Train Epoch 1, Loss 0.6348612308502197
Train Epoch 2, Loss 0.5637406706809998
Accuracy at epoch 142: 0.912109375, total training samples: 397
Epoch: 143
Negative Examples
['aaababaa', 'aabaa', 'abaaaa', 'baba', 'babaaaaa', 'babbbbba', 'bbababa', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaa', 'aaaaaaa', 'aaaaabab', 'aaaaabb', 'aaab', 'aabaabab', 'aabab', 'abaabb']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaa', 'aaaaaaa', 'aaaaabb', 'abaabb']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7669110894203186
Train Epoch 1, Loss 0.6964676380157471
Train Epoch 2, Loss 0.6370172500610352
Accuracy at epoch 143: 0.919921875, total training samples: 401
Epoch: 144
Negative Examples
['aaaaa', 'abbaa', 'abbbbaa', 'babaabaa', 'bbaa', 'bbaabab', 'bbabb', 'bbabba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabab', 'aabaaaab', 'aabaaab', 'aabab', 'abaaaab', 'abaab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 144: No counterexamples found, skipped.
Accuracy at epoch 144: 0.927734375, total training samples: 401
Epoch: 145
Negative Examples
['aaaa', 'aaaabb', 'aababaab', 'ababaab', 'abbbbbbb', 'baaaaa', 'babaab', 'bbabb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabaab', 'aaabab', 'abaaaaab', 'abaaab', 'babbbab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aababaab', 'ababaab', 'babbbab']
Pos Pos Neg
Train Epoch 0, Loss 0.71185302734375
Train Epoch 1, Loss 0.6539294719696045
Train Epoch 2, Loss 0.5987784266471863
Accuracy at epoch 145: 0.9375, total training samples: 404
Epoch: 146
Negative Examples
['aabbb', 'ababbaa', 'abbbaab', 'bbaba']
Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaab', 'aababaab', 'abaaab', 'abaab', 'abab', 'ababaaab', 'baaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaab']
Neg
Train Epoch 0, Loss 0.8133450150489807
Train Epoch 1, Loss 0.7213851809501648
Train Epoch 2, Loss 0.6436451077461243
Accuracy at epoch 146: 0.587890625, total training samples: 405
Epoch: 147
Negative Examples
['aaaab', 'aaab', 'aaabab', 'aabaaab', 'aabab', 'abaaa', 'abaaabb', 'ababab', 'baaabab', 'baabbaab', 'baabbbbb', 'babaaba', 'bbabbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabaab']
Pos
Counterexamples
['aaaab', 'aaab', 'aaabab', 'aabaaab', 'aabab', 'ababab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7332735061645508
Train Epoch 1, Loss 0.6644132733345032
Train Epoch 2, Loss 0.6053580641746521
Accuracy at epoch 147: 0.970703125, total training samples: 411
Epoch: 148
Negative Examples
['aaaba', 'abaaaba', 'babaab', 'bbabbb', 'bbbbbbbb']
Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabaaa', 'aabaabab', 'aabab', 'abaabaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabaaa']
Neg
Train Epoch 0, Loss 0.6951115131378174
Train Epoch 1, Loss 0.6042007207870483
Train Epoch 2, Loss 0.5236524343490601
Accuracy at epoch 148: 0.87890625, total training samples: 412
Epoch: 149
Negative Examples
['aaba', 'abaab', 'ababbaa', 'babbbbaa', 'bbabbbbb', 'bbbb', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabaab', 'aaab', 'aaabaab', 'aaabab', 'aabab', 'abab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abaab']
Pos
Train Epoch 0, Loss 0.7238215804100037
Train Epoch 1, Loss 0.6391462087631226
Train Epoch 2, Loss 0.5616058111190796
Accuracy at epoch 149: 0.78125, total training samples: 413
Epoch: 150
Negative Examples
['aaaabaa', 'aaabbba', 'abaa', 'abaaaaa', 'ababaa', 'bbaaa']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabab', 'aabab', 'abaab', 'abaabab', 'abbaab', 'bababbb']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbaab', 'bababbb']
Neg Neg
Train Epoch 0, Loss 0.8195319771766663
Train Epoch 1, Loss 0.7166385650634766
Train Epoch 2, Loss 0.6328182220458984
Accuracy at epoch 150: 0.90234375, total training samples: 415
Epoch: 151
Negative Examples
['abaabba', 'abbb', 'babb', 'babbbbab', 'bbaaba']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaaabaab', 'aaaabab', 'aaab', 'aabaab', 'abaaaab', 'abaab', 'abaabaab', 'abaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 151: No counterexamples found, skipped.
Accuracy at epoch 151: 0.8984375, total training samples: 415
Epoch: 152
Negative Examples
['aaba', 'abaa', 'ababab', 'ababb', 'abba', 'abbabaab', 'baaa', 'baaaabb', 'babbaaaa', 'bbaaa', 'bbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabab', 'aabaab', 'abaaaaab']
Pos Pos Pos Pos Pos
Counterexamples
['ababab']
Pos
Train Epoch 0, Loss 0.7264211177825928
Train Epoch 1, Loss 0.6222062110900879
Train Epoch 2, Loss 0.5332865715026855
Accuracy at epoch 152: 0.90625, total training samples: 416
Epoch: 153
Negative Examples
['aaaaaabb', 'aaba', 'aabaabb', 'abba', 'babaaba', 'bbabb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabaab', 'aabab', 'abaab', 'abab', 'abbb', 'bbab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbb', 'bbab']
Neg Neg
Train Epoch 0, Loss 0.7736711502075195
Train Epoch 1, Loss 0.6935275197029114
Train Epoch 2, Loss 0.6278590559959412
Accuracy at epoch 153: 0.962890625, total training samples: 418
Epoch: 154
Negative Examples
['aaaa', 'aaabb', 'abaa', 'ababbb', 'ababbba', 'baaba', 'baabaabb', 'bbbbabaa']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aaababab', 'aaabbab', 'aababaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabbab']
Neg
Train Epoch 0, Loss 0.7947713732719421
Train Epoch 1, Loss 0.7072076201438904
Train Epoch 2, Loss 0.6275995969772339
Accuracy at epoch 154: 0.611328125, total training samples: 419
Epoch: 155
Negative Examples
['aaabaab', 'aaabab', 'aaba', 'abaab', 'abab', 'ababab', 'abbaaaba', 'baaa', 'baaaaaaa', 'babba', 'babbbbaa', 'bbab', 'bbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab']
Pos
Counterexamples
['aaabaab', 'aaabab', 'abaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7684088349342346
Train Epoch 1, Loss 0.7026776075363159
Train Epoch 2, Loss 0.64144366979599
Accuracy at epoch 155: 0.939453125, total training samples: 424
Epoch: 156
Negative Examples
['aaaba', 'aaabb', 'aababbb', 'baaaaa', 'baaba', 'baabb', 'bababaaa', 'bbbbaba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'abaaab', 'abaab', 'abab', 'abbab', 'baaaaaab', 'bbaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbab', 'baaaaaab', 'bbaaab']
Neg Neg Neg
Train Epoch 0, Loss 0.7903611063957214
Train Epoch 1, Loss 0.7218582034111023
Train Epoch 2, Loss 0.6616776585578918
Accuracy at epoch 156: 0.541015625, total training samples: 427
Epoch: 157
Negative Examples
['aaaaaab', 'aaaab', 'aaab', 'aaabbbba', 'aabaaab', 'aabab', 'abaaab', 'baaaaa', 'baabaab', 'babbaaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaababab']
Pos Pos
Counterexamples
['aaaaaab', 'aaaab', 'aaab', 'aabaaab', 'aabab', 'abaaab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7275200486183167
Train Epoch 1, Loss 0.6537330746650696
Train Epoch 2, Loss 0.5928229689598083
Accuracy at epoch 157: 0.931640625, total training samples: 433
Epoch: 158
Negative Examples
['aababaab', 'aabbab', 'abaa', 'abaabba', 'baaa', 'baaaaaa', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aabaaab', 'aabab', 'abab']
Pos Pos Pos Pos Pos
Counterexamples
['aababaab']
Pos
Train Epoch 0, Loss 0.8127772212028503
Train Epoch 1, Loss 0.6953607797622681
Train Epoch 2, Loss 0.6087778210639954
Accuracy at epoch 158: 0.9140625, total training samples: 434
Epoch: 159
Negative Examples
['aaaaba', 'aabaa', 'aabb', 'abbb', 'bbaa', 'bbababb', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaabb', 'aabaaab', 'aabab', 'abaaab', 'abaaabab', 'abaab', 'abababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabb']
Neg
Train Epoch 0, Loss 0.7095598578453064
Train Epoch 1, Loss 0.6237006187438965
Train Epoch 2, Loss 0.5489845871925354
Accuracy at epoch 159: 0.927734375, total training samples: 435
Epoch: 160
Negative Examples
['aabb', 'abbabb', 'abbb', 'babaabaa', 'babaabba']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaabab', 'aabaabab', 'abaab', 'abab', 'ababab', 'babab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['babab']
Neg
Train Epoch 0, Loss 0.7804620862007141
Train Epoch 1, Loss 0.6869907975196838
Train Epoch 2, Loss 0.6089199781417847
Accuracy at epoch 160: 0.65234375, total training samples: 436
Epoch: 161
Negative Examples
['aaaab', 'aaabaaba', 'aaabab', 'aabaaaab', 'abaaaab', 'abaabab', 'ababab', 'abbb', 'abbbaab', 'abbbab', 'bbaaa', 'bbbb', 'bbbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baab']
Pos
Counterexamples
['aaaab', 'aaabab', 'aabaaaab', 'abaaaab', 'abaabab', 'ababab', 'baab']
Pos Pos Pos Pos Pos Pos Neg
Train Epoch 0, Loss 0.7291240692138672
Train Epoch 1, Loss 0.6872739791870117
Train Epoch 2, Loss 0.649530291557312
Accuracy at epoch 161: 0.849609375, total training samples: 443
Epoch: 162
Negative Examples
['aaabaaaa', 'abaaab', 'abaab', 'abab', 'babb', 'bbaa', 'bbababb', 'bbba', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aaabaaab', 'aaabab', 'aaababab', 'abaaabab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abaaab', 'abaab', 'abab']
Pos Pos Pos
Train Epoch 0, Loss 0.719231367111206
Train Epoch 1, Loss 0.640537440776825
Train Epoch 2, Loss 0.5691735744476318
Accuracy at epoch 162: 0.837890625, total training samples: 446
Epoch: 163
Negative Examples
['aaba', 'baaa', 'babb', 'bbaaa', 'bbaabbb', 'bbbbbaa']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabaaab', 'aaabab', 'abaa', 'abaaab', 'abaaabab', 'abaab', 'abbabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaa', 'abbabab']
Neg Neg
Train Epoch 0, Loss 0.8173224925994873
Train Epoch 1, Loss 0.6992847919464111
Train Epoch 2, Loss 0.596329927444458
Accuracy at epoch 163: 0.8984375, total training samples: 448
Epoch: 164
Negative Examples
['aaabbbaa', 'aabaaabb', 'abaabab', 'abab', 'baabba', 'bababbbb', 'babbaaaa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaab', 'aaaabab', 'aaabaab', 'aabaab', 'aababab', 'abaab', 'ababaaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaabab', 'abab']
Pos Pos
Train Epoch 0, Loss 0.7226088047027588
Train Epoch 1, Loss 0.6295995116233826
Train Epoch 2, Loss 0.5499459505081177
Accuracy at epoch 164: 0.87890625, total training samples: 450
Epoch: 165
Negative Examples
['aaabb', 'ababbbb', 'abba', 'abbba', 'baabaaba', 'baabba', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaababab', 'aabaaab', 'aababab', 'abaab', 'abaabab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 165: No counterexamples found, skipped.
Accuracy at epoch 165: 0.873046875, total training samples: 450
Epoch: 166
Negative Examples
['aaaaaaba', 'aabb', 'abababa', 'abbaba', 'baaa', 'babaaaba', 'bbaaa', 'bbaba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabaaab', 'aabaaab', 'aabaab', 'aababab', 'abababab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 166: No counterexamples found, skipped.
Accuracy at epoch 166: 0.87109375, total training samples: 450
Epoch: 167
Negative Examples
['aaabb', 'aabbba', 'ababaaa', 'ababb', 'abbaa', 'babaa', 'babbbaa', 'bbababbb', 'bbbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaa', 'aaaaab', 'aaabaab', 'abaaaab', 'abaab', 'ababab', 'baaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaaa', 'baaab']
Neg Neg
Train Epoch 0, Loss 0.8135384917259216
Train Epoch 1, Loss 0.726617693901062
Train Epoch 2, Loss 0.6486301422119141
Accuracy at epoch 167: 0.8359375, total training samples: 452
Epoch: 168
Negative Examples
['aaaaba', 'aabbaab', 'abaaab', 'abbaabaa', 'abbbab', 'abbbb', 'baaa', 'baaab']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaabaab', 'aaabab', 'abaaaaab', 'abaab', 'abaabaab', 'abab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaaab']
Pos
Train Epoch 0, Loss 0.7269384264945984
Train Epoch 1, Loss 0.6382792592048645
Train Epoch 2, Loss 0.5593509078025818
Accuracy at epoch 168: 0.931640625, total training samples: 453
Epoch: 169
Negative Examples
['aaaa', 'aababb', 'aabb', 'baba', 'bbababb', 'bbbbaa']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaab', 'aaabab', 'aaababaa', 'aabab', 'abaaab', 'abaabbab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaababaa', 'abaabbab']
Neg Neg
Train Epoch 0, Loss 0.8044681549072266
Train Epoch 1, Loss 0.6795119047164917
Train Epoch 2, Loss 0.5713015198707581
Accuracy at epoch 169: 0.833984375, total training samples: 455
Epoch: 170
Negative Examples
['aaaa', 'abab', 'ababaab', 'abbbaab', 'abbbb', 'abbbbaa', 'bbaabab', 'bbbaaaa', 'bbbaaba', 'bbbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaaabab', 'aabaab', 'aabab', 'abaaaaab', 'abaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abab', 'ababaab']
Pos Pos
Train Epoch 0, Loss 0.710677981376648
Train Epoch 1, Loss 0.6278365254402161
Train Epoch 2, Loss 0.5616410970687866
Accuracy at epoch 170: 0.90234375, total training samples: 457
Epoch: 171
Negative Examples
['abaaaaa', 'abbabaaa', 'abbbb', 'bababba', 'babbaa', 'bbaabb', 'bbabbbbb', 'bbbabaa', 'bbbabbaa', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaab', 'aabaaab', 'aabaabab', 'abab']
Pos Pos Pos Pos Pos
Counterexamples
[]

Round 171: No counterexamples found, skipped.
Accuracy at epoch 171: 0.931640625, total training samples: 457
Epoch: 172
Negative Examples
['aaaabb', 'abaaa', 'bbabbba', 'bbabbbbb', 'bbbbaaaa']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaab', 'aababab', 'aababbab', 'abab', 'ababab', 'babab', 'bbaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aababbab', 'babab', 'bbaab']
Neg Neg Neg
Train Epoch 0, Loss 0.7910013794898987
Train Epoch 1, Loss 0.7015277743339539
Train Epoch 2, Loss 0.6190345287322998
Accuracy at epoch 172: 0.88671875, total training samples: 460
Epoch: 173
Negative Examples
['aaaabbab', 'ababab', 'babaaa', 'babbb', 'bbbaaab', 'bbbba', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabab', 'aabaab', 'aabab', 'abaaaab', 'abaaabab', 'abab', 'ababaaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababab']
Pos
Train Epoch 0, Loss 0.7955834269523621
Train Epoch 1, Loss 0.6701629161834717
Train Epoch 2, Loss 0.5683971643447876
Accuracy at epoch 173: 0.931640625, total training samples: 461
Epoch: 174
Negative Examples
['abaaaaa', 'baaaaaba', 'baaaabba', 'babba', 'babbaa', 'bbbaa', 'bbbaabaa', 'bbbbaaa']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaabaab', 'aaab', 'aaabab', 'aaababab', 'abaab', 'abab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 174: No counterexamples found, skipped.
Accuracy at epoch 174: 0.908203125, total training samples: 461
Epoch: 175
Negative Examples
['bbab', 'bbabaa', 'bbba', 'bbbb']
Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aababaab', 'aabbab', 'abaaaaab', 'abab', 'abababab', 'baaaabb', 'baaaabbb', 'baabbab', 'bbaaaabb']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbab', 'baaaabb', 'baaaabbb', 'baabbab', 'bbaaaabb']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7830909490585327
Train Epoch 1, Loss 0.7092303037643433
Train Epoch 2, Loss 0.6433688998222351
Accuracy at epoch 175: 0.923828125, total training samples: 466
Epoch: 176
Negative Examples
['aaababa', 'abaaabb', 'abaab', 'abbbb', 'abbbbaa', 'babaa', 'babb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabaab', 'aaaabab', 'aaabab', 'aaababab', 'aabaab', 'abab', 'bbaaabab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'bbaaabab']
Pos Neg
Train Epoch 0, Loss 0.7075815200805664
Train Epoch 1, Loss 0.6548958420753479
Train Epoch 2, Loss 0.6080901622772217
Accuracy at epoch 176: 0.78125, total training samples: 468
Epoch: 177
Negative Examples
['aaababab', 'abab', 'abbbaa', 'abbbab', 'baaaba', 'baaba', 'baabaaa', 'bababaa', 'bababb', 'babbaabb', 'babbb', 'bbbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'abaaab']
Pos Pos
Counterexamples
['aaababab', 'abab']
Pos Pos
Train Epoch 0, Loss 0.7645972967147827
Train Epoch 1, Loss 0.6729289293289185
Train Epoch 2, Loss 0.5952597856521606
Accuracy at epoch 177: 0.904296875, total training samples: 470
Epoch: 178
Negative Examples
['aaba', 'aabbbba', 'abaaaa', 'abaaabaa', 'baaa', 'bbbaa', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabaaaab', 'aabaab', 'aabab', 'aabb', 'ababaab', 'baaaaab', 'babab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabb', 'baaaaab', 'babab', 'bbab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7575438618659973
Train Epoch 1, Loss 0.6976662874221802
Train Epoch 2, Loss 0.6431151032447815
Accuracy at epoch 178: 0.5234375, total training samples: 474
Epoch: 179
Negative Examples
['aaaaa', 'aaabab', 'aabbaa', 'abaaab', 'abaab', 'abaabaaa', 'abab', 'ababab', 'abbb', 'baabaaaa', 'bababa', 'babba', 'bbbb', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aababaab']
Pos
Counterexamples
['aaabab', 'abaaab', 'abaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7548426985740662
Train Epoch 1, Loss 0.6749467849731445
Train Epoch 2, Loss 0.6125668883323669
Accuracy at epoch 179: 0.919921875, total training samples: 479
Epoch: 180
Negative Examples
['aaaa', 'aaabbb', 'aaba', 'abbabaa', 'babbabb', 'bbabbbab', 'bbbbbbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabab', 'aabaaab', 'abaabab', 'ababaaab', 'ababaab', 'ababab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['bbab']
Neg
Train Epoch 0, Loss 0.7230709791183472
Train Epoch 1, Loss 0.6311851143836975
Train Epoch 2, Loss 0.5504665970802307
Accuracy at epoch 180: 0.81640625, total training samples: 480
Epoch: 181
Negative Examples
['abaaa', 'abab', 'ababab', 'abababab', 'ababbaaa', 'baaababb', 'baaabb', 'baabbaba', 'baba', 'bbabbaaa', 'bbbabbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'abaab', 'ababaaab']
Pos Pos Pos Pos
Counterexamples
['abab', 'ababab', 'abababab']
Pos Pos Pos
Train Epoch 0, Loss 0.7992333769798279
Train Epoch 1, Loss 0.6755066514015198
Train Epoch 2, Loss 0.5790965557098389
Accuracy at epoch 181: 0.943359375, total training samples: 483
Epoch: 182
Negative Examples
['aaaa', 'aaaaaa', 'babaa', 'babbaa', 'babbb', 'bbaa', 'bbabbb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabaaab', 'aaabab', 'aabab', 'aabbab', 'abaabab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbab']
Neg
Train Epoch 0, Loss 0.7808086276054382
Train Epoch 1, Loss 0.675811767578125
Train Epoch 2, Loss 0.5805089473724365
Accuracy at epoch 182: 0.849609375, total training samples: 484
Epoch: 183
Negative Examples
['aaba', 'abaabaab', 'ababaab', 'abba', 'abbaaa', 'abbbaba', 'baaa', 'babbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aababab', 'abaaaaab', 'abaaabab', 'ababab', 'abababab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abaabaab', 'ababaab']
Pos Pos
Train Epoch 0, Loss 0.7203372716903687
Train Epoch 1, Loss 0.6370851993560791
Train Epoch 2, Loss 0.561598539352417
Accuracy at epoch 183: 0.869140625, total training samples: 486
Epoch: 184
Negative Examples
['aababa', 'abaa', 'abaabbba', 'baababb', 'bbba', 'bbbba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaaabab', 'aaab', 'aaabaab', 'aaabb', 'aabaabab', 'aabab', 'abaab', 'ababab', 'baabaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabb', 'baabaaab']
Neg Neg
Train Epoch 0, Loss 0.9149075746536255
Train Epoch 1, Loss 0.8232221007347107
Train Epoch 2, Loss 0.7392277717590332
Accuracy at epoch 184: 0.83984375, total training samples: 488
Epoch: 185
Negative Examples
['aabbaba', 'aabbbaaa', 'abab', 'abbb', 'abbbaaa', 'bababaa', 'bbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaab']
Pos Pos Pos
Counterexamples
['abab']
Pos
Train Epoch 0, Loss 0.7195324301719666
Train Epoch 1, Loss 0.6399539709091187
Train Epoch 2, Loss 0.5647293925285339
Accuracy at epoch 185: 0.896484375, total training samples: 489
Epoch: 186
Negative Examples
['aaaaa', 'aaaaabb', 'aabaabb', 'aabb', 'abaaabb', 'abaabb', 'baba', 'bbbbaba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aabaab', 'aababab', 'abaaaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 186: No counterexamples found, skipped.
Accuracy at epoch 186: 0.87109375, total training samples: 489
Epoch: 187
Negative Examples
['aabb', 'abaaaaa', 'abba', 'baabbabb', 'babaa', 'bbbb', 'bbbbaaaa', 'bbbbbbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaab', 'aaab', 'aabaaab', 'abab', 'ababab', 'abbbaaaa']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbbaaaa']
Neg
Train Epoch 0, Loss 0.699821949005127
Train Epoch 1, Loss 0.6006729602813721
Train Epoch 2, Loss 0.5084205269813538
Accuracy at epoch 187: 0.939453125, total training samples: 490
Epoch: 188
Negative Examples
['aaba', 'aabb', 'abbabbbb', 'abbb', 'baabaa', 'babaaaa', 'babbbb', 'bbabbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaaab', 'aaab', 'abaaab', 'abab', 'ababaaab', 'abababab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 188: No counterexamples found, skipped.
Accuracy at epoch 188: 0.94140625, total training samples: 490
Epoch: 189
Negative Examples
['aabbb', 'abaaa', 'abbb', 'abbbbbba', 'baabb', 'bbaaa']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaab', 'aabaab', 'aabab', 'abaab', 'abaabbab', 'abab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaabbab', 'bbab']
Neg Neg
Train Epoch 0, Loss 0.744184136390686
Train Epoch 1, Loss 0.6505683064460754
Train Epoch 2, Loss 0.5676220655441284
Accuracy at epoch 189: 0.759765625, total training samples: 492
Epoch: 190
Negative Examples
['aaabab', 'aababaab', 'aabbb', 'aabbbb', 'abab', 'baab', 'baabaaba', 'baabbba', 'bababab', 'babbb', 'babbbba', 'bbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabaab', 'aaab', 'aabaaaab']
Pos Pos Pos Pos
Counterexamples
['aaabab', 'aababaab', 'abab']
Pos Pos Pos
Train Epoch 0, Loss 0.741659939289093
Train Epoch 1, Loss 0.6568816900253296
Train Epoch 2, Loss 0.5960037708282471
Accuracy at epoch 190: 0.953125, total training samples: 495
Epoch: 191
Negative Examples
['aaaaaa', 'aabaaba', 'abaaabb', 'ababbbb', 'babababa', 'bababb', 'bbbabbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabbab', 'aaab', 'abaaaab', 'abaabab', 'abab', 'ababaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaabbab']
Neg
Train Epoch 0, Loss 0.8344211578369141
Train Epoch 1, Loss 0.718636691570282
Train Epoch 2, Loss 0.6055931448936462
Accuracy at epoch 191: 0.783203125, total training samples: 496
Epoch: 192
Negative Examples
['aaaabab', 'abaabbba', 'abababab', 'abbba', 'baaa', 'bbaabab', 'bbababb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaab', 'aabab', 'abaaaaab', 'ababaaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaabab', 'abababab']
Pos Pos
Train Epoch 0, Loss 0.7521631121635437
Train Epoch 1, Loss 0.6553510427474976
Train Epoch 2, Loss 0.5774869918823242
Accuracy at epoch 192: 0.904296875, total training samples: 498
Epoch: 193
Negative Examples
['aaaa', 'aaabbb', 'aababba', 'abaa', 'abaab', 'abbbb', 'baab', 'baabba', 'bbaaa', 'bbabbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aababab', 'abaaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos
Counterexamples
['abaab']
Pos
Train Epoch 0, Loss 0.7100087404251099
Train Epoch 1, Loss 0.6195594072341919
Train Epoch 2, Loss 0.5411551594734192
Accuracy at epoch 193: 0.87890625, total training samples: 499
Epoch: 194
Negative Examples
['abaa', 'abbbaa']
Neg Neg
Positive Examples
['aaaaa', 'aaab', 'aabab', 'abaab', 'abaabaab', 'abab', 'abababab', 'babaaabb', 'bbbbbaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaa', 'babaaabb', 'bbbbbaab']
Neg Neg Neg
Train Epoch 0, Loss 0.7711100578308105
Train Epoch 1, Loss 0.6817514896392822
Train Epoch 2, Loss 0.6132119297981262
Accuracy at epoch 194: 0.9453125, total training samples: 502
Epoch: 195
Negative Examples
['aaaaa', 'aaabaabb', 'aabaaaa', 'abaaaaba', 'baaaa', 'baaabbbb', 'baabbb', 'baabbbb', 'babb', 'bbaa', 'bbaababb', 'bbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'ababab', 'abbaaaab']
Pos Pos Pos
Counterexamples
['abbaaaab']
Neg
Train Epoch 0, Loss 0.8063591718673706
Train Epoch 1, Loss 0.7107400298118591
Train Epoch 2, Loss 0.6327094435691833
Accuracy at epoch 195: 0.865234375, total training samples: 503
Epoch: 196
Negative Examples
['aabbaab', 'abaa', 'abaabb', 'ababbaa', 'abbb', 'baab', 'babaaa', 'babbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaaab', 'aaab', 'aaababab', 'aababab', 'abab', 'ababab', 'babab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['babab']
Neg
Train Epoch 0, Loss 0.7290281653404236
Train Epoch 1, Loss 0.6265590190887451
Train Epoch 2, Loss 0.5417680144309998
Accuracy at epoch 196: 0.5234375, total training samples: 504
Epoch: 197
Negative Examples
['aaaa', 'aaaabba', 'aaabab', 'aabaaab', 'aabb', 'abaab', 'abab', 'abababa', 'ababb', 'abbbba', 'baaaba', 'baab', 'bbaab', 'bbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab']
Pos
Counterexamples
['aaabab', 'aabaaab', 'abaab', 'abab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.8021056056022644
Train Epoch 1, Loss 0.7074937224388123
Train Epoch 2, Loss 0.6470848321914673
Accuracy at epoch 197: 0.966796875, total training samples: 508
Epoch: 198
Negative Examples
['aaabaaa', 'aabbabb', 'abbaaa', 'abbb', 'babbabb', 'bbaabba', 'bbbaaa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaababab', 'aabaaab', 'aabaab', 'aabab', 'abaaab', 'abaaabab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 198: No counterexamples found, skipped.
Accuracy at epoch 198: 0.978515625, total training samples: 508
Epoch: 199
Negative Examples
['abaabaa', 'abbabaaa', 'baaa', 'bbbabab']
Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaabab', 'aaaab', 'aaaabab', 'aaab', 'aaabab', 'aabaabab', 'aababaab', 'abaab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 199: No counterexamples found, skipped.
Accuracy at epoch 199: 0.982421875, total training samples: 508
Epoch: 200
Negative Examples
['aaabaaba', 'aabba', 'aabbaa', 'abbbaaab', 'abbbba', 'bbaa', 'bbbbbbab']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaabab', 'aaabab', 'aabaab', 'aabaabab', 'abaaab', 'abaaabab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 200: No counterexamples found, skipped.
Accuracy at epoch 200: 0.966796875, total training samples: 508
Epoch: 201
Negative Examples
['aaaabaa', 'aabba', 'aabbbbb', 'abaaa', 'ababaab', 'babaa', 'babbbbaa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aaaabab', 'aaab', 'aaabab', 'abab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['ababaab']
Pos
Train Epoch 0, Loss 0.7136468887329102
Train Epoch 1, Loss 0.6060024499893188
Train Epoch 2, Loss 0.5350624322891235
Accuracy at epoch 201: 0.9296875, total training samples: 509
Epoch: 202
Negative Examples
['aaaabbb', 'ababb', 'ababba', 'baaabb', 'babba', 'babbaaaa', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaab', 'aaaabaab', 'aaababab', 'aababaab', 'abaab', 'abaabab', 'bababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['bababab']
Neg
Train Epoch 0, Loss 0.7802629470825195
Train Epoch 1, Loss 0.628153383731842
Train Epoch 2, Loss 0.5336467027664185
Accuracy at epoch 202: 0.875, total training samples: 510
Epoch: 203
Negative Examples
['aaaaa', 'ababab', 'ababb', 'abbb', 'abbbbbb', 'baaa', 'baaba', 'baabbbab', 'babb', 'babbaaa', 'bbaa', 'bbaab', 'bbbaaaa', 'bbbaaabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab']
Pos
Counterexamples
['ababab']
Pos
Train Epoch 0, Loss 0.7945488691329956
Train Epoch 1, Loss 0.6645810008049011
Train Epoch 2, Loss 0.59050053358078
Accuracy at epoch 203: 0.93359375, total training samples: 511
Epoch: 204
Negative Examples
['aaaabaaa', 'aababa', 'abaabaa', 'baaababa', 'babba', 'bbbaa', 'bbbabaa', 'bbbabbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabab', 'aaababab', 'ababaaab', 'ababab', 'bbaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['bbaab']
Neg
Train Epoch 0, Loss 0.7198387980461121
Train Epoch 1, Loss 0.6172354817390442
Train Epoch 2, Loss 0.5309200882911682
Accuracy at epoch 204: 0.947265625, total training samples: 512
Epoch: 205
Negative Examples
['aaababba', 'aaabbba', 'baaabaa', 'babaab', 'babba', 'bbabbaa', 'bbba', 'bbbabba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabaab', 'aaab', 'aaabab', 'aabaaabb', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabaaabb']
Neg
Train Epoch 0, Loss 0.7099565863609314
Train Epoch 1, Loss 0.5952665209770203
Train Epoch 2, Loss 0.5007582902908325
Accuracy at epoch 205: 0.498046875, total training samples: 513
Epoch: 206
Negative Examples
['aaaab', 'aaaabab', 'aaba', 'aabaabab', 'aabab', 'aababab', 'abaa', 'abaaaab', 'ababab', 'ababbbba', 'baaa', 'baaba', 'babaaaba', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaab', 'aaaabab', 'aabaabab', 'aabab', 'aababab', 'abaaaab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.8616275191307068
Train Epoch 1, Loss 0.7215786576271057
Train Epoch 2, Loss 0.6041887402534485
Accuracy at epoch 206: 0.962890625, total training samples: 520
Epoch: 207
Negative Examples
['aaaaaa', 'aabaa', 'aabb', 'abbabbba', 'baaaaaaa', 'babaaba', 'bababbbb', 'babbbab', 'bbaaa', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaabab', 'abaaaaab', 'abaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 207: No counterexamples found, skipped.
Accuracy at epoch 207: 0.96484375, total training samples: 520
Epoch: 208
Negative Examples
['aabbabb', 'abaaaba', 'ababaa', 'abbaba', 'abbbabbb', 'bbba', 'bbbaaa', 'bbbaba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabaab', 'aaabab', 'aabaaab', 'aabab', 'abaaab', 'abaabab', 'abab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 208: No counterexamples found, skipped.
Accuracy at epoch 208: 0.974609375, total training samples: 520
Epoch: 209
Negative Examples
['aaba', 'abaabbb', 'abba', 'babaaaa', 'bababb']
Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aababab', 'abaaab', 'abaaabab', 'abaabab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 209: No counterexamples found, skipped.
Accuracy at epoch 209: 0.97265625, total training samples: 520
Epoch: 210
Negative Examples
['aaaaa', 'babaaa', 'babaabb', 'babaabbb', 'bbbbb']
Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabaab', 'aabaabab', 'aabab', 'aababaab', 'abaaab', 'abaaabab', 'abaab', 'abab', 'abbab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbab']
Neg
Train Epoch 0, Loss 0.7185855507850647
Train Epoch 1, Loss 0.5946655869483948
Train Epoch 2, Loss 0.4995589554309845
Accuracy at epoch 210: 0.826171875, total training samples: 521
Epoch: 211
Negative Examples
['aaabba', 'aaba', 'aabaab', 'abaabab', 'abab', 'baababba', 'bbab', 'bbbbaaba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabaab', 'aaababab', 'aabaaaab', 'aabaaab', 'abaaab', 'baaaaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabaab', 'abaabab', 'abab', 'baaaaaab']
Pos Pos Pos Neg
Train Epoch 0, Loss 0.799771785736084
Train Epoch 1, Loss 0.7283180952072144
Train Epoch 2, Loss 0.6789677143096924
Accuracy at epoch 211: 0.67578125, total training samples: 525
Epoch: 212
Negative Examples
['aaaabaab', 'aaab', 'aaabbb', 'aaba', 'aabba', 'aabbba', 'abaababa', 'baabbbaa', 'bbaaaaaa', 'bbaabaaa', 'bbabb', 'bbbaaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabaaab', 'aabab', 'baabab']
Pos Pos Pos
Counterexamples
['aaaabaab', 'aaab', 'baabab']
Pos Pos Neg
Train Epoch 0, Loss 0.7415412068367004
Train Epoch 1, Loss 0.6583496928215027
Train Epoch 2, Loss 0.5721781253814697
Accuracy at epoch 212: 0.802734375, total training samples: 528
Epoch: 213
Negative Examples
['aababbb', 'aabbaaba', 'abaaa', 'abbaaaa', 'abbba', 'baab', 'baabbab', 'babab', 'babbabab', 'babbbab', 'bbaaa', 'bbaaabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaab', 'aaabab', 'aabab']
Pos Pos Pos Pos
Counterexamples
[]

Round 213: No counterexamples found, skipped.
Accuracy at epoch 213: 0.787109375, total training samples: 528
Epoch: 214
Negative Examples
['aababaab', 'abaabaab', 'ababaaab', 'abbb', 'bababa', 'babbbbaa', 'bbaaa', 'bbaabaaa', 'bbbabaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabaab', 'aaabab', 'aabaab', 'aabab', 'abaaabab']
Pos Pos Pos Pos Pos
Counterexamples
['aababaab', 'abaabaab', 'ababaaab']
Pos Pos Pos
Train Epoch 0, Loss 0.8970990180969238
Train Epoch 1, Loss 0.6403123736381531
Train Epoch 2, Loss 0.5621636509895325
Accuracy at epoch 214: 0.916015625, total training samples: 531
Epoch: 215
Negative Examples
['aaaaba', 'aabbaba', 'ababaaba', 'abbbbb', 'baaaa', 'bbaa']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabaaab', 'aabaab', 'aabab', 'abaab', 'abababab', 'bbaaaaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['bbaaaaab']
Neg
Train Epoch 0, Loss 0.8327275514602661
Train Epoch 1, Loss 0.6519535183906555
Train Epoch 2, Loss 0.4488532543182373
Accuracy at epoch 215: 0.70703125, total training samples: 532
Epoch: 216
Negative Examples
['aaab', 'aabaab', 'aababbb', 'aabb', 'abab', 'ababaab', 'ababab', 'abababab', 'abbbba', 'baabaab', 'baabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab']
Pos
Counterexamples
['aaab', 'aabaab', 'abab', 'ababaab', 'ababab', 'abababab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.805842399597168
Train Epoch 1, Loss 0.7057873606681824
Train Epoch 2, Loss 0.6203967928886414
Accuracy at epoch 216: 0.85546875, total training samples: 538
Epoch: 217
Negative Examples
['aaaab', 'aabbabaa', 'aabbb', 'abaab', 'bbabbbab', 'bbbb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabaab', 'aaabbabb', 'aabaaaab', 'aabab', 'aababaab', 'abbab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaab', 'abaab', 'aaabbabb', 'abbab']
Pos Pos Neg Neg
Train Epoch 0, Loss 0.7113403081893921
Train Epoch 1, Loss 0.6692190170288086
Train Epoch 2, Loss 0.6248835921287537
Accuracy at epoch 217: 0.896484375, total training samples: 542
Epoch: 218
Negative Examples
['aababbab', 'abaa', 'ababba', 'ababbb', 'abbaa', 'baaa', 'baabaaba', 'babbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabbaa', 'aabaaaab', 'abaabab', 'ababab']
Pos Pos Pos Pos Pos
Counterexamples
['aaabbaa']
Neg
Train Epoch 0, Loss 0.7043352723121643
Train Epoch 1, Loss 0.5449236631393433
Train Epoch 2, Loss 0.40760254859924316
Accuracy at epoch 218: 0.724609375, total training samples: 543
Epoch: 219
Negative Examples
['abaaabab', 'abaab', 'abaabba', 'abab', 'ababab', 'abbba', 'abbbabba', 'abbbbb', 'baaaabb', 'babbab', 'babbbabb', 'bbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab']
Pos
Counterexamples
['abaaabab', 'abaab', 'abab', 'ababab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.8481842279434204
Train Epoch 1, Loss 0.7121586799621582
Train Epoch 2, Loss 0.5978595018386841
Accuracy at epoch 219: 0.919921875, total training samples: 547
Epoch: 220
Negative Examples
['aababa', 'aabb', 'aabbbb', 'aabbbbab', 'abbbbb', 'babb', 'babbaab', 'bbabaaab', 'bbbaaab', 'bbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aaabaab', 'aababab', 'abababab']
Pos Pos Pos Pos Pos
Counterexamples
[]

Round 220: No counterexamples found, skipped.
Accuracy at epoch 220: 0.923828125, total training samples: 547
Epoch: 221
Negative Examples
['aaba', 'aabaa', 'abaaa', 'abaab', 'abbaba', 'abbb', 'abbbb', 'baababb', 'babaaa', 'bbbbabba', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabaab', 'aabab', 'baaabab']
Pos Pos Pos Pos
Counterexamples
['abaab', 'baaabab']
Pos Neg
Train Epoch 0, Loss 0.8756912350654602
Train Epoch 1, Loss 0.7749923467636108
Train Epoch 2, Loss 0.6890099048614502
Accuracy at epoch 221: 0.888671875, total training samples: 549
Epoch: 222
Negative Examples
['abaa', 'abaabab', 'ababaaab', 'abba', 'abbaa', 'abbba', 'abbbbbba', 'babaabaa', 'bbaa', 'bbbbabbb', 'bbbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aabaaaab', 'abab']
Pos Pos Pos
Counterexamples
['abaabab', 'ababaaab']
Pos Pos
Train Epoch 0, Loss 0.7322595119476318
Train Epoch 1, Loss 0.5978888869285583
Train Epoch 2, Loss 0.530276358127594
Accuracy at epoch 222: 0.90625, total training samples: 551
Epoch: 223
Negative Examples
['aababa', 'aabbabaa', 'abbbba', 'baba', 'babbbba']
Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaabaab', 'aaabab', 'aabaab', 'aababaab', 'aababab', 'aabbaaab', 'babaaaab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbaaab', 'babaaaab', 'bbab']
Neg Neg Neg
Train Epoch 0, Loss 0.8309047222137451
Train Epoch 1, Loss 0.7238442301750183
Train Epoch 2, Loss 0.6325781345367432
Accuracy at epoch 223: 0.478515625, total training samples: 554
Epoch: 224
Negative Examples
['aaaa', 'aaaaabb', 'aaab', 'aaababab', 'aaba', 'aabab', 'abaaab', 'abaab', 'abab', 'ababab', 'abbb', 'baaaabba', 'baaab', 'bbbaaba', 'bbbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaab', 'aaababab', 'aabab', 'abaaab', 'abaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.8185335993766785
Train Epoch 1, Loss 0.6810140609741211
Train Epoch 2, Loss 0.5656812787055969
Accuracy at epoch 224: 0.853515625, total training samples: 561
Epoch: 225
Negative Examples
['aaabaab', 'aaba', 'abaabaaa', 'abaababb', 'abbbbaa', 'babaaa', 'babbbb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaaab', 'aabaaab', 'aabaabab', 'abaaaaab', 'abaaab', 'abab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabaab']
Pos
Train Epoch 0, Loss 0.6955442428588867
Train Epoch 1, Loss 0.5827851295471191
Train Epoch 2, Loss 0.4879538416862488
Accuracy at epoch 225: 0.916015625, total training samples: 562
Epoch: 226
Negative Examples
['aaabaaa', 'aabbba', 'abbababa', 'baababba', 'baabbba', 'babb', 'babbbb', 'bbaaaaa']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaab', 'aaabaab', 'aabaab', 'abab', 'ababaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 226: No counterexamples found, skipped.
Accuracy at epoch 226: 0.90234375, total training samples: 562
Epoch: 227
Negative Examples
['aabbaaa', 'abaa', 'ababbabb']
Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaababab', 'aabab', 'aababab', 'abaab', 'abab', 'ababaab', 'abababab', 'baab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baab']
Neg
Train Epoch 0, Loss 0.8492205739021301
Train Epoch 1, Loss 0.6958010196685791
Train Epoch 2, Loss 0.5764572620391846
Accuracy at epoch 227: 0.8828125, total training samples: 563
Epoch: 228
Negative Examples
['aaba', 'aabbaaaa', 'aabbbaab', 'abaaba', 'abbaaaa', 'abbabab', 'baaa', 'bababa']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabaab', 'aaabaab', 'aabaab', 'aabaabab', 'aabab', 'abaaaaab', 'abaaabab', 'abab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 228: No counterexamples found, skipped.
Accuracy at epoch 228: 0.888671875, total training samples: 563
Epoch: 229
Negative Examples
['aaba', 'aabbbbaa', 'abaab', 'abababab', 'abbaba', 'baaa', 'babab', 'babbb', 'bbabbaaa', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaab', 'aaabab', 'aabaaaab', 'abab']
Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'abababab']
Pos Pos
Train Epoch 0, Loss 0.8028844594955444
Train Epoch 1, Loss 0.6609019041061401
Train Epoch 2, Loss 0.53224778175354
Accuracy at epoch 229: 0.8828125, total training samples: 565
Epoch: 230
Negative Examples
['aaaba', 'aababba', 'abaa', 'abbbbb', 'baba', 'bababb', 'babbbb', 'bbaabbba', 'bbbaabaa', 'bbbbabaa', 'bbbbb', 'bbbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aabab', 'abaabab']
Pos Pos Pos Pos
Counterexamples
[]

Round 230: No counterexamples found, skipped.
Accuracy at epoch 230: 0.873046875, total training samples: 565
Epoch: 231
Negative Examples
['abaabaaa', 'baaba', 'baabbbb', 'bbbabb', 'bbbb']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaaab', 'aaab', 'aabaa', 'aabaab', 'aabab', 'aababab', 'abaab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabaa']
Neg
Train Epoch 0, Loss 0.7221662998199463
Train Epoch 1, Loss 0.6070892214775085
Train Epoch 2, Loss 0.5092623233795166
Accuracy at epoch 231: 0.890625, total training samples: 566
Epoch: 232
Negative Examples
['aabaa', 'abbabb', 'abbbaaa', 'baaa', 'babbaaba', 'bbbaab', 'bbbbbaaa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabaab', 'aaaabab', 'aaabab', 'aabbbaab', 'abaaaab', 'abaaabab', 'ababaab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbbaab']
Neg
Train Epoch 0, Loss 0.8672842979431152
Train Epoch 1, Loss 0.7192696928977966
Train Epoch 2, Loss 0.5955333709716797
Accuracy at epoch 232: 0.70703125, total training samples: 567
Epoch: 233
Negative Examples
['aaaab', 'aaabba', 'aaba', 'abaab', 'ababbba', 'abbaab', 'abbbaab', 'abbbabb', 'baab', 'babaa', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabab', 'abaaaaab', 'abab', 'ababab', 'abababab']
Pos Pos Pos Pos Pos
Counterexamples
['aaaab', 'abaab']
Pos Pos
Train Epoch 0, Loss 0.8055943250656128
Train Epoch 1, Loss 0.6898194551467896
Train Epoch 2, Loss 0.5926420092582703
Accuracy at epoch 233: 0.9375, total training samples: 569
Epoch: 234
Negative Examples
['abababaa', 'bbabaaa']
Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaaaabab', 'aaaabaab', 'aaabab', 'aabaaab', 'abaaaab', 'abaab', 'abaabaab', 'abaabab', 'ababab', 'baabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baabab']
Neg
Train Epoch 0, Loss 0.7275503277778625
Train Epoch 1, Loss 0.5931203365325928
Train Epoch 2, Loss 0.5079054832458496
Accuracy at epoch 234: 0.810546875, total training samples: 570
Epoch: 235
Negative Examples
['aababaa', 'abaaba', 'babba', 'bbaaabba', 'bbaabbb', 'bbbab', 'bbbabba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaaab', 'aaaabab', 'aaab', 'aaabaab', 'aababab', 'abaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 235: No counterexamples found, skipped.
Accuracy at epoch 235: 0.794921875, total training samples: 570
Epoch: 236
Negative Examples
['aababaab', 'aabb', 'abaaabb', 'abaab', 'abab', 'ababaab', 'ababb', 'abbaaaaa', 'baaababb', 'baabaa', 'baabb', 'babaa', 'bbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaabab', 'aabab']
Pos Pos Pos
Counterexamples
['aababaab', 'abaab', 'abab', 'ababaab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.8511370420455933
Train Epoch 1, Loss 0.7353758811950684
Train Epoch 2, Loss 0.6300103664398193
Accuracy at epoch 236: 0.916015625, total training samples: 574
Epoch: 237
Negative Examples
['abbb', 'abbbaba', 'baabaaa', 'bababaa', 'babb', 'bbaabbaa']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aabab', 'abaaaab', 'abaaabab', 'abaab', 'abab', 'abababab', 'bbaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['bbaab']
Neg
Train Epoch 0, Loss 0.914074182510376
Train Epoch 1, Loss 0.7611953020095825
Train Epoch 2, Loss 0.6403734087944031
Accuracy at epoch 237: 0.884765625, total training samples: 575
Epoch: 238
Negative Examples
['aabbaaaa', 'ababaab', 'abbaab', 'abbb', 'abbbbbb', 'baaabb', 'babbbaab', 'bbababb', 'bbabbaa', 'bbbaa', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaabab', 'aaab', 'aabaaab', 'baaab']
Pos Pos Pos Pos Pos
Counterexamples
['ababaab', 'baaab']
Pos Neg
Train Epoch 0, Loss 0.7416952252388
Train Epoch 1, Loss 0.6683567762374878
Train Epoch 2, Loss 0.6316361427307129
Accuracy at epoch 238: 0.876953125, total training samples: 577
Epoch: 239
Negative Examples
['aaaba', 'aabab', 'abab', 'ababb', 'babab', 'babbbaaa', 'bbaabbbb', 'bbbab', 'bbbbabaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabab', 'aaababab', 'aababaab']
Pos Pos Pos Pos Pos
Counterexamples
['aabab', 'abab']
Pos Pos
Train Epoch 0, Loss 0.7861332893371582
Train Epoch 1, Loss 0.6783956289291382
Train Epoch 2, Loss 0.581016480922699
Accuracy at epoch 239: 0.91015625, total training samples: 579
Epoch: 240
Negative Examples
['aabbba', 'abababa', 'baab', 'babbbaa', 'bbaab', 'bbbb', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaababab', 'abaaaab', 'abaab', 'abaabaab', 'abaabab', 'abbabab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbabab']
Neg
Train Epoch 0, Loss 0.7128257155418396
Train Epoch 1, Loss 0.4935896694660187
Train Epoch 2, Loss 0.42091935873031616
Accuracy at epoch 240: 0.8125, total training samples: 580
Epoch: 241
Negative Examples
['aababaab', 'aabbb', 'abbaab', 'abbbbabb', 'baabaa', 'bbaba', 'bbbab', 'bbbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaaabaab', 'aaabab', 'abaaab', 'abaab', 'abaabaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aababaab']
Pos
Train Epoch 0, Loss 0.7749103903770447
Train Epoch 1, Loss 0.45891252160072327
Train Epoch 2, Loss 0.3964165151119232
Accuracy at epoch 241: 0.869140625, total training samples: 581
Epoch: 242
Negative Examples
['aaba', 'abababaa', 'baaa', 'baabb', 'babb']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaaaabab', 'aaabaaab', 'aaabab', 'aababab', 'abaaab', 'ababaab', 'baaaaa']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaaaa']
Neg
Train Epoch 0, Loss 0.6959006786346436
Train Epoch 1, Loss 0.5813637375831604
Train Epoch 2, Loss 0.46093952655792236
Accuracy at epoch 242: 0.892578125, total training samples: 582
Epoch: 243
Negative Examples
['aaaabaa', 'aabaaaa', 'abab', 'abbabbaa', 'bbabb', 'bbabba', 'bbabbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaababab', 'aabaab', 'abaaab', 'ababaaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abab']
Pos
Train Epoch 0, Loss 0.7424155473709106
Train Epoch 1, Loss 0.6334189772605896
Train Epoch 2, Loss 0.5342224836349487
Accuracy at epoch 243: 0.822265625, total training samples: 583
Epoch: 244
Negative Examples
['aabaa', 'abbaba', 'babbaaaa', 'bbaaa', 'bbabbaa']
Neg Neg Neg Neg Neg
Positive Examples
['aaaababb', 'aaab', 'aaababab', 'aabaab', 'abaaab', 'abaab', 'ababaaab', 'babaaab', 'bbaaab', 'bbbabbb']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaababb', 'babaaab', 'bbaaab', 'bbbabbb']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.847144365310669
Train Epoch 1, Loss 0.7172971367835999
Train Epoch 2, Loss 0.6380451321601868
Accuracy at epoch 244: 0.947265625, total training samples: 587
Epoch: 245
Negative Examples
['abaabaa', 'abbbbb', 'bababa', 'bbabaaaa', 'bbbabaab']
Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabab', 'aabaaaab', 'aabab', 'abaaaabb', 'abaab', 'abaabaab', 'abaabbab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaaaabb', 'abaabbab']
Neg Neg
Train Epoch 0, Loss 0.9271862506866455
Train Epoch 1, Loss 0.8228942155838013
Train Epoch 2, Loss 0.7189522981643677
Accuracy at epoch 245: 0.9296875, total training samples: 589
Epoch: 246
Negative Examples
['aabbaa', 'aabbbba', 'abba', 'baaba', 'babaa', 'bbab', 'bbabbaaa', 'bbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaabab', 'aaab', 'aaabaaab', 'aaababab', 'aabaaab', 'baaaaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaaaaab']
Neg
Train Epoch 0, Loss 0.7002100944519043
Train Epoch 1, Loss 0.5841149091720581
Train Epoch 2, Loss 0.48178809881210327
Accuracy at epoch 246: 0.4140625, total training samples: 590
Epoch: 247
Negative Examples
['aaaa', 'aaaaaa', 'aaaaaab', 'aaaab', 'aaabaaab', 'aaabaab', 'aabbaab', 'abaaaab', 'abaaabab', 'ababaaab', 'abbbb', 'babaaa', 'bbaaa', 'bbabaaa', 'bbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abbbbb']
Pos
Counterexamples
['aaaaaab', 'aaaab', 'aaabaaab', 'aaabaab', 'abaaaab', 'abaaabab', 'ababaaab', 'abbbbb']
Pos Pos Pos Pos Pos Pos Pos Neg
Train Epoch 0, Loss 0.9050099849700928
Train Epoch 1, Loss 0.7890421152114868
Train Epoch 2, Loss 0.6891788840293884
Accuracy at epoch 247: 0.650390625, total training samples: 598
Epoch: 248
Negative Examples
['aaaa', 'aaaab', 'abaab', 'abab', 'abbbab', 'baaaabbb', 'bbabb', 'bbbb', 'bbbbab', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aabaabab', 'aabab']
Pos Pos Pos
Counterexamples
['aaaab', 'abaab', 'abab']
Pos Pos Pos
Train Epoch 0, Loss 0.7587912678718567
Train Epoch 1, Loss 0.6684426665306091
Train Epoch 2, Loss 0.586142361164093
Accuracy at epoch 248: 0.96484375, total training samples: 601
Epoch: 249
Negative Examples
['abaabbb', 'babbbbb', 'bbaba', 'bbbaba', 'bbbb']
Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabaab', 'aabaab', 'aabab', 'aababaab', 'abaaaab', 'abaaab', 'abab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 249: No counterexamples found, skipped.
Accuracy at epoch 249: 0.966796875, total training samples: 601
Epoch: 250
Negative Examples
['aaaaa', 'aabb', 'bbaababa', 'bbabb', 'bbbba', 'bbbbbba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'aaab', 'aaabaab', 'aabaab', 'aabab', 'ababaab', 'baab', 'babaaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baab', 'babaaaab']
Neg Neg
Train Epoch 0, Loss 0.7738193273544312
Train Epoch 1, Loss 0.6240997314453125
Train Epoch 2, Loss 0.5005379915237427
Accuracy at epoch 250: 0.859375, total training samples: 603
Epoch: 251
Negative Examples
['aabb', 'abaabaab', 'abababba', 'abbaaaba', 'babaa', 'babaaaaa', 'babababb', 'bbaabba', 'bbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaab', 'aaabaab', 'aabab', 'aababab', 'abab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaabaab']
Pos
Train Epoch 0, Loss 0.8930038213729858
Train Epoch 1, Loss 0.6692842841148376
Train Epoch 2, Loss 0.5997341275215149
Accuracy at epoch 251: 0.953125, total training samples: 604
Epoch: 252
Negative Examples
['abaaabb', 'abba', 'abbbbab', 'baaa', 'baaaba', 'baaababa', 'bababbaa', 'bbbab', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aaabab', 'aaababab', 'aabab', 'abaaab', 'ababaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 252: No counterexamples found, skipped.
Accuracy at epoch 252: 0.9609375, total training samples: 604
Epoch: 253
Negative Examples
['aaaaa', 'babaaa', 'bbbaaaba', 'bbbabb', 'bbbbabba']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aaaabab', 'aaabab', 'aaababab', 'aabab', 'abaaaab', 'abaabab', 'ababab', 'abababab', 'abbab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbab']
Neg
Train Epoch 0, Loss 0.7322892546653748
Train Epoch 1, Loss 0.611891508102417
Train Epoch 2, Loss 0.5164963006973267
Accuracy at epoch 253: 0.853515625, total training samples: 605
Epoch: 254
Negative Examples
['aabaa', 'aabbabaa', 'aabbb', 'abab', 'abababab', 'baaaba', 'baabbab', 'bababb', 'babbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabb', 'aaab', 'abaaab']
Pos Pos Pos Pos
Counterexamples
['abab', 'abababab', 'aaaabb']
Pos Pos Neg
Train Epoch 0, Loss 0.7573127746582031
Train Epoch 1, Loss 0.6370596289634705
Train Epoch 2, Loss 0.551880419254303
Accuracy at epoch 254: 0.935546875, total training samples: 608
Epoch: 255
Negative Examples
['aaaa', 'abbabaa', 'baaaaaa', 'baaba', 'baabbba', 'bbaaba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaab', 'aaabaaab', 'aaabab', 'aababab', 'abaaaab', 'abaaab', 'abab', 'abababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 255: No counterexamples found, skipped.
Accuracy at epoch 255: 0.90625, total training samples: 608
Epoch: 256
Negative Examples
['aabaabbb', 'aababa', 'abaab', 'abbaa', 'baaabaab', 'baab', 'bbaaaba', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaab', 'aabaab', 'aabab', 'abab', 'abbaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'abbaab']
Pos Neg
Train Epoch 0, Loss 0.7325890064239502
Train Epoch 1, Loss 0.6732376217842102
Train Epoch 2, Loss 0.6155086755752563
Accuracy at epoch 256: 0.84375, total training samples: 610
Epoch: 257
Negative Examples
['aabbbaa', 'ababaab', 'abbaaba', 'abbbaa', 'abbbbaa', 'baabbbb', 'babbabab', 'bbaaaaa', 'bbaab', 'bbabbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaaab', 'aababab', 'abaaab', 'abaaabab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['ababaab']
Pos
Train Epoch 0, Loss 0.8569989204406738
Train Epoch 1, Loss 0.6118406057357788
Train Epoch 2, Loss 0.5032956600189209
Accuracy at epoch 257: 0.89453125, total training samples: 611
Epoch: 258
Negative Examples
['abababb', 'abba', 'abbba', 'babaa', 'bbbbb']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaa', 'aaab', 'aaabab', 'aabaab', 'aabab', 'abaab', 'abaabab', 'baaaaaa', 'bbaaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaa', 'baaaaaa', 'bbaaabab']
Neg Neg Neg
Train Epoch 0, Loss 0.8372378349304199
Train Epoch 1, Loss 0.7293186187744141
Train Epoch 2, Loss 0.6199248433113098
Accuracy at epoch 258: 0.82421875, total training samples: 614
Epoch: 259
Negative Examples
['aabaa', 'aabab', 'abaaaaa', 'abaaaaba', 'abab', 'baaabbb', 'bbaa', 'bbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aaabaaab', 'aabaaab', 'aababaab', 'ababaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aabab', 'abab']
Pos Pos
Train Epoch 0, Loss 0.7186692953109741
Train Epoch 1, Loss 0.6217100024223328
Train Epoch 2, Loss 0.534788966178894
Accuracy at epoch 259: 0.91015625, total training samples: 616
Epoch: 260
Negative Examples
['aaaba', 'babbba', 'bbaab', 'bbababa', 'bbabbbba']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aaaabab', 'aaabb', 'aabab', 'abaaaaab', 'ababaaab', 'baab', 'baabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabb', 'baab', 'baabab']
Neg Neg Neg
Train Epoch 0, Loss 0.8120227456092834
Train Epoch 1, Loss 0.6906034350395203
Train Epoch 2, Loss 0.5989639759063721
Accuracy at epoch 260: 0.94921875, total training samples: 619
Epoch: 261
Negative Examples
['aababaa', 'abbabaaa', 'abbb', 'baaabbaa', 'baab', 'bbbbbb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabaab', 'aaab', 'aaabab', 'aaababab', 'aabaab', 'aababab', 'abaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 261: No counterexamples found, skipped.
Accuracy at epoch 261: 0.96484375, total training samples: 619
Epoch: 262
Negative Examples
['aaaa', 'aabba', 'aabbaba', 'aabbb', 'abaa', 'ababab', 'abbba', 'abbbbabb', 'baaaa', 'bbaaa', 'bbaaab', 'bbaabaa', 'bbbbab', 'bbbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aababab']
Pos Pos
Counterexamples
['ababab']
Pos
Train Epoch 0, Loss 0.7432922124862671
Train Epoch 1, Loss 0.5605141520500183
Train Epoch 2, Loss 0.45711585879325867
Accuracy at epoch 262: 0.7890625, total training samples: 620
Epoch: 263
Negative Examples
['aaaaab', 'aabba', 'aabbbbbb', 'abaa', 'abaab', 'bbabaab']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabaaab', 'aaababab', 'aabbabab', 'abab', 'babab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaab', 'abaab', 'aabbabab', 'babab']
Pos Pos Neg Neg
Train Epoch 0, Loss 0.8735365867614746
Train Epoch 1, Loss 0.7768065333366394
Train Epoch 2, Loss 0.6744247674942017
Accuracy at epoch 263: 0.98046875, total training samples: 624
Epoch: 264
Negative Examples
['aaabbabb', 'abbbabb', 'baabbb', 'baba', 'bbbbaba']
Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabab', 'aaababab', 'aabaab', 'aababab', 'abaab', 'abab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 264: No counterexamples found, skipped.
Accuracy at epoch 264: 0.96484375, total training samples: 624
Epoch: 265
Negative Examples
['aaabbba', 'aabaa', 'ababb', 'abbaaaba', 'baaaaa', 'baabaa', 'bbaaab', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaab', 'aabab', 'aabbaab', 'abaaaaab', 'abab', 'ababaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbaab']
Neg
Train Epoch 0, Loss 0.8884724378585815
Train Epoch 1, Loss 0.6312708258628845
Train Epoch 2, Loss 0.5029411315917969
Accuracy at epoch 265: 0.66796875, total training samples: 625
Epoch: 266
Negative Examples
['aaababbb', 'aaabba', 'aababaa', 'aababab', 'abaaaab', 'abaaabab', 'abaab', 'abaabaab', 'abaabab', 'abbab', 'babb', 'bbbaaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaab']
Pos Pos
Counterexamples
['aababab', 'abaaaab', 'abaaabab', 'abaab', 'abaabaab', 'abaabab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.9362220168113708
Train Epoch 1, Loss 0.754342257976532
Train Epoch 2, Loss 0.6000680923461914
Accuracy at epoch 266: 0.9609375, total training samples: 631
Epoch: 267
Negative Examples
['aababb', 'abaa', 'abaaa', 'abaaabb', 'ababa', 'abbbaa', 'babaabb', 'bbababa']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabab', 'abaaaaab', 'abaaaab', 'abab', 'baaaabb']
Pos Pos Pos Pos Pos Pos
Counterexamples
['baaaabb']
Neg
Train Epoch 0, Loss 0.7048241496086121
Train Epoch 1, Loss 0.5964100956916809
Train Epoch 2, Loss 0.49934202432632446
Accuracy at epoch 267: 0.841796875, total training samples: 632
Epoch: 268
Negative Examples
['aabaabb', 'aababbba', 'aabbbaaa', 'ababb', 'ababbb', 'abbabba', 'babbbba', 'babbbbaa', 'bbbbaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabaaab', 'aaabab', 'abab']
Pos Pos Pos Pos Pos
Counterexamples
[]

Round 268: No counterexamples found, skipped.
Accuracy at epoch 268: 0.841796875, total training samples: 632
Epoch: 269
Negative Examples
['aaabaaa', 'aaabbba', 'aabaab', 'ababaaab', 'abbaaab', 'abbabbb', 'bbaaba', 'bbba', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabaaab', 'aabab', 'abaabab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aabaab', 'ababaaab']
Pos Pos
Train Epoch 0, Loss 0.7283680438995361
Train Epoch 1, Loss 0.6243247389793396
Train Epoch 2, Loss 0.5475274920463562
Accuracy at epoch 269: 0.939453125, total training samples: 634
Epoch: 270
Negative Examples
['aaaababa', 'ababaa', 'abbabb', 'baababba']
Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aaabaaab', 'aaababab', 'aabaaab', 'aabaab', 'aabab', 'abaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 270: No counterexamples found, skipped.
Accuracy at epoch 270: 0.95703125, total training samples: 634
Epoch: 271
Negative Examples
['aaaba', 'aaabbabb', 'aabb', 'abaaaa', 'abaaabb', 'abbababa', 'abbbabb', 'babbb', 'babbbbba', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aaab', 'aaabab', 'abaab']
Pos Pos Pos Pos Pos
Counterexamples
[]

Round 271: No counterexamples found, skipped.
Accuracy at epoch 271: 0.94140625, total training samples: 634
Epoch: 272
Negative Examples
['aaba', 'aabaa', 'ababbba', 'abbaaa', 'abbbbab', 'bbaaa', 'bbabb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaab', 'aaabab', 'aaababab', 'aabaabab', 'aabab', 'aababab', 'abab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 272: No counterexamples found, skipped.
Accuracy at epoch 272: 0.955078125, total training samples: 634
Epoch: 273
Negative Examples
['aabba', 'abaaa', 'abbbbbba', 'baaa', 'babbbba', 'bbbba', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aaabab', 'aabaab', 'abaaab', 'abaab', 'abab', 'ababaaab', 'babaaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['babaaaab']
Neg
Train Epoch 0, Loss 0.8565470576286316
Train Epoch 1, Loss 0.7172729969024658
Train Epoch 2, Loss 0.552887499332428
Accuracy at epoch 273: 0.85546875, total training samples: 635
Epoch: 274
Negative Examples
['aaaaab', 'aaabbaba', 'aabaaaab', 'aabb', 'aabbb', 'ababaa', 'abba', 'abbb', 'baaaaabb', 'bbaba', 'bbba', 'bbbabaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaabaab', 'aaabab', 'aabab']
Pos Pos Pos Pos
Counterexamples
['aaaaab', 'aabaaaab']
Pos Pos
Train Epoch 0, Loss 0.7316392064094543
Train Epoch 1, Loss 0.6281598806381226
Train Epoch 2, Loss 0.5482645034790039
Accuracy at epoch 274: 0.93359375, total training samples: 637
Epoch: 275
Negative Examples
['abaaa', 'abaaaa', 'ababab', 'baabb', 'babbba', 'bbbbab']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaabab', 'aaaab', 'aaaabaab', 'aaab', 'aaabab', 'aabaaab', 'aabab', 'abaaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababab']
Pos
Train Epoch 0, Loss 0.7050056457519531
Train Epoch 1, Loss 0.5207788348197937
Train Epoch 2, Loss 0.3664548993110657
Accuracy at epoch 275: 0.908203125, total training samples: 638
Epoch: 276
Negative Examples
['aabbaaaa', 'ababbbb', 'abbaaaa', 'abbb']
Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aaab', 'aaabaaab', 'aaabaab', 'aabaaaab', 'abaab', 'abaabaab', 'abab', 'baaab', 'babaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaab', 'babaaab']
Neg Neg
Train Epoch 0, Loss 1.0760865211486816
Train Epoch 1, Loss 0.8640136122703552
Train Epoch 2, Loss 0.698610782623291
Accuracy at epoch 276: 0.779296875, total training samples: 640
Epoch: 277
Negative Examples
['aaab', 'aabbbbb', 'abaab', 'ababaa', 'abbabb', 'babbb', 'babbba', 'bbaaaaa', 'bbabbaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabaaab', 'abaaab', 'abaabaab']
Pos Pos Pos
Counterexamples
['aaab', 'abaab']
Pos Pos
Train Epoch 0, Loss 0.7263495922088623
Train Epoch 1, Loss 0.6129541993141174
Train Epoch 2, Loss 0.514979898929596
Accuracy at epoch 277: 0.92578125, total training samples: 642
Epoch: 278
Negative Examples
['ababbbb', 'abbbaabb', 'bbabbba', 'bbba']
Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaab', 'aabaab', 'aabaabab', 'aabab', 'abaab', 'abab', 'abbab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbab']
Neg
Train Epoch 0, Loss 0.8081396222114563
Train Epoch 1, Loss 0.6716113686561584
Train Epoch 2, Loss 0.5653326511383057
Accuracy at epoch 278: 0.814453125, total training samples: 643
Epoch: 279
Negative Examples
['aaba', 'aababab', 'aabbbaab', 'abaa', 'abab', 'ababaaab', 'ababab', 'baabbaa', 'babba', 'bbaa', 'bbbba', 'bbbbbb', 'bbbbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaab']
Pos Pos Pos
Counterexamples
['aababab', 'abab', 'ababaaab', 'ababab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.8440354466438293
Train Epoch 1, Loss 0.6782654523849487
Train Epoch 2, Loss 0.5097165107727051
Accuracy at epoch 279: 0.94140625, total training samples: 647
Epoch: 280
Negative Examples
['aaaabaa', 'ababa', 'babb', 'bbaaabb', 'bbbaaaba', 'bbbbbb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaaabaab', 'aabaaab', 'abab', 'ababaab', 'ababaabb', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababaabb']
Neg
Train Epoch 0, Loss 0.7036769986152649
Train Epoch 1, Loss 0.5149147510528564
Train Epoch 2, Loss 0.42392683029174805
Accuracy at epoch 280: 0.8671875, total training samples: 648
Epoch: 281
Negative Examples
['aaababab', 'abaa', 'abbb', 'abbbbba', 'bbaa', 'bbba', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aabaaaab', 'abaaaab', 'abaab', 'abab', 'baaaaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aaababab', 'baaaaab']
Pos Neg
Train Epoch 0, Loss 0.8379013538360596
Train Epoch 1, Loss 0.7056734561920166
Train Epoch 2, Loss 0.6295512914657593
Accuracy at epoch 281: 0.810546875, total training samples: 650
Epoch: 282
Negative Examples
['aaaaa', 'aabbbb', 'abaab', 'bbaba', 'bbbaabab']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaab', 'aaaabab', 'aaab', 'aaabaaab', 'aabaabab', 'abaaab', 'abaabab', 'ababab', 'bbabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'bbabab']
Pos Neg
Train Epoch 0, Loss 0.7422105669975281
Train Epoch 1, Loss 0.6774536371231079
Train Epoch 2, Loss 0.6076769232749939
Accuracy at epoch 282: 0.908203125, total training samples: 652
Epoch: 283
Negative Examples
['aabaabab', 'aabb', 'aabbbbb', 'abbbab', 'bababba', 'babbbbab', 'bbbbabb']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaababab', 'abab']
Pos Pos Pos
Counterexamples
['aabaabab']
Pos
Train Epoch 0, Loss 0.7691891193389893
Train Epoch 1, Loss 0.5256053805351257
Train Epoch 2, Loss 0.43315476179122925
Accuracy at epoch 283: 0.931640625, total training samples: 653
Epoch: 284
Negative Examples
['aaaaaabb', 'aabba', 'baba', 'babbaa', 'bbabbb', 'bbabbbbb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaabab', 'aabaab', 'abaaaab', 'abaab', 'abab', 'ababaab', 'baabaab', 'bbaaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baabaab', 'bbaaabab']
Neg Neg
Train Epoch 0, Loss 0.7076150178909302
Train Epoch 1, Loss 0.6040831208229065
Train Epoch 2, Loss 0.5287004709243774
Accuracy at epoch 284: 0.94140625, total training samples: 655
Epoch: 285
Negative Examples
['aaabb', 'ababaab', 'ababb', 'abbaaba', 'abbb']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aabaaaab', 'aabaaab', 'aabaab', 'abaaaab', 'abaabaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababaab']
Pos
Train Epoch 0, Loss 0.8991208076477051
Train Epoch 1, Loss 0.6110144853591919
Train Epoch 2, Loss 0.4950169622898102
Accuracy at epoch 285: 0.947265625, total training samples: 656
Epoch: 286
Negative Examples
['aabaa', 'abaa', 'baaa', 'baababa', 'baabbab', 'bbaaabb', 'bbaabaab', 'bbab', 'bbbbaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aabaaaab', 'abaaab', 'abaabaab', 'ababab', 'ababbab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababbab']
Neg
Train Epoch 0, Loss 0.9985365867614746
Train Epoch 1, Loss 0.7848328351974487
Train Epoch 2, Loss 0.5959339141845703
Accuracy at epoch 286: 0.912109375, total training samples: 657
Epoch: 287
Negative Examples
['baaa', 'bababab']
Neg Neg
Positive Examples
['aaaaab', 'aaaabaab', 'aaaabab', 'aaab', 'aaabaab', 'aabaaab', 'abaab', 'abab', 'baaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaab']
Neg
Train Epoch 0, Loss 0.7807262539863586
Train Epoch 1, Loss 0.6174214482307434
Train Epoch 2, Loss 0.5167461633682251
Accuracy at epoch 287: 0.748046875, total training samples: 658
Epoch: 288
Negative Examples
['aabab', 'abaab', 'abab', 'ababa', 'abbabbab', 'baabaaba', 'bababab', 'bbaaabab', 'bbbba', 'bbbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaab', 'aaaabaab', 'aabaaaab']
Pos Pos Pos Pos
Counterexamples
['aabab', 'abaab', 'abab']
Pos Pos Pos
Train Epoch 0, Loss 0.8200642466545105
Train Epoch 1, Loss 0.6969773769378662
Train Epoch 2, Loss 0.6011338829994202
Accuracy at epoch 288: 0.97265625, total training samples: 661
Epoch: 289
Negative Examples
['aaaba', 'aaabaa', 'abaabb', 'abbaabb', 'abbb', 'bababab', 'bbaabbb', 'bbabaab', 'bbabbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabab', 'aaab', 'aabaaab', 'aababab', 'abaab']
Pos Pos Pos Pos Pos
Counterexamples
[]

Round 289: No counterexamples found, skipped.
Accuracy at epoch 289: 0.958984375, total training samples: 661
Epoch: 290
Negative Examples
['aaaaa', 'aaaaabba', 'aaaba', 'ababaaab', 'babb', 'bbabaaab', 'bbabb', 'bbbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaab', 'aaab', 'aabaabab', 'abaaab', 'ababab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['ababaaab']
Pos
Train Epoch 0, Loss 0.7099140882492065
Train Epoch 1, Loss 0.5478987097740173
Train Epoch 2, Loss 0.5025885701179504
Accuracy at epoch 290: 0.919921875, total training samples: 662
Epoch: 291
Negative Examples
['aaaabaaa', 'aabaa', 'abbabbaa', 'baabaa', 'baabbb', 'bbabab', 'bbabbaa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaabaaab', 'aabaabab', 'abaaab', 'abaaabab', 'abab', 'baaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaaab']
Neg
Train Epoch 0, Loss 0.8740032315254211
Train Epoch 1, Loss 0.7402744293212891
Train Epoch 2, Loss 0.6110813021659851
Accuracy at epoch 291: 0.728515625, total training samples: 663
Epoch: 292
Negative Examples
['aaaab', 'aabbba', 'abaaaaaa', 'abaaab', 'abaab', 'baaaa', 'baaaaa', 'bbabbb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aaababab', 'aababbbb', 'abaabaab', 'abab']
Pos Pos Pos Pos Pos
Counterexamples
['aaaab', 'abaaab', 'abaab', 'aababbbb']
Pos Pos Pos Neg
Train Epoch 0, Loss 0.7376450896263123
Train Epoch 1, Loss 0.6451812982559204
Train Epoch 2, Loss 0.5562003254890442
Accuracy at epoch 292: 0.9765625, total training samples: 667
Epoch: 293
Negative Examples
['aabb', 'abbbaaab', 'babaa', 'babaab', 'babbbb', 'bbabaa', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabaaab', 'aabaab', 'abaab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 293: No counterexamples found, skipped.
Accuracy at epoch 293: 0.96484375, total training samples: 667
Epoch: 294
Negative Examples
['aaaabb', 'abbaabbb', 'abbbab', 'bbaa', 'bbaaaa']
Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aabaaab', 'aabaab', 'abaaaab', 'abaab', 'abab', 'ababaab', 'abababab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 294: No counterexamples found, skipped.
Accuracy at epoch 294: 0.9765625, total training samples: 667
Epoch: 295
Negative Examples
['aaabaa', 'aabaaabb', 'abaababb', 'abbaba', 'abbbbba', 'baaa', 'babba', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaabab', 'aaabaab', 'aaababab', 'aabaab', 'aabab', 'abaabaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
[]

Round 295: No counterexamples found, skipped.
Accuracy at epoch 295: 0.974609375, total training samples: 667
Epoch: 296
Negative Examples
['aaaa', 'aabaaba', 'ababb', 'baba', 'bbabba', 'bbbaba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aabaab', 'aababaab', 'aabbabab', 'abaaaaab', 'abaaaab', 'abaaabab', 'abaab', 'abaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbabab']
Neg
Train Epoch 0, Loss 0.9338827133178711
Train Epoch 1, Loss 0.5946367383003235
Train Epoch 2, Loss 0.47856414318084717
Accuracy at epoch 296: 0.779296875, total training samples: 668
Epoch: 297
Negative Examples
['abaab', 'abaabaa', 'abab', 'bababb', 'babbbabb', 'bbab', 'bbaba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaaab', 'aaaabab', 'aaab', 'aaabab', 'aabaaaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'abab']
Pos Pos
Train Epoch 0, Loss 0.7520075440406799
Train Epoch 1, Loss 0.5866093635559082
Train Epoch 2, Loss 0.4783433675765991
Accuracy at epoch 297: 0.9296875, total training samples: 670
Epoch: 298
Negative Examples
['aaaaa', 'aaaabbaa', 'abbba', 'abbbabaa', 'babba', 'babbbba', 'bbabab', 'bbabba', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'aaab', 'aabaabab', 'aababaab', 'aababab', 'baab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baab']
Neg
Train Epoch 0, Loss 0.9588386416435242
Train Epoch 1, Loss 0.7460123300552368
Train Epoch 2, Loss 0.5904861092567444
Accuracy at epoch 298: 0.93359375, total training samples: 671
Epoch: 299
Negative Examples
['aababaab', 'abaaa', 'abaaba', 'ababa', 'babab', 'bbabbb', 'bbbabab', 'bbbababa', 'bbbabbab', 'bbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaabab', 'aaabaab', 'aaabab', 'aabaab']
Pos Pos Pos Pos Pos
Counterexamples
['aababaab']
Pos
Train Epoch 0, Loss 0.7569973468780518
Train Epoch 1, Loss 0.5460678339004517
Train Epoch 2, Loss 0.48207348585128784
Accuracy at epoch 299: 0.9140625, total training samples: 672
Pos train / Tot train = 402 / 672
