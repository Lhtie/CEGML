Train Epoch 0, Loss 0.7013667300343513
Train Epoch 1, Loss 0.6479287147521973
Train Epoch 2, Loss 0.6059842109680176
Pretrained on 128 supervised examples, Eval Acc: 0.734375
Early stopping at epoch 261. Loss did not improve for 10 epochs.
Generate examples Step 261, Loss 0.2739494807506336
Epoch: 0
Negative Examples
['abaabaca', 'aca', 'acabaca', 'b', 'bbcbb', 'bcca', 'cbabbcbb', 'cbcbbccc', 'cc', 'ccbacc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['a', 'bbaabcac', 'bbcabcba', 'bcaa']
Pos Pos Pos Pos
Counterexamples
['a', 'a', 'a', 'bbaabcac', 'cbacaabb', 'abcbabc', 'bbcabcba', 'bbbbaba', 'ccccbbb', 'bcaa', 'bccacaaa', 'bbcbccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.74224853515625
Train Epoch 1, Loss 0.6477711200714111
Train Epoch 2, Loss 0.5647561550140381
Accuracy at epoch 0: 0.6015625, total training samples: 12
Early stopping at epoch 311. Loss did not improve for 10 epochs.
Generate examples Step 311, Loss 0.29915488329835427
Epoch: 1
Negative Examples
['babacba', 'babc', 'bcbacba', 'cabbcba', 'cbbaba', 'cbbabc', 'ccbbaba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabc', 'abccba', 'ba', 'caba', 'cacbbabc', 'cbc', 'cbccba', 'ccbaaac']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabc', 'abbaaba', 'cbcaaaab', 'ba', 'abbbbbcc', 'abacbbcc', 'caba', 'aacbcacc', 'acbbbaa', 'cacbbabc', 'bcbaabbc', 'bbaababc', 'cbc', 'cccbbaca', 'aacacbcc', 'cbccba', 'acbbbcbb', 'abbcbab', 'ccbaaac', 'ccbaaaac', 'ccbbacac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.6060686409473419
Train Epoch 1, Loss 0.421470582485199
Train Epoch 2, Loss 0.26662398874759674
Accuracy at epoch 1: 0.51953125, total training samples: 33
Early stopping at epoch 463. Loss did not improve for 10 epochs.
Generate examples Step 463, Loss 0.8151216654685037
Epoch: 2
Negative Examples
['aaacabc', 'aabbaabc', 'ababbc', 'abc', 'babbc', 'bacbbc', 'bbc', 'bc', 'bcbbcbbc', 'c', 'cabc', 'cbbabbc', 'cbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 2: No counterexamples found, skipped.
Accuracy at epoch 2: 0.515625, total training samples: 33
Early stopping at epoch 387. Loss did not improve for 10 epochs.
Generate examples Step 387, Loss 0.8034701253642741
Epoch: 3
Negative Examples
['a', 'aabc', 'abac', 'ac', 'bababbc', 'bbbcbbc', 'bbc', 'bc', 'c', 'cbacabc', 'cbbabbc', 'cbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abac', 'abacac', 'abccaac']
Pos Pos Pos
Train Epoch 0, Loss 1.6631654500961304
Train Epoch 1, Loss 1.3732352256774902
Train Epoch 2, Loss 1.1416791677474976
Accuracy at epoch 3: 0.51953125, total training samples: 36
Early stopping at epoch 369. Loss did not improve for 10 epochs.
Generate examples Step 369, Loss 0.3415148331506832
Epoch: 4
Negative Examples
['aaaaaba', 'aaabc', 'aabc', 'abaacbc', 'abbbcaba', 'acbacbc', 'accbaabc', 'bba', 'bc', 'bcbba', 'bccba', 'c', 'caaabc', 'cababba', 'ccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abbbcaba', 'abcccaba', 'abaccba']
Pos Pos Pos
Train Epoch 0, Loss 1.0806787014007568
Train Epoch 1, Loss 0.897469699382782
Train Epoch 2, Loss 0.7517285346984863
Accuracy at epoch 4: 0.576171875, total training samples: 39
Early stopping at epoch 322. Loss did not improve for 10 epochs.
Generate examples Step 322, Loss 0.2706398513664033
Epoch: 5
Negative Examples
['a', 'acba', 'ba', 'babc', 'bbbbabc', 'bbcbc', 'c', 'cabbaba', 'cabbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acbc', 'baaba', 'bbacba', 'bc', 'cbc', 'cbcbaabc']
Pos Pos Pos Pos Pos Pos
Counterexamples
['acbc', 'aaccbccc', 'ccabcacc', 'bbacba', 'acbbcc', 'cbcccbaa', 'bc', 'cacccab', 'ccbabc', 'cbc', 'aabbbbcb', 'caacacbc', 'cbcbaabc', 'cccaab', 'cbaccab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5443953275680542
Train Epoch 1, Loss 0.47287335991859436
Train Epoch 2, Loss 0.40287095308303833
Accuracy at epoch 5: 0.505859375, total training samples: 54
Early stopping at epoch 272. Loss did not improve for 10 epochs.
Generate examples Step 272, Loss 0.3407461418337001
Epoch: 6
Negative Examples
['a', 'aabbc', 'aabc', 'ababbc', 'abbc', 'abcabc', 'acba', 'acbba', 'ba', 'bbabba', 'c', 'cbabbc', 'cbbbccbc', 'ccbbcaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 6: No counterexamples found, skipped.
Accuracy at epoch 6: 0.50390625, total training samples: 54
Early stopping at epoch 281. Loss did not improve for 10 epochs.
Generate examples Step 281, Loss 0.2856166863166694
Epoch: 7
Negative Examples
['a', 'aaaccbc', 'ababbbc', 'abbc', 'ba', 'bba', 'bbbc', 'bc', 'c', 'cbc', 'ccaaacba', 'ccaabba', 'ccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 7: No counterexamples found, skipped.
Accuracy at epoch 7: 0.51953125, total training samples: 54
Early stopping at epoch 293. Loss did not improve for 10 epochs.
Generate examples Step 293, Loss 0.3208148548392212
Epoch: 8
Negative Examples
['aaac', 'ababbc', 'baabba', 'babbc', 'bbababba', 'bbbcaba', 'bc', 'bcbbccbc', 'bccba', 'c', 'cacbcbbc', 'cba', 'cbba', 'cbc', 'cbccba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 8: No counterexamples found, skipped.
Accuracy at epoch 8: 0.50390625, total training samples: 54
Early stopping at epoch 368. Loss did not improve for 10 epochs.
Generate examples Step 368, Loss 0.4000179488161392
Epoch: 9
Negative Examples
['aabbabbc', 'abbc', 'abc', 'acaba', 'acbbc', 'acbbcbbc', 'ba', 'baabba', 'babba', 'bbcbba', 'bbccbc', 'bcbba', 'c', 'cbaccba', 'cbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 9: No counterexamples found, skipped.
Accuracy at epoch 9: 0.478515625, total training samples: 54
Early stopping at epoch 322. Loss did not improve for 10 epochs.
Generate examples Step 322, Loss 0.40849783453778954
Epoch: 10
Negative Examples
['aabba', 'ababbc', 'abbc', 'abcbcbba', 'baaabc', 'babba', 'babcaba', 'baccba', 'bbaabbc', 'bbaabc', 'bbacbbc', 'bcbbacbc', 'c', 'caacaba', 'ccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baccba', 'abcbbcba', 'abcacaba']
Pos Pos Pos
Train Epoch 0, Loss 1.2007819414138794
Train Epoch 1, Loss 1.010885238647461
Train Epoch 2, Loss 0.8519269824028015
Accuracy at epoch 10: 0.587890625, total training samples: 57
Early stopping at epoch 229. Loss did not improve for 10 epochs.
Generate examples Step 229, Loss 0.2674461145115935
Epoch: 11
Negative Examples
['aba', 'acacaaa', 'ba', 'bacbb', 'caabc', 'cba', 'cbbabc', 'cbc', 'cbcabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bc', 'bccbacbc']
Pos Pos
Counterexamples
['bc', 'bccbcbc', 'abbc', 'bccbacbc', 'bcacbbc', 'acbcacba']
Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7030225396156311
Train Epoch 1, Loss 0.5950246453285217
Train Epoch 2, Loss 0.4963034391403198
Accuracy at epoch 11: 0.439453125, total training samples: 63
Early stopping at epoch 425. Loss did not improve for 10 epochs.
Generate examples Step 425, Loss 0.5264887976254655
Epoch: 12
Negative Examples
['a', 'aacbbc', 'abbc', 'acbcbbc', 'accba', 'ba', 'baabbc', 'bacaba', 'bba', 'bc', 'c', 'caba', 'cbaabba', 'cbba', 'ccba', 'ccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacaba', 'abaaba', 'ababa']
Pos Pos Pos
Train Epoch 0, Loss 1.1121265888214111
Train Epoch 1, Loss 0.9447088241577148
Train Epoch 2, Loss 0.8068687915802002
Accuracy at epoch 12: 0.6015625, total training samples: 66
Early stopping at epoch 289. Loss did not improve for 10 epochs.
Generate examples Step 289, Loss 0.3020135728449657
Epoch: 13
Negative Examples
['a', 'aabbaba', 'aabc', 'abc', 'accababa', 'bbcbc', 'bc', 'bcba', 'c', 'caacba', 'cbbcba', 'cbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 13: No counterexamples found, skipped.
Accuracy at epoch 13: 0.5625, total training samples: 66
Early stopping at epoch 309. Loss did not improve for 10 epochs.
Generate examples Step 309, Loss 0.2589778254109044
Epoch: 14
Negative Examples
['a', 'aaabbaba', 'aaba', 'aabbaba', 'ababa', 'bbabc', 'bcbc', 'caacbc', 'cacbaba', 'cacbcbc', 'cba', 'cbc', 'cbcabcbc', 'ccbbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['ababa', 'abaaaaba', 'abacbbba']
Pos Pos Pos
Train Epoch 0, Loss 0.8658458590507507
Train Epoch 1, Loss 0.7465781569480896
Train Epoch 2, Loss 0.6431194543838501
Accuracy at epoch 14: 0.6484375, total training samples: 69
Early stopping at epoch 275. Loss did not improve for 10 epochs.
Generate examples Step 275, Loss 0.25497440151546313
Epoch: 15
Negative Examples
['a', 'aca', 'babb', 'bacc', 'bcbb', 'c', 'cbb', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabbbcba', 'ba', 'babcbc', 'bbcba', 'bcbbcba', 'cbabc', 'cbabcba', 'cbcba']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbbcba', 'ababcbab', 'bcbccaaa', 'ba', 'bacaabbc', 'abacaccc', 'babcbc', 'bccaacc', 'aababcab', 'bbcba', 'bcacc', 'aabbcbba', 'bcbbcba', 'bcaabbac', 'cbbbaaab', 'cbabc', 'cacaabb', 'cacbccba', 'cbabcba', 'bbabcabb', 'bbcbcbbc', 'cbcba', 'aaba', 'cabcbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7023542523384094
Train Epoch 1, Loss 0.5560535192489624
Train Epoch 2, Loss 0.4247332215309143
Accuracy at epoch 15: 0.490234375, total training samples: 93
Early stopping at epoch 278. Loss did not improve for 10 epochs.
Generate examples Step 278, Loss 0.28701514838844217
Epoch: 16
Negative Examples
['a', 'aacacbc', 'acaba', 'accaaabc', 'ba', 'baaaabc', 'babc', 'babcba', 'bba', 'bbaaba', 'bbccba', 'bc', 'caba', 'ccaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 16: No counterexamples found, skipped.
Accuracy at epoch 16: 0.4765625, total training samples: 93
Early stopping at epoch 296. Loss did not improve for 10 epochs.
Generate examples Step 296, Loss 0.45968761056761953
Epoch: 17
Negative Examples
['a', 'aaabc', 'aacacabc', 'abbc', 'ba', 'bbcbbc', 'bc', 'bcabba', 'bcabc', 'cabc', 'caccaba', 'cbba', 'cbbbcabc', 'cbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 17: No counterexamples found, skipped.
Accuracy at epoch 17: 0.501953125, total training samples: 93
Early stopping at epoch 311. Loss did not improve for 10 epochs.
Generate examples Step 311, Loss 0.49871156804072553
Epoch: 18
Negative Examples
['aaacaba', 'aabc', 'abcabc', 'abccaba', 'acbba', 'acbbc', 'ba', 'bbaacaba', 'bbcaabc', 'bbccbbc', 'bc', 'bccabc', 'c', 'cacabc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abccaba', 'baaacba', 'baccccba']
Pos Pos Pos
Train Epoch 0, Loss 1.1508110761642456
Train Epoch 1, Loss 0.9816812872886658
Train Epoch 2, Loss 0.8433861136436462
Accuracy at epoch 18: 0.53125, total training samples: 96
Early stopping at epoch 310. Loss did not improve for 10 epochs.
Generate examples Step 310, Loss 0.29807179179222254
Epoch: 19
Negative Examples
['a', 'aba', 'acabc', 'ba', 'baabc', 'bbaaba', 'bbaba', 'bc', 'bcba', 'cabc', 'cacabc', 'ccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bbcaba']
Pos
Counterexamples
['bbcaba', 'caacaacb', 'cabbcbaa']
Neg Neg Neg
Train Epoch 0, Loss 0.5482401251792908
Train Epoch 1, Loss 0.4707608222961426
Train Epoch 2, Loss 0.40079641342163086
Accuracy at epoch 19: 0.48828125, total training samples: 99
Early stopping at epoch 262. Loss did not improve for 10 epochs.
Generate examples Step 262, Loss 0.3678748862598332
Epoch: 20
Negative Examples
['aaccbaba', 'abba', 'abc', 'abcaccbc', 'abcba', 'acabc', 'ba', 'bacccba', 'bbba', 'bbc', 'bcbaaba', 'bcbc', 'caba', 'cabba', 'cabbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'ababbaba', 'abbbbbba', 'abcba', 'abacccba', 'baabbaba', 'bacccba', 'abaabbba', 'bacaaaba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 1.0940802097320557
Train Epoch 1, Loss 0.9453742504119873
Train Epoch 2, Loss 0.8214571475982666
Accuracy at epoch 20: 0.517578125, total training samples: 108
Early stopping at epoch 274. Loss did not improve for 10 epochs.
Generate examples Step 274, Loss 0.26831224940039894
Epoch: 21
Negative Examples
['a', 'aaababa', 'ba', 'babc', 'bbacbcba', 'bbbcba', 'bcba', 'bcbc', 'bcbcba', 'c', 'cba', 'cbcbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 21: No counterexamples found, skipped.
Accuracy at epoch 21: 0.5390625, total training samples: 108
Early stopping at epoch 292. Loss did not improve for 10 epochs.
Generate examples Step 292, Loss 0.21981781253228821
Epoch: 22
Negative Examples
['a', 'abacc', 'abcbbabb', 'babb', 'bacc', 'bb', 'c', 'cacabcca', 'cbb', 'cbcbabb', 'cc', 'cca', 'ccbabb', 'ccbabcbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 22: No counterexamples found, skipped.
Accuracy at epoch 22: 0.5546875, total training samples: 108
Early stopping at epoch 321. Loss did not improve for 10 epochs.
Generate examples Step 321, Loss 0.27586699911156054
Epoch: 23
Negative Examples
['a', 'aaaabcbc', 'aba', 'abbcba', 'abbcbcbc', 'acba', 'ba', 'babc', 'c', 'caabbcbc', 'cabbaba', 'ccabcba', 'ccacbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baababa']
Pos
Counterexamples
['baababa', 'abbabbab', 'ccabbbaa']
Neg Neg Neg
Train Epoch 0, Loss 0.48749053478240967
Train Epoch 1, Loss 0.4271325170993805
Train Epoch 2, Loss 0.3690623342990875
Accuracy at epoch 23: 0.509765625, total training samples: 111
Early stopping at epoch 243. Loss did not improve for 10 epochs.
Generate examples Step 243, Loss 0.35700928798464476
Epoch: 24
Negative Examples
['aba', 'abc', 'accccbc', 'ba', 'bbaaaba', 'bbbcabc', 'bbcbccbc', 'bc', 'bcbaaaba', 'c', 'cbc', 'ccbbccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 24: No counterexamples found, skipped.
Accuracy at epoch 24: 0.5078125, total training samples: 111
Early stopping at epoch 268. Loss did not improve for 10 epochs.
Generate examples Step 268, Loss 0.32312475582480876
Epoch: 25
Negative Examples
['aacabc', 'abc', 'accbc', 'accccaba', 'ba', 'baaabc', 'bacabcbc', 'bacba', 'bbacaba', 'bbbaacbc', 'bc', 'c', 'caaccabc', 'ccba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacba', 'abcccba', 'abaaacba']
Pos Pos Pos
Train Epoch 0, Loss 0.9758962988853455
Train Epoch 1, Loss 0.8179675936698914
Train Epoch 2, Loss 0.6840114593505859
Accuracy at epoch 25: 0.6171875, total training samples: 114
Early stopping at epoch 316. Loss did not improve for 10 epochs.
Generate examples Step 316, Loss 0.25013384655447035
Epoch: 26
Negative Examples
['a', 'abc', 'baba', 'bbbbacbb', 'bbcbc', 'bc', 'cbaba', 'cbabcbc', 'cbbabc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aba', 'bbbbbaba', 'bbcba', 'bcbbcba', 'cba', 'cbc']
Pos Pos Pos Pos Pos Pos
Counterexamples
['baba', 'baacccba', 'abbbcaba', 'aba', 'abacaca', 'bacbbcca', 'bbbbbaba', 'ccaccbc', 'accbcbbc', 'bbcba', 'bbcacccb', 'baaabccb', 'bcbbcba', 'bbbbaaa', 'ccacacab', 'cba', 'aabab', 'accabaac', 'cbc', 'cbbbaaa', 'bacabcbb']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5342290699481964
Train Epoch 1, Loss 0.48602576553821564
Train Epoch 2, Loss 0.4438291937112808
Accuracy at epoch 26: 0.650390625, total training samples: 135
Early stopping at epoch 415. Loss did not improve for 10 epochs.
Generate examples Step 415, Loss 0.21615145065320227
Epoch: 27
Negative Examples
['a', 'abc', 'baabcba', 'bcababc', 'bcbc', 'cbbcbc']
Neg Neg Neg Neg Neg Neg
Positive Examples
['abbaba', 'abbcba', 'acbbcba', 'ba', 'bbcba', 'bcba', 'cbaba']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbaba', 'ccabaaca', 'bcaaaacc', 'abbcba', 'bbbaaccc', 'abbabbac', 'acbbcba', 'aabaccac', 'bcaababb', 'ba', 'baabbabb', 'ababbccc', 'bbcba', 'caacbccc', 'cbccbcbc', 'bcba', 'cbaababa', 'ccbacab', 'cbaba', 'caaabbbc', 'cbbbacca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.4628947675228119
Train Epoch 1, Loss 0.3152633011341095
Train Epoch 2, Loss 0.20827632397413254
Accuracy at epoch 27: 0.564453125, total training samples: 156
Early stopping at epoch 440. Loss did not improve for 10 epochs.
Generate examples Step 440, Loss 0.5131727740202361
Epoch: 28
Negative Examples
['a', 'aabba', 'abba', 'abbaabba', 'ba', 'bba', 'bbccbba', 'bcabba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'ababbaba', 'babbaaba']
Pos Pos Pos
Train Epoch 0, Loss 1.4859977960586548
Train Epoch 1, Loss 1.2141436338424683
Train Epoch 2, Loss 0.9835338592529297
Accuracy at epoch 28: 0.55859375, total training samples: 159
Early stopping at epoch 326. Loss did not improve for 10 epochs.
Generate examples Step 326, Loss 0.24914342930557531
Epoch: 29
Negative Examples
['aabaaba', 'aba', 'ba', 'bbaba', 'bbbcba', 'bcaba', 'cbbaabc', 'cbcaba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baaba', 'caba']
Pos Pos
Counterexamples
['caba', 'baacbccb', 'baacbccc']
Neg Neg Neg
Train Epoch 0, Loss 0.30320653319358826
Train Epoch 1, Loss 0.23528797924518585
Train Epoch 2, Loss 0.17957590520381927
Accuracy at epoch 29: 0.48828125, total training samples: 162
Early stopping at epoch 465. Loss did not improve for 10 epochs.
Generate examples Step 465, Loss 0.6681649962464116
Epoch: 30
Negative Examples
['a', 'abba', 'abbccbba', 'abccbba', 'acbba', 'bba', 'bbccbba', 'bccbba', 'cabcabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'babbacba', 'baccaaba']
Pos Pos Pos
Train Epoch 0, Loss 1.2540141344070435
Train Epoch 1, Loss 0.9584958553314209
Train Epoch 2, Loss 0.7078173160552979
Accuracy at epoch 30: 0.552734375, total training samples: 165
Early stopping at epoch 438. Loss did not improve for 10 epochs.
Generate examples Step 438, Loss 0.2855077202268505
Epoch: 31
Negative Examples
['a', 'aba', 'abaaba', 'abaabc', 'abacba', 'abbaabb', 'abcaba', 'bacbc', 'ccbaabc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaba', 'acbcaba', 'acbccba', 'baaba', 'bbccba']
Pos Pos Pos Pos Pos
Counterexamples
['abaaba', 'abcaba', 'abacaba', 'abacba', 'abcaacba', 'baaabbba', 'abcaba', 'abbbaaba', 'baaaaaba', 'aaba', 'bcbcacc', 'cabbbca', 'acbcaba', 'acbaab', 'abaababa', 'acbccba', 'bbacaa', 'cbabbcbc', 'bbccba', 'bbcbbabc', 'acacaccc']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5692266821861267
Train Epoch 1, Loss 0.5195718854665756
Train Epoch 2, Loss 0.49327319860458374
Accuracy at epoch 31: 0.56640625, total training samples: 186
Early stopping at epoch 368. Loss did not improve for 10 epochs.
Generate examples Step 368, Loss 0.31218159747963675
Epoch: 32
Negative Examples
['a', 'aba', 'ba', 'baabc', 'bbaabc', 'ccba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['abaaba', 'abbcaba', 'baaba', 'bacbcaba', 'caba', 'ccbaaba']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abbcaba', 'caabcaaa', 'cacbbbab', 'bacbcaba', 'aaccacac', 'abcacbab', 'caba', 'aabcbaa', 'aabcacb', 'ccbaaba', 'cacaccca', 'abbabcca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.4044368267059326
Train Epoch 1, Loss 0.30292344093322754
Train Epoch 2, Loss 0.22594137489795685
Accuracy at epoch 32: 0.48046875, total training samples: 198
Early stopping at epoch 387. Loss did not improve for 10 epochs.
Generate examples Step 387, Loss 0.5244565351107686
Epoch: 33
Negative Examples
['a', 'abba', 'acbba', 'ba', 'bba', 'bbbcabba', 'cabba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'ababbcba', 'baacaba']
Pos Pos Pos
Train Epoch 0, Loss 1.3876023292541504
Train Epoch 1, Loss 1.1335512399673462
Train Epoch 2, Loss 0.9175422787666321
Accuracy at epoch 33: 0.53515625, total training samples: 201
Early stopping at epoch 429. Loss did not improve for 10 epochs.
Generate examples Step 429, Loss 0.31674309130324874
Epoch: 34
Negative Examples
['a', 'aba', 'abbaaba', 'baaba', 'bcaba', 'cbcaba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['abbbaaba', 'caba', 'cccba']
Pos Pos Pos
Counterexamples
['baaba', 'abaacaba', 'abccacba', 'caba', 'bbaaccb', 'caaccbb', 'cccba', 'bbbbab', 'bbcbacba']
Pos Pos Pos Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5262539982795715
Train Epoch 1, Loss 0.49501991271972656
Train Epoch 2, Loss 0.4787077307701111
Accuracy at epoch 34: 0.64453125, total training samples: 210
Early stopping at epoch 493. Loss did not improve for 10 epochs.
Generate examples Step 493, Loss 0.24670613807463934
Epoch: 35
Negative Examples
['a', 'acbcabc', 'babc', 'bb', 'bbaba', 'bbbbaca', 'cacc', 'cbbaba', 'ccbbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bcbbaba', 'cabc']
Pos Pos
Counterexamples
['bcbbaba', 'bcabbbac', 'bbccabbb', 'cabc', 'ccbbbbcb', 'ccaccabc']
Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5334906578063965
Train Epoch 1, Loss 0.4163319170475006
Train Epoch 2, Loss 0.3203626573085785
Accuracy at epoch 35: 0.53515625, total training samples: 216
Early stopping at epoch 626. Loss did not improve for 10 epochs.
Generate examples Step 626, Loss 0.395052895686653
Epoch: 36
Negative Examples
['aaba', 'ba', 'bbaaba', 'bbbcaba']
Neg Neg Neg Neg
Positive Examples
['abbbaaba', 'abcaaba', 'abcbaaba', 'babba', 'bbbaaba', 'caaba', 'cbaaaba', 'cbbbaaba']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abcbaaba', 'aacbbcb', 'babccabc', 'babba', 'bacbbca', 'bacaccaa', 'bbbaaba', 'abbabcbb', 'caaabbcb', 'caaba', 'abcbcbac', 'cbabbbbc', 'cbaaaba', 'bbcbbbbb', 'ccbcacca', 'cbbbaaba', 'caabaaa', 'acaaacbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.30550386011600494
Train Epoch 1, Loss 0.1758125275373459
Train Epoch 2, Loss 0.09914171695709229
Accuracy at epoch 36: 0.466796875, total training samples: 234
Early stopping at epoch 502. Loss did not improve for 10 epochs.
Generate examples Step 502, Loss 0.9905540848347112
Epoch: 37
Negative Examples
['a', 'aabba', 'abba', 'ba', 'baabba', 'bba', 'bbbcabba', 'bbcaabba', 'bcaabba', 'bcabba', 'ccabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'abbbacba', 'abcba']
Pos Pos Pos
Train Epoch 0, Loss 2.4430763721466064
Train Epoch 1, Loss 2.018556833267212
Train Epoch 2, Loss 1.6304302215576172
Accuracy at epoch 37: 0.48828125, total training samples: 237
Early stopping at epoch 456. Loss did not improve for 10 epochs.
Generate examples Step 456, Loss 0.598253647790696
Epoch: 38
Negative Examples
['aabba', 'ba', 'bbacbba', 'bcabba', 'cabba', 'ccabba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['abbaabba', 'abbcabba', 'bbcabba', 'bccabba', 'cbbaabba', 'cbbcabba']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abbaabba', 'acaaaca', 'ccccbaba', 'abbcabba', 'ccccbb', 'cbacbc', 'bbcabba', 'acbcbcba', 'cbcbaaa', 'bccabba', 'ababcccc', 'bbcbacbb', 'cbbaabba', 'accccbbc', 'cbbacacb', 'cbbcabba', 'cccabcbc', 'abcabacc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.22372093796730042
Train Epoch 1, Loss 0.11169201880693436
Train Epoch 2, Loss 0.059713225811719894
Accuracy at epoch 38: 0.4921875, total training samples: 255
Early stopping at epoch 485. Loss did not improve for 10 epochs.
Generate examples Step 485, Loss 1.651443372047487
Epoch: 39
Negative Examples
['a', 'aaabba', 'aabba', 'abba', 'abbcabba', 'ba', 'bacabba', 'bba', 'bbaabba', 'bbcaabba', 'caabba', 'cabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'baacccba', 'abaabbba']
Pos Pos Pos
Train Epoch 0, Loss 2.89701771736145
Train Epoch 1, Loss 2.4658522605895996
Train Epoch 2, Loss 2.055997848510742
Accuracy at epoch 39: 0.529296875, total training samples: 258
Early stopping at epoch 457. Loss did not improve for 10 epochs.
Generate examples Step 457, Loss 0.7388239921961289
Epoch: 40
Negative Examples
['a', 'aabba', 'abba', 'acabba', 'ba', 'bba', 'cabba', 'cbaaabba', 'ccabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'baccacba', 'baacaaba']
Pos Pos Pos
Train Epoch 0, Loss 1.6571861505508423
Train Epoch 1, Loss 1.2966502904891968
Train Epoch 2, Loss 0.9823935627937317
Accuracy at epoch 40: 0.51171875, total training samples: 261
Early stopping at epoch 480. Loss did not improve for 10 epochs.
Generate examples Step 480, Loss 0.38989991085940734
Epoch: 41
Negative Examples
['a', 'aaabaaba', 'aaba', 'aba', 'bcaba', 'caba', 'cbaaba', 'cbbaaba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 41: No counterexamples found, skipped.
Accuracy at epoch 41: 0.546875, total training samples: 261
Early stopping at epoch 497. Loss did not improve for 10 epochs.
Generate examples Step 497, Loss 0.3461223524437851
Epoch: 42
Negative Examples
['aaba', 'aba', 'abbaaba', 'acaaba', 'ba', 'bacbaaba', 'bcaba', 'bcbbaaba', 'caba', 'cbbbaaba', 'cbcaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 42: No counterexamples found, skipped.
Accuracy at epoch 42: 0.490234375, total training samples: 261
Early stopping at epoch 425. Loss did not improve for 10 epochs.
Generate examples Step 425, Loss 0.28289441141724025
Epoch: 43
Negative Examples
['aba', 'abcaba', 'acbcaba', 'ba', 'baaba', 'babaaba', 'bbaaba', 'bbbaaba', 'cacbaaba', 'cbaaba', 'cbbcaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abcaba', 'bacbbaba', 'ababa', 'baaba', 'baaba', 'baacbbba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 1.088719129562378
Train Epoch 1, Loss 0.8265581130981445
Train Epoch 2, Loss 0.6165028214454651
Accuracy at epoch 43: 0.625, total training samples: 267
Early stopping at epoch 575. Loss did not improve for 10 epochs.
Generate examples Step 575, Loss 0.29380551249616677
Epoch: 44
Negative Examples
['a', 'babbabc', 'bcabcaca']
Neg Neg Neg
Positive Examples
['aabbaba', 'ababbcba', 'abbaba', 'acabbaba', 'ba', 'baba', 'babcaca', 'bbaba', 'cbabbaba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbaba', 'aabcbaa', 'ccacacbc', 'abbaba', 'babcbcbc', 'bacbbbab', 'acabbaba', 'cccbbaab', 'acaabaab', 'ba', 'baacccbb', 'abc', 'babcaca', 'abcabcb', 'ccccbaca', 'bbaba', 'aacbccaa', 'cacbcaac', 'cbabbaba', 'cbbbbcab', 'ccabbcbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.37406809628009796
Train Epoch 1, Loss 0.20143647491931915
Train Epoch 2, Loss 0.1116843968629837
Accuracy at epoch 44: 0.46484375, total training samples: 288
Early stopping at epoch 442. Loss did not improve for 10 epochs.
Generate examples Step 442, Loss 0.6293113078947261
Epoch: 45
Negative Examples
['abba', 'ba', 'bba']
Neg Neg Neg
Positive Examples
['bbaabba', 'bbbcabba', 'caabba', 'cabba']
Pos Pos Pos Pos
Counterexamples
['abba', 'bacbbba', 'abbbaba', 'bbaabba', 'bababb', 'aaaacbcc', 'bbbcabba', 'cccabbcc', 'acabbac', 'caabba', 'ccababcc', 'bcbbaaba', 'cabba', 'accbacb', 'bccbabb']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.8876686096191406
Train Epoch 1, Loss 0.7947935461997986
Train Epoch 2, Loss 0.7163776159286499
Accuracy at epoch 45: 0.509765625, total training samples: 303
Early stopping at epoch 495. Loss did not improve for 10 epochs.
Generate examples Step 495, Loss 0.40359915859035905
Epoch: 46
Negative Examples
['a', 'aaba', 'abcacaba', 'ba', 'bba', 'bcbaaba', 'caba', 'cccaba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bacaaba', 'caaba']
Pos Pos
Counterexamples
['abcacaba', 'abccaaba', 'baabbaba', 'caaba', 'bbacbaab', 'cbcbccab']
Pos Pos Pos Neg Neg Neg
Train Epoch 0, Loss 0.6032822728157043
Train Epoch 1, Loss 0.5270673632621765
Train Epoch 2, Loss 0.47100773453712463
Accuracy at epoch 46: 0.654296875, total training samples: 309
Early stopping at epoch 309. Loss did not improve for 10 epochs.
Generate examples Step 309, Loss 0.2868903499457144
Epoch: 47
Negative Examples
['a', 'abc', 'babcabc', 'bc', 'bcabc']
Neg Neg Neg Neg Neg
Positive Examples
['aba', 'acaacaba', 'ba', 'baba', 'bcaba']
Pos Pos Pos Pos Pos
Counterexamples
['aba', 'abbbacaa', 'babbaaaa', 'acaacaba', 'cbacbaaa', 'ccca', 'ba', 'babbc', 'abbbcbbc', 'bcaba', 'cbbabbcc', 'bcacacb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5496580004692078
Train Epoch 1, Loss 0.43259361386299133
Train Epoch 2, Loss 0.33438464999198914
Accuracy at epoch 47: 0.466796875, total training samples: 321
Early stopping at epoch 507. Loss did not improve for 10 epochs.
Generate examples Step 507, Loss 0.6097666010495246
Epoch: 48
Negative Examples
['a', 'aaaba', 'aaba', 'aba', 'ba', 'bba', 'bbaababa', 'bbccaaba', 'bcacaba', 'caaba', 'cbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 48: No counterexamples found, skipped.
Accuracy at epoch 48: 0.50390625, total training samples: 321
Early stopping at epoch 389. Loss did not improve for 10 epochs.
Generate examples Step 389, Loss 0.38760808706283567
Epoch: 49
Negative Examples
['a', 'aaba', 'aba', 'ba', 'bbaaaaba', 'bbacbaba', 'bbcacaba', 'caaba', 'caba', 'cababa', 'cacaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 49: No counterexamples found, skipped.
Accuracy at epoch 49: 0.455078125, total training samples: 321
Early stopping at epoch 454. Loss did not improve for 10 epochs.
Generate examples Step 454, Loss 0.4790738767974979
Epoch: 50
Negative Examples
['a', 'aacaba', 'aba', 'ababa', 'acaba', 'ba', 'baaaaba', 'baacaba', 'bacaaba', 'caba', 'cccaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['ababa', 'bacacba', 'baabbaba', 'baaaaba', 'bacaba', 'abcbbba', 'baacaba', 'abbbba', 'baacaaba', 'bacaaba', 'baaaba', 'abcaaba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.9479092955589294
Train Epoch 1, Loss 0.723644495010376
Train Epoch 2, Loss 0.5467522740364075
Accuracy at epoch 50: 0.69921875, total training samples: 333
Early stopping at epoch 357. Loss did not improve for 10 epochs.
Generate examples Step 357, Loss 0.34556792888727933
Epoch: 51
Negative Examples
['a']
Neg
Positive Examples
['aba', 'abaacaba', 'abccaba', 'abcccabc', 'acaba', 'ba', 'bcbcaba', 'bccaba', 'caaccaba', 'caba', 'cbaba', 'ccabc']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aba', 'abcacaaa', 'abbbaa', 'abcccabc', 'caccacba', 'cbbacac', 'acaba', 'cccaabbb', 'aaaaaaac', 'ba', 'abcbb', 'abbbbbc', 'bcbcaba', 'cccaa', 'cabaab', 'bccaba', 'caccacab', 'bcbbcbab', 'caaccaba', 'babcacaa', 'bbaacba', 'caba', 'abaccbab', 'accacab', 'cbaba', 'cbaccc', 'acbbaca', 'ccabc', 'bcbabb', 'bbcbacbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.47236083447933197
Train Epoch 1, Loss 0.28356361389160156
Train Epoch 2, Loss 0.16170894354581833
Accuracy at epoch 51: 0.525390625, total training samples: 363
Early stopping at epoch 529. Loss did not improve for 10 epochs.
Generate examples Step 529, Loss 0.952660756178622
Epoch: 52
Negative Examples
['a', 'aabba', 'abba', 'abbba', 'ba', 'bba', 'bbacabba', 'bbba', 'bbccabba', 'bbccbbba', 'bcacbbba', 'bccabbba', 'caabba', 'ccbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'abcaba', 'abcacaba']
Pos Pos Pos
Train Epoch 0, Loss 1.779394268989563
Train Epoch 1, Loss 1.41944420337677
Train Epoch 2, Loss 1.1098861694335938
Accuracy at epoch 52: 0.525390625, total training samples: 366
Early stopping at epoch 445. Loss did not improve for 10 epochs.
Generate examples Step 445, Loss 0.534215099207489
Epoch: 53
Negative Examples
['a', 'ba', 'baacaba', 'bacaaba', 'baccaaba', 'bcaaaba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['bbacaaba', 'bbcaaaba', 'bccaaba']
Pos Pos Pos
Counterexamples
['baacaba', 'abacaaba', 'abcacba', 'bacaaba', 'abcbbba', 'baaccaba', 'baccaaba', 'abaaaaba', 'baacba', 'bbacaaba', 'acbcacab', 'cbcabaa', 'bbcaaaba', 'bbabcaaa', 'cbaccbaa', 'bccaaba', 'cabaaa', 'ccaabbcc']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.419394388794899
Train Epoch 1, Loss 0.3482203744351864
Train Epoch 2, Loss 0.31013286113739014
Accuracy at epoch 53: 0.658203125, total training samples: 384
Early stopping at epoch 406. Loss did not improve for 10 epochs.
Generate examples Step 406, Loss 0.4308985965990024
Epoch: 54
Negative Examples
['a']
Neg
Positive Examples
['aabaaba', 'aabaabc', 'aabcaba', 'aba', 'acaaba', 'ba', 'cbabcaba', 'cbbaba']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabaaba', 'bccabacb', 'bbbbaa', 'aabaabc', 'cbccbcbc', 'cbbacab', 'aabcaba', 'acbacaab', 'accbcc', 'aba', 'babbbbaa', 'abacbbca', 'acaaba', 'ccabbaca', 'ccabbbcb', 'ba', 'bacacabb', 'baaccccc', 'cbabcaba', 'ccccacca', 'aababcaa', 'cbbaba', 'aaccabaa', 'bbbbcbca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.4329293370246887
Train Epoch 1, Loss 0.24780428409576416
Train Epoch 2, Loss 0.1398758366703987
Accuracy at epoch 54: 0.51171875, total training samples: 408
Early stopping at epoch 525. Loss did not improve for 10 epochs.
Generate examples Step 525, Loss 0.8273257004670771
Epoch: 55
Negative Examples
['a', 'abba', 'abbcabba', 'abcaabba', 'ba', 'baabba', 'bba', 'bbbaabba', 'cabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'baaaacba', 'bacacba']
Pos Pos Pos
Train Epoch 0, Loss 1.3820685148239136
Train Epoch 1, Loss 1.0082966089248657
Train Epoch 2, Loss 0.7079806327819824
Accuracy at epoch 55: 0.560546875, total training samples: 411
Early stopping at epoch 624. Loss did not improve for 10 epochs.
Generate examples Step 624, Loss 0.4134580078125
Epoch: 56
Negative Examples
['a', 'abbbcaba', 'abbbccba', 'bacba', 'cba', 'cbacba', 'ccbacba', 'ccbbacba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abbccaba', 'abbcccba', 'acba', 'bcbacba', 'ccba']
Pos Pos Pos Pos Pos
Counterexamples
['abbbcaba', 'abbbbbba', 'baaccaba', 'abbbccba', 'abaccba', 'abbbbbba', 'bacba', 'baaacba', 'abaaba', 'abbccaba', 'ccaaacbc', 'bbabbbaa', 'abbcccba', 'bcccacca', 'ccbabacc', 'acba', 'cabbaaca', 'ccbcacab', 'bcbacba', 'caacacaa', 'cabcbab', 'ccba', 'cccbcaca', 'caccbabc']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5436413884162903
Train Epoch 1, Loss 0.4916577488183975
Train Epoch 2, Loss 0.4688442796468735
Accuracy at epoch 56: 0.662109375, total training samples: 435
Early stopping at epoch 577. Loss did not improve for 10 epochs.
Generate examples Step 577, Loss 0.30913671639757584
Epoch: 57
Negative Examples
['a', 'aba', 'abbcaba']
Neg Neg Neg
Positive Examples
['aaba', 'abaaba', 'abbbaaba', 'ba', 'baaba', 'bbbcaba', 'bbcaba', 'caba', 'cbbaaba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaba', 'caacbabc', 'acbaacbc', 'ba', 'baccbbbb', 'bacacbbc', 'bbbcaba', 'bbbcccb', 'babbbcab', 'bbcaba', 'ccabbcca', 'cbcacaa', 'caba', 'abcbbbcb', 'accacbba', 'cbbaaba', 'caaaaca', 'ccbaabc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.32683537155389786
Train Epoch 1, Loss 0.17271678894758224
Train Epoch 2, Loss 0.0925343707203865
Accuracy at epoch 57: 0.50390625, total training samples: 453
Early stopping at epoch 629. Loss did not improve for 10 epochs.
Generate examples Step 629, Loss 1.0723664043441652
Epoch: 58
Negative Examples
['a', 'acabba', 'accbba', 'ba', 'bacabba', 'baccbba', 'bba', 'bbacabba', 'bbccabba', 'ccbba', 'cccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 58: No counterexamples found, skipped.
Accuracy at epoch 58: 0.544921875, total training samples: 453
Early stopping at epoch 598. Loss did not improve for 10 epochs.
Generate examples Step 598, Loss 1.0518346337324789
Epoch: 59
Negative Examples
['a', 'abba', 'acabba', 'ba', 'bacabba', 'bba', 'bbccabba', 'bcccbba', 'cabba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'baccaaba', 'ababbaba']
Pos Pos Pos
Train Epoch 0, Loss 2.14251971244812
Train Epoch 1, Loss 1.7266877889633179
Train Epoch 2, Loss 1.3618497848510742
Accuracy at epoch 59: 0.52734375, total training samples: 456
Early stopping at epoch 694. Loss did not improve for 10 epochs.
Generate examples Step 694, Loss 0.4830696862378566
Epoch: 60
Negative Examples
['acaaba', 'accaba', 'ba', 'baccaba', 'bba', 'bbacaba', 'bbcaaaba', 'cbcaaaba', 'cbcacaba', 'ccaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['cbbccbba']
Pos
Counterexamples
['baccaba', 'babbcba', 'ababbcba', 'cbbccbba', 'caaaabba', 'accccabb']
Pos Pos Pos Neg Neg Neg
Train Epoch 0, Loss 0.9218011498451233
Train Epoch 1, Loss 0.8012890815734863
Train Epoch 2, Loss 0.6978526711463928
Accuracy at epoch 60: 0.603515625, total training samples: 462
Early stopping at epoch 398. Loss did not improve for 10 epochs.
Generate examples Step 398, Loss 0.34310033177970944
Epoch: 61
Negative Examples
['a', 'aba', 'ba']
Neg Neg Neg
Positive Examples
['abacbaba', 'abaccaba', 'baba', 'bbaacaba', 'bbacbaba', 'bbbacaba', 'bcababa', 'bccaba', 'caba', 'cbaababa']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abacbaba', 'baaababc', 'bcbbbbc', 'bbaacaba', 'bccbcccc', 'ccbbbcab', 'bbacbaba', 'bbbcbc', 'bbacaacb', 'bbbacaba', 'cccccbac', 'aaabaabc', 'bcababa', 'ababccb', 'caaacccb', 'bccaba', 'ccabcabc', 'cbcabbbb', 'caba', 'cbaccbba', 'cbbbbaac', 'cbaababa', 'bcacaccb', 'bcababcc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.3628953695297241
Train Epoch 1, Loss 0.1864347606897354
Train Epoch 2, Loss 0.09360651299357414
Accuracy at epoch 61: 0.482421875, total training samples: 486
Early stopping at epoch 565. Loss did not improve for 10 epochs.
Generate examples Step 565, Loss 1.29638366037881
Epoch: 62
Negative Examples
['a', 'aacbbba', 'abba', 'acabba', 'acbbba', 'accbbba', 'ba', 'bacbbba', 'bba', 'cabba', 'ccbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'baacccba', 'babbccba', 'bacbbba', 'abcba', 'abccbbba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 2.251972198486328
Train Epoch 1, Loss 1.81814706325531
Train Epoch 2, Loss 1.4326715469360352
Accuracy at epoch 62: 0.4765625, total training samples: 492
Early stopping at epoch 463. Loss did not improve for 10 epochs.
Generate examples Step 463, Loss 0.6439009948537268
Epoch: 63
Negative Examples
['a', 'aacabba', 'acabba', 'ba', 'bacabba', 'bccabba', 'cabba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 63: No counterexamples found, skipped.
Accuracy at epoch 63: 0.4921875, total training samples: 492
Early stopping at epoch 512. Loss did not improve for 10 epochs.
Generate examples Step 512, Loss 0.781979876768287
Epoch: 64
Negative Examples
['a', 'aacabba', 'abba', 'accabba', 'baacabba', 'bba', 'bbacabba', 'bbccabba', 'bccabba', 'cabba', 'ccabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'baaabbba', 'abcacba']
Pos Pos Pos
Train Epoch 0, Loss 1.0615369081497192
Train Epoch 1, Loss 0.784045934677124
Train Epoch 2, Loss 0.5700740218162537
Accuracy at epoch 64: 0.6171875, total training samples: 495
Early stopping at epoch 509. Loss did not improve for 10 epochs.
Generate examples Step 509, Loss 0.2988642090383698
Epoch: 65
Negative Examples
['a', 'cba']
Neg Neg
Positive Examples
['aacccba', 'ba', 'bacba', 'bacccba', 'bcbacba', 'cbacacba', 'ccba']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aacccba', 'bcaacbcc', 'ccbac', 'ba', 'abcaaacc', 'baccacc', 'bcbacba', 'bcbbaabc', 'bccbcaca', 'cbacacba', 'abaabccb', 'bccacbcb', 'ccba', 'bbbabca', 'bcbbaabc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.40408673882484436
Train Epoch 1, Loss 0.2980804741382599
Train Epoch 2, Loss 0.2150263786315918
Accuracy at epoch 65: 0.513671875, total training samples: 510
Early stopping at epoch 453. Loss did not improve for 10 epochs.
Generate examples Step 453, Loss 0.6188408666770364
Epoch: 66
Negative Examples
['a', 'baaaabba', 'bacabba', 'bba', 'bbacabba']
Neg Neg Neg Neg Neg
Positive Examples
['aacabba', 'acabba', 'accabba', 'baacabba', 'bccabba']
Pos Pos Pos Pos Pos
Counterexamples
['aacabba', 'accabbc', 'abaaabaa', 'acabba', 'bbcbabc', 'ccacaacc', 'accabba', 'cbccaaba', 'accbccab', 'baacabba', 'baaaacaa', 'baacccca', 'bccabba', 'cbacbcc', 'cbaacccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.34593698382377625
Train Epoch 1, Loss 0.22710269689559937
Train Epoch 2, Loss 0.1470334231853485
Accuracy at epoch 66: 0.50390625, total training samples: 525
Early stopping at epoch 560. Loss did not improve for 10 epochs.
Generate examples Step 560, Loss 1.3431714715804646
Epoch: 67
Negative Examples
['a', 'acbbba', 'ba', 'baacbbba', 'baccbbba', 'cbbba', 'ccbbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baacbbba', 'abccaaba', 'abaacaba', 'baccbbba', 'bacaacba', 'baaccba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 2.1362388134002686
Train Epoch 1, Loss 1.7205191850662231
Train Epoch 2, Loss 1.3516148328781128
Accuracy at epoch 67: 0.50390625, total training samples: 531
Early stopping at epoch 486. Loss did not improve for 10 epochs.
Generate examples Step 486, Loss 0.5908167341406585
Epoch: 68
Negative Examples
['a', 'aabba', 'abba', 'acbba', 'ba', 'baaabba', 'baaabbba', 'bba', 'bbaaabba', 'cabba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'abaacba', 'abaaba', 'baaabbba', 'abbbbbba', 'bacbbba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 1.0923198461532593
Train Epoch 1, Loss 0.8496968150138855
Train Epoch 2, Loss 0.6562508940696716
Accuracy at epoch 68: 0.59765625, total training samples: 537
Early stopping at epoch 630. Loss did not improve for 10 epochs.
Generate examples Step 630, Loss 0.3955481464010789
Epoch: 69
Negative Examples
['a']
Neg
Positive Examples
['aaacba', 'abacba', 'acaaba', 'acba', 'ba', 'baacba', 'bbacacba', 'bbbaacba', 'bbbcccba', 'caccacba', 'cbbcacba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaacba', 'cccbcacb', 'bbbcaaca', 'acaaba', 'abbacbba', 'cabcbabb', 'acba', 'ccbaabcb', 'baaabaac', 'ba', 'abccacbb', 'bacbbcbb', 'bbacacba', 'acababba', 'babcacba', 'bbbaacba', 'acccabc', 'bcababa', 'bbbcccba', 'bcbcacaa', 'bbbaccbc', 'caccacba', 'cacbcbbc', 'aaaacaca', 'cbbcacba', 'acbacccc', 'cbcaaabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.49959738552570343
Train Epoch 1, Loss 0.26644452661275864
Train Epoch 2, Loss 0.13828139007091522
Accuracy at epoch 69: 0.501953125, total training samples: 564
Early stopping at epoch 511. Loss did not improve for 10 epochs.
Generate examples Step 511, Loss 0.9613198811421171
Epoch: 70
Negative Examples
['a', 'aaacbbba', 'aacbbba', 'acbbba', 'acccbbba', 'ba', 'baacbbba', 'bba', 'bbba', 'ccbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baacbbba', 'baccba', 'baabbba']
Pos Pos Pos
Train Epoch 0, Loss 1.6598960161209106
Train Epoch 1, Loss 1.2562357187271118
Train Epoch 2, Loss 0.926114559173584
Accuracy at epoch 70: 0.498046875, total training samples: 567
Early stopping at epoch 520. Loss did not improve for 10 epochs.
Generate examples Step 520, Loss 0.5430802654853938
Epoch: 71
Negative Examples
['a', 'acabcba', 'accbcba', 'ba', 'bba']
Neg Neg Neg Neg Neg
Positive Examples
['cbbba']
Pos
Counterexamples
['cbbba', 'acacc', 'cccbbbbb']
Neg Neg Neg
Train Epoch 0, Loss 0.42058220505714417
Train Epoch 1, Loss 0.2909035384654999
Train Epoch 2, Loss 0.19727051258087158
Accuracy at epoch 71: 0.478515625, total training samples: 570
Early stopping at epoch 471. Loss did not improve for 10 epochs.
Generate examples Step 471, Loss 0.5989056313568253
Epoch: 72
Negative Examples
['a', 'aacabba', 'abba', 'abbba', 'acabbba', 'ba', 'bba', 'bbaabbba', 'bbba', 'bbcabbba', 'cbbba', 'ccbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'abbbcaba', 'abaaba']
Pos Pos Pos
Train Epoch 0, Loss 1.931674599647522
Train Epoch 1, Loss 1.57612943649292
Train Epoch 2, Loss 1.2565709352493286
Accuracy at epoch 72: 0.5, total training samples: 573
Early stopping at epoch 466. Loss did not improve for 10 epochs.
Generate examples Step 466, Loss 0.492299825399795
Epoch: 73
Negative Examples
['aaabacba', 'acacba', 'acba', 'acccba', 'ba', 'bacaaba', 'bacacba', 'bbccacba', 'bccacba', 'cacba', 'cba', 'ccaacba', 'ccacba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacaaba', 'abccccba', 'abbbacba', 'bacacba', 'abcabbba', 'baabbaba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.9094575047492981
Train Epoch 1, Loss 0.6891825795173645
Train Epoch 2, Loss 0.5128905177116394
Accuracy at epoch 73: 0.716796875, total training samples: 579
Early stopping at epoch 346. Loss did not improve for 10 epochs.
Generate examples Step 346, Loss 0.2383681047460875
Epoch: 74
Negative Examples
['a', 'abbaca']
Neg Neg
Positive Examples
['aaba', 'aba', 'abbaba', 'ba', 'baba', 'babbaba', 'bbaba', 'bccbaba', 'cababa', 'cbaba', 'cbbaba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaba', 'bbcbaccb', 'cccabac', 'aba', 'bacaccaa', 'abcaccca', 'abbaba', 'bcbacbac', 'caabacaa', 'ba', 'abcccabb', 'abcacbb', 'bbaba', 'abbbabca', 'bcccbacb', 'bccbaba', 'bbcacbb', 'cbbaccc', 'cababa', 'acccaaa', 'accbcbcb', 'cbaba', 'cacacaac', 'ccccbbb', 'cbbaba', 'cabbcbba', 'caccbcac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.4409377723932266
Train Epoch 1, Loss 0.2694976329803467
Train Epoch 2, Loss 0.16079166531562805
Accuracy at epoch 74: 0.490234375, total training samples: 606
Early stopping at epoch 550. Loss did not improve for 10 epochs.
Generate examples Step 550, Loss 0.8809601675360692
Epoch: 75
Negative Examples
['a', 'aaabba', 'aabba', 'abba', 'baaaabba', 'bacabba', 'bba', 'bbacabba', 'bbcaabba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'baabbaba', 'abcaccba']
Pos Pos Pos
Train Epoch 0, Loss 1.5431209802627563
Train Epoch 1, Loss 1.24733567237854
Train Epoch 2, Loss 0.9984081387519836
Accuracy at epoch 75: 0.5703125, total training samples: 609
Early stopping at epoch 391. Loss did not improve for 10 epochs.
Generate examples Step 391, Loss 0.3807497335483833
Epoch: 76
Negative Examples
['a', 'ba', 'cba']
Neg Neg Neg
Positive Examples
['aaacba', 'aacba', 'acacba', 'acba', 'baacba', 'bacba', 'cabaacba', 'cbcaccba', 'ccba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaacba', 'ccabac', 'bcaacb', 'aacba', 'ababaab', 'bcabbcca', 'acacba', 'abbcbbaa', 'aaaaccbb', 'acba', 'baabab', 'abbabacc', 'cabaacba', 'acabccaa', 'cbabccbb', 'cbcaccba', 'acbaacba', 'aaacaca', 'ccba', 'bbabccaa', 'ababcbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.35684414207935333
Train Epoch 1, Loss 0.1787399873137474
Train Epoch 2, Loss 0.09078801050782204
Accuracy at epoch 76: 0.494140625, total training samples: 630
Early stopping at epoch 552. Loss did not improve for 10 epochs.
Generate examples Step 552, Loss 1.368588306373541
Epoch: 77
Negative Examples
['a', 'aaccbbba', 'acbbba', 'ba', 'baacbbba', 'baccbbba', 'bba', 'bbba', 'cbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baacbbba', 'abbbcaba', 'abbbacba', 'baccbbba', 'bacba', 'abccba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 2.352022171020508
Train Epoch 1, Loss 1.9807767868041992
Train Epoch 2, Loss 1.6380771398544312
Accuracy at epoch 77: 0.5234375, total training samples: 636
Early stopping at epoch 462. Loss did not improve for 10 epochs.
Generate examples Step 462, Loss 0.6119506239376109
Epoch: 78
Negative Examples
['a', 'abba', 'acacba', 'accabba', 'ba', 'bacbcba', 'baccabba', 'bba', 'cabba', 'cbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'abacba', 'bacba']
Pos Pos Pos
Train Epoch 0, Loss 1.2522636651992798
Train Epoch 1, Loss 0.9197890162467957
Train Epoch 2, Loss 0.6439712643623352
Accuracy at epoch 78: 0.642578125, total training samples: 639
Early stopping at epoch 473. Loss did not improve for 10 epochs.
Generate examples Step 473, Loss 0.2800806916086986
Epoch: 79
Negative Examples
['a', 'ba', 'bcbbaaba', 'cba', 'cbcbbcba']
Neg Neg Neg Neg Neg
Positive Examples
['abaaba', 'acba', 'baaba', 'bacaaba', 'bacba', 'bbbacba', 'bbcbacba', 'bccaaba']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['acba', 'cbcaccaa', 'bccbaaac', 'bbbacba', 'abbacba', 'ababbcbc', 'bbcbacba', 'acbcbbbc', 'aaccbbbb', 'bccaaba', 'bccbccac', 'bcbabaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.4470832347869873
Train Epoch 1, Loss 0.3180529773235321
Train Epoch 2, Loss 0.22479502856731415
Accuracy at epoch 79: 0.533203125, total training samples: 651
Early stopping at epoch 490. Loss did not improve for 10 epochs.
Generate examples Step 490, Loss 0.5671774769873338
Epoch: 80
Negative Examples
['a', 'aacabba', 'abba', 'acabba', 'acccba', 'acccbba', 'ba', 'bacabba', 'baccbba', 'bba', 'cabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'ababbcba', 'baaabbba']
Pos Pos Pos
Train Epoch 0, Loss 1.122488021850586
Train Epoch 1, Loss 0.8590629696846008
Train Epoch 2, Loss 0.6468479633331299
Accuracy at epoch 80: 0.62890625, total training samples: 654
Early stopping at epoch 495. Loss did not improve for 10 epochs.
Generate examples Step 495, Loss 0.3804221554929691
Epoch: 81
Negative Examples
['a', 'aaba', 'abaababa', 'cba']
Neg Neg Neg Neg
Positive Examples
['acacba', 'baaacba', 'bacacba', 'bbacbcba', 'caaba', 'caacba', 'cacba']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['acacba', 'caaaccbb', 'ccccac', 'bbacbcba', 'bccbcaac', 'abbacbca', 'caaba', 'ccbacabb', 'bcccabca', 'caacba', 'bccccacb', 'bbcbcabc', 'cacba', 'bbabbbcb', 'cbacaaca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.4377380609512329
Train Epoch 1, Loss 0.323574036359787
Train Epoch 2, Loss 0.2339630275964737
Accuracy at epoch 81: 0.47265625, total training samples: 669
Early stopping at epoch 420. Loss did not improve for 10 epochs.
Generate examples Step 420, Loss 0.5164663960135182
Epoch: 82
Negative Examples
['a', 'abbba', 'ba', 'baacabba', 'bba', 'bbba', 'bcba', 'cbba', 'cbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 82: No counterexamples found, skipped.
Accuracy at epoch 82: 0.501953125, total training samples: 669
Early stopping at epoch 495. Loss did not improve for 10 epochs.
Generate examples Step 495, Loss 0.6176174320280552
Epoch: 83
Negative Examples
['a', 'aaabbbba', 'aaacbbba', 'aabbba', 'aacbbba', 'abbba', 'acbcba', 'ba', 'baacbbba', 'bba', 'cbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baacbbba', 'abbbaaba', 'abacaba']
Pos Pos Pos
Train Epoch 0, Loss 1.5298253297805786
Train Epoch 1, Loss 1.2503377199172974
Train Epoch 2, Loss 1.0017482042312622
Accuracy at epoch 83: 0.595703125, total training samples: 672
Early stopping at epoch 404. Loss did not improve for 10 epochs.
Generate examples Step 404, Loss 0.2975356809151026
Epoch: 84
Negative Examples
['ba', 'bbbaba', 'bcababa', 'bccababa', 'caababa', 'cba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['abbaba', 'abccbcba', 'acbaba', 'bacbcba', 'bcba', 'cbaba']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abbaba', 'abcbaca', 'cbcbbaca', 'abccbcba', 'acccbbc', 'bbaacbac', 'acbaba', 'bbbbcbaa', 'caacaacb', 'bacbcba', 'bcccbacb', 'baabccbc', 'bcba', 'cabbcba', 'acbcccb', 'cbaba', 'ccbbbcbb', 'bbccccbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.2857706844806671
Train Epoch 1, Loss 0.167389914393425
Train Epoch 2, Loss 0.10002435557544231
Accuracy at epoch 84: 0.50390625, total training samples: 690
Early stopping at epoch 606. Loss did not improve for 10 epochs.
Generate examples Step 606, Loss 1.260512301026302
Epoch: 85
Negative Examples
['a', 'aaaaabba', 'aaaabba', 'aaabba', 'aabba', 'aacaabba', 'acaabba', 'ba', 'caabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 85: No counterexamples found, skipped.
Accuracy at epoch 85: 0.505859375, total training samples: 690
Early stopping at epoch 494. Loss did not improve for 10 epochs.
Generate examples Step 494, Loss 0.6710392963404608
Epoch: 86
Negative Examples
['a', 'aaaaabba', 'aaaabba', 'aaabba', 'aacaabba', 'abba', 'accabba', 'ba', 'baccabba', 'bba', 'caabba', 'cabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'baaaccba', 'abacccba']
Pos Pos Pos
Train Epoch 0, Loss 1.717675805091858
Train Epoch 1, Loss 1.334493637084961
Train Epoch 2, Loss 0.9927268028259277
Accuracy at epoch 86: 0.513671875, total training samples: 693
Early stopping at epoch 518. Loss did not improve for 10 epochs.
Generate examples Step 518, Loss 0.533072531797523
Epoch: 87
Negative Examples
['a', 'acacba', 'ba', 'baacba', 'babba', 'bba', 'bbaacba', 'bcbaacba', 'caacba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bbaaacba']
Pos
Counterexamples
['baacba', 'abacaaba', 'abccba', 'bbaaacba', 'bccabbc', 'aaacbbcc']
Pos Pos Pos Neg Neg Neg
Train Epoch 0, Loss 0.6964191794395447
Train Epoch 1, Loss 0.6085996031761169
Train Epoch 2, Loss 0.5470532774925232
Accuracy at epoch 87: 0.640625, total training samples: 699
Early stopping at epoch 376. Loss did not improve for 10 epochs.
Generate examples Step 376, Loss 0.41282898227162956
Epoch: 88
Negative Examples
['a']
Neg
Positive Examples
['aacbacba', 'abacba', 'acba', 'acbacba', 'ba', 'bcbaaba', 'bcbacba', 'bccba', 'cacbaaba', 'cacbacba', 'cba', 'ccba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aacbacba', 'abbbccbc', 'acacaacc', 'acba', 'aaccacac', 'cbccbba', 'acbacba', 'babccab', 'ccbcbba', 'ba', 'abaccabb', 'baccc', 'bcbaaba', 'caabba', 'cacbbaa', 'bcbacba', 'accbccbb', 'bccbbacc', 'bccba', 'bbcbcab', 'acbccbaa', 'cacbaaba', 'caaabaa', 'cbbbbcaa', 'cacbacba', 'cacbcbaa', 'cbbaabca', 'cba', 'ababaa', 'aabcacaa', 'ccba', 'bcbabc', 'bcbcaac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.3550601601600647
Train Epoch 1, Loss 0.160683902601401
Train Epoch 2, Loss 0.07903841510415077
Accuracy at epoch 88: 0.49609375, total training samples: 732
Early stopping at epoch 591. Loss did not improve for 10 epochs.
Generate examples Step 591, Loss 1.4551525319347511
Epoch: 89
Negative Examples
['a', 'aaabbba', 'abbba', 'ba', 'baaabbba', 'bba', 'bbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaabbba', 'abaacaba', 'abcbbcba']
Pos Pos Pos
Train Epoch 0, Loss 2.1891865730285645
Train Epoch 1, Loss 1.8100929260253906
Train Epoch 2, Loss 1.4563493728637695
Accuracy at epoch 89: 0.521484375, total training samples: 735
Early stopping at epoch 587. Loss did not improve for 10 epochs.
Generate examples Step 587, Loss 0.7150884412077009
Epoch: 90
Negative Examples
['a', 'aaacaba', 'aababa', 'aacaba', 'ababa', 'ba', 'baaababa', 'baababa', 'baba', 'bbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['ababa', 'baccba', 'bacacba', 'baba', 'baccbbba', 'baabbcba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 1.1484147310256958
Train Epoch 1, Loss 0.8599531054496765
Train Epoch 2, Loss 0.6281315684318542
Accuracy at epoch 90: 0.62890625, total training samples: 741
Early stopping at epoch 405. Loss did not improve for 10 epochs.
Generate examples Step 405, Loss 0.3556987514783596
Epoch: 91
Negative Examples
['a', 'aba', 'ba']
Neg Neg Neg
Positive Examples
['aabababa', 'abccaba', 'baba', 'bababa', 'bcba', 'bcbababa', 'caba', 'cbaba']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabababa', 'bcbabcb', 'cccbaaac', 'bababa', 'bbcacaa', 'acbbaaab', 'bcba', 'caacbbac', 'abbbbaaa', 'bcbababa', 'ccbcbaa', 'ccaccaba', 'caba', 'acbccbaa', 'caccbcc', 'cbaba', 'aacbaabc', 'ccbcccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.2844288647174835
Train Epoch 1, Loss 0.1659252904355526
Train Epoch 2, Loss 0.10040853172540665
Accuracy at epoch 91: 0.5234375, total training samples: 759
Early stopping at epoch 603. Loss did not improve for 10 epochs.
Generate examples Step 603, Loss 0.865363314550444
Epoch: 92
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'bba', 'bbaabbba', 'bbaacbba', 'bbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aacbba']
Pos
Counterexamples
['aacbba', 'aabacacb', 'aababcaa']
Neg Neg Neg
Train Epoch 0, Loss 0.2629324197769165
Train Epoch 1, Loss 0.15748530626296997
Train Epoch 2, Loss 0.09754084795713425
Accuracy at epoch 92: 0.509765625, total training samples: 762
Early stopping at epoch 595. Loss did not improve for 10 epochs.
Generate examples Step 595, Loss 1.5655670742060515
Epoch: 93
Negative Examples
['a', 'aaabbba', 'ba', 'baaabbba', 'baacbbba', 'bba', 'bbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaabbba', 'abaaaaba', 'baaacba', 'baacbbba', 'baccaba', 'bacaaba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 2.382359266281128
Train Epoch 1, Loss 1.976793885231018
Train Epoch 2, Loss 1.5829623937606812
Accuracy at epoch 93: 0.515625, total training samples: 768
Early stopping at epoch 616. Loss did not improve for 10 epochs.
Generate examples Step 616, Loss 0.8873991341219922
Epoch: 94
Negative Examples
['a', 'aabbba', 'aacbba', 'aaccba', 'ba', 'baabbba', 'baacbba', 'baaccba', 'bba', 'bbaacbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baabbba', 'babbba', 'abaaba', 'baaccba', 'baaabbba', 'bacccba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 1.1559635400772095
Train Epoch 1, Loss 0.8449776768684387
Train Epoch 2, Loss 0.6049512028694153
Accuracy at epoch 94: 0.560546875, total training samples: 774
Early stopping at epoch 537. Loss did not improve for 10 epochs.
Generate examples Step 537, Loss 0.3665084451104629
Epoch: 95
Negative Examples
['ba', 'cba']
Neg Neg
Positive Examples
['ababa', 'accba', 'bacaba', 'bacbcba', 'bbaccba', 'bcba', 'cbbaccba', 'ccba']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['accba', 'bbabccb', 'caaacabc', 'bacbcba', 'cccbbabc', 'ababaaaa', 'bbaccba', 'bacabca', 'cbabb', 'bcba', 'bacbabbb', 'abcbcb', 'cbbaccba', 'ccbbbcca', 'bbcbaabc', 'ccba', 'cbcaccca', 'babccabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.3161770701408386
Train Epoch 1, Loss 0.14915117993950844
Train Epoch 2, Loss 0.0753457061946392
Accuracy at epoch 95: 0.498046875, total training samples: 792
Early stopping at epoch 597. Loss did not improve for 10 epochs.
Generate examples Step 597, Loss 1.0999182229456694
Epoch: 96
Negative Examples
['a', 'aacbbba', 'acbbba', 'ba', 'baacbbba', 'bba', 'bbba', 'cbbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baacbbba', 'bacaacba', 'bacbbcba']
Pos Pos Pos
Train Epoch 0, Loss 1.890950083732605
Train Epoch 1, Loss 1.4776577949523926
Train Epoch 2, Loss 1.1096535921096802
Accuracy at epoch 96: 0.486328125, total training samples: 795
Early stopping at epoch 667. Loss did not improve for 10 epochs.
Generate examples Step 667, Loss 0.5708415728218541
Epoch: 97
Negative Examples
['a', 'ba', 'baabcba', 'bba', 'bbacbcba', 'bbba', 'cbcba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaababa', 'aababa', 'bbacbbba', 'ccbba']
Pos Pos Pos Pos
Counterexamples
['aaababa', 'ccbabbc', 'accabcbc', 'aababa', 'abbcaaab', 'bbbacaca', 'bbacbbba', 'acbbaab', 'ccbacba', 'ccbba', 'ccaaabcb', 'ccccbccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.3411255180835724
Train Epoch 1, Loss 0.22186177968978882
Train Epoch 2, Loss 0.14365004003047943
Accuracy at epoch 97: 0.5, total training samples: 807
Early stopping at epoch 548. Loss did not improve for 10 epochs.
Generate examples Step 548, Loss 0.8532554207603789
Epoch: 98
Negative Examples
['a', 'aaacbbba', 'aacbbba', 'accbbba', 'baacbbba', 'bba', 'bbba', 'cbbba', 'ccbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baacbbba', 'baccbbba', 'bacacba']
Pos Pos Pos
Train Epoch 0, Loss 1.5267242193222046
Train Epoch 1, Loss 1.1044673919677734
Train Epoch 2, Loss 0.7595811486244202
Accuracy at epoch 98: 0.494140625, total training samples: 810
Early stopping at epoch 583. Loss did not improve for 10 epochs.
Generate examples Step 583, Loss 0.5737547708598718
Epoch: 99
Negative Examples
['a', 'ba', 'baabcba', 'bba', 'bbba', 'cbcba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['bacbcba']
Pos
Counterexamples
['bacbcba', 'acbcbaac', 'cacbabba']
Neg Neg Neg
Train Epoch 0, Loss 0.36311057209968567
Train Epoch 1, Loss 0.24228514730930328
Train Epoch 2, Loss 0.1621674746274948
Accuracy at epoch 99: 0.5390625, total training samples: 813
Early stopping at epoch 551. Loss did not improve for 10 epochs.
Generate examples Step 551, Loss 1.059866716356381
Epoch: 100
Negative Examples
['a', 'aabbba', 'aacbbba', 'aaccbba', 'ba', 'baaabbba', 'bba', 'bbba', 'cbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaabbba', 'baacacba', 'abcaaaba']
Pos Pos Pos
Train Epoch 0, Loss 1.8568607568740845
Train Epoch 1, Loss 1.468770980834961
Train Epoch 2, Loss 1.1263251304626465
Accuracy at epoch 100: 0.521484375, total training samples: 816
Early stopping at epoch 693. Loss did not improve for 10 epochs.
Generate examples Step 693, Loss 0.6070213837479308
Epoch: 101
Negative Examples
['ba', 'bacacba', 'bba', 'cccba']
Neg Neg Neg Neg
Positive Examples
['acbba', 'bacccba', 'bbaaacba', 'bbaccaba', 'bbacccba']
Pos Pos Pos Pos Pos
Counterexamples
['bacacba', 'baccaba', 'ababbba', 'acbba', 'bcbbaac', 'cabcacbb', 'bbaaacba', 'bbaca', 'caabbba', 'bbaccaba', 'aaabacac', 'bbcbbcb', 'bbacccba', 'ccbcbbac', 'ccbbaccc']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.593397855758667
Train Epoch 1, Loss 0.5239770412445068
Train Epoch 2, Loss 0.48803675174713135
Accuracy at epoch 101: 0.634765625, total training samples: 831
Early stopping at epoch 489. Loss did not improve for 10 epochs.
Generate examples Step 489, Loss 0.31623314582571693
Epoch: 102
Negative Examples
['a', 'baaaba']
Neg Neg
Positive Examples
['aacba', 'acaba', 'ba', 'baaaaba', 'baacaba', 'ccaba', 'ccba']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaaba', 'abcaccba', 'ababbba', 'aacba', 'cbbccbab', 'bbcaac', 'acaba', 'acbbcbc', 'bcbbbcbc', 'ba', 'abaaaabb', 'baaaaabb', 'ccaba', 'cacbbcb', 'bbbcccbc', 'ccba', 'ccbaacaa', 'cabbcbc']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.3158286400139332
Train Epoch 1, Loss 0.26586054638028145
Train Epoch 2, Loss 0.23631248995661736
Accuracy at epoch 102: 0.59765625, total training samples: 849
Early stopping at epoch 467. Loss did not improve for 10 epochs.
Generate examples Step 467, Loss 0.41833556255596316
Epoch: 103
Negative Examples
['a', 'bccba', 'cba']
Neg Neg Neg
Positive Examples
['aacba', 'abacaba', 'abaccba', 'abbcccba', 'acaba', 'ba', 'bccaba', 'cbba', 'ccba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aacba', 'bbaaaaca', 'aaccccac', 'abbcccba', 'cbbbaaca', 'abccbcc', 'acaba', 'bbabccb', 'ccbbbabb', 'ba', 'abcccc', 'abcbb', 'bccaba', 'acbcbcbc', 'acaaacc', 'cbba', 'acbcabab', 'bcbcbcba', 'ccba', 'aacaaac', 'cccabbca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.3888009339570999
Train Epoch 1, Loss 0.18988707661628723
Train Epoch 2, Loss 0.09396698698401451
Accuracy at epoch 103: 0.484375, total training samples: 870
Early stopping at epoch 676. Loss did not improve for 10 epochs.
Generate examples Step 676, Loss 1.364498096167425
Epoch: 104
Negative Examples
['a', 'ba', 'bba', 'bbba', 'cbbba']
Neg Neg Neg Neg Neg
Positive Examples
['aacbbba', 'baccbbba', 'ccbbba']
Pos Pos Pos
Counterexamples
['aacbbba', 'abcbbbcb', 'acccabcc', 'ccbbba', 'abaacbcb', 'bbbabacc']
Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.2547297179698944
Train Epoch 1, Loss 0.12251559644937515
Train Epoch 2, Loss 0.06423988938331604
Accuracy at epoch 104: 0.48046875, total training samples: 876
Early stopping at epoch 683. Loss did not improve for 10 epochs.
Generate examples Step 683, Loss 2.0205362587295776
Epoch: 105
Negative Examples
['a', 'aaccabba', 'accabba', 'ba', 'bba', 'cabba', 'ccabba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 105: No counterexamples found, skipped.
Accuracy at epoch 105: 0.4921875, total training samples: 876
Early stopping at epoch 752. Loss did not improve for 10 epochs.
Generate examples Step 752, Loss 2.7611428187346236
Epoch: 106
Negative Examples
['aaccabba', 'abba', 'accabba', 'ba', 'baccabba', 'cabba', 'ccabba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'abaaaaba', 'baccba']
Pos Pos Pos
Train Epoch 0, Loss 3.0912787914276123
Train Epoch 1, Loss 2.598177671432495
Train Epoch 2, Loss 2.1005935668945312
Accuracy at epoch 106: 0.47265625, total training samples: 879
Early stopping at epoch 649. Loss did not improve for 10 epochs.
Generate examples Step 649, Loss 1.1205749853757712
Epoch: 107
Negative Examples
['a', 'aabba', 'abaabba', 'abba', 'abbcabba', 'abccabba', 'ba', 'bba', 'bcabba', 'cabba', 'ccabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'abaaccba', 'bacabbba']
Pos Pos Pos
Train Epoch 0, Loss 1.6379715204238892
Train Epoch 1, Loss 1.2046061754226685
Train Epoch 2, Loss 0.8296012878417969
Accuracy at epoch 107: 0.6015625, total training samples: 882
Early stopping at epoch 699. Loss did not improve for 10 epochs.
Generate examples Step 699, Loss 0.4980192934189524
Epoch: 108
Negative Examples
['abbacaba', 'bba']
Neg Neg
Positive Examples
['accba', 'baaaba', 'baccba', 'bbaacba', 'bbbcacba', 'bbbcccba', 'bbcacba', 'bbcccba', 'cbaccba', 'cbba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['accba', 'abbbabca', 'ccccbbbc', 'bbaacba', 'cbcacac', 'caaaaaba', 'bbbcacba', 'cabcccac', 'bbcabacc', 'bbbcccba', 'ccbbccbb', 'bcacacbc', 'bbcacba', 'cbbcb', 'aacacbbb', 'bbcccba', 'acacbab', 'ccccca', 'cbaccba', 'abcabaab', 'cbcbaac', 'cbba', 'bbaba', 'ccbbaccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.29601601511240005
Train Epoch 1, Loss 0.1198396123945713
Train Epoch 2, Loss 0.05662361904978752
Accuracy at epoch 108: 0.462890625, total training samples: 906
Early stopping at epoch 668. Loss did not improve for 10 epochs.
Generate examples Step 668, Loss 1.5846930765428529
Epoch: 109
Negative Examples
['accbbba', 'ba', 'baccbbba', 'bba', 'bbba', 'cbbba', 'ccbbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baccbbba', 'baaccba', 'baaccaba']
Pos Pos Pos
Train Epoch 0, Loss 2.32841420173645
Train Epoch 1, Loss 1.8068445920944214
Train Epoch 2, Loss 1.3089808225631714
Accuracy at epoch 109: 0.48046875, total training samples: 909
Early stopping at epoch 624. Loss did not improve for 10 epochs.
Generate examples Step 624, Loss 0.6109391857624054
Epoch: 110
Negative Examples
['baababa', 'bba', 'bbba']
Neg Neg Neg
Positive Examples
['aabbba', 'baccbba', 'bbaabbba', 'bbaacbba', 'bbaccbba', 'bcabbba', 'bcacbba']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbba', 'bacababa', 'bcaaacca', 'baccbba', 'baaccaa', 'abcaaaa', 'bbaabbba', 'cacbcaab', 'cccbabaa', 'bbaacbba', 'bacbaccb', 'baccbcca', 'bbaccbba', 'bcaaa', 'babacaa', 'bcabbba', 'cbaba', 'ccaabbb', 'bcacbba', 'cabbcccc', 'abcabbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.28844113647937775
Train Epoch 1, Loss 0.10556823387742043
Train Epoch 2, Loss 0.04962555691599846
Accuracy at epoch 110: 0.48046875, total training samples: 930
Early stopping at epoch 628. Loss did not improve for 10 epochs.
Generate examples Step 628, Loss 2.448896961106026
Epoch: 111
Negative Examples
['a', 'aaccbbba', 'accbbba', 'ba', 'bba', 'cbbba']
Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 111: No counterexamples found, skipped.
Accuracy at epoch 111: 0.470703125, total training samples: 930
Early stopping at epoch 652. Loss did not improve for 10 epochs.
Generate examples Step 652, Loss 2.532246906213337
Epoch: 112
Negative Examples
['a', 'aaccbbba', 'ba', 'baccbbba', 'bba', 'bbba', 'cbbba', 'ccbbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baccbbba', 'abcba', 'baacaba']
Pos Pos Pos
Train Epoch 0, Loss 3.082653760910034
Train Epoch 1, Loss 2.655020236968994
Train Epoch 2, Loss 2.2206287384033203
Accuracy at epoch 112: 0.498046875, total training samples: 933
Early stopping at epoch 594. Loss did not improve for 10 epochs.
Generate examples Step 594, Loss 0.994082064288003
Epoch: 113
Negative Examples
['a', 'aabbba', 'acbbba', 'ba', 'baaabbba', 'bacabbba', 'baccbbba', 'bba', 'bbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaabbba', 'babbaba', 'bacbbcba', 'bacabbba', 'baabbaba', 'abbbaaba', 'baccbbba', 'ababbba', 'babbcba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 1.8313225507736206
Train Epoch 1, Loss 1.4343758821487427
Train Epoch 2, Loss 1.083122730255127
Accuracy at epoch 113: 0.5625, total training samples: 942
Early stopping at epoch 577. Loss did not improve for 10 epochs.
Generate examples Step 577, Loss 0.6816780739898912
Epoch: 114
Negative Examples
['a', 'aba', 'ababa', 'abbba', 'ba', 'baba', 'cbaba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acababa', 'bacababa', 'baccbaba', 'cababa']
Pos Pos Pos Pos
Counterexamples
['ababa', 'abccbbba', 'bacbbba', 'baba', 'bacabbba', 'baacba', 'acababa', 'cccbabab', 'cccbcaca', 'bacababa', 'cbcabb', 'bcbabbcc', 'baccbaba', 'aabacbb', 'ccbbbcab', 'cababa', 'cbaabbba', 'caacbcba']
Pos Pos Pos Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7581774592399597
Train Epoch 1, Loss 0.5789702534675598
Train Epoch 2, Loss 0.5079669654369354
Accuracy at epoch 114: 0.501953125, total training samples: 960
Early stopping at epoch 572. Loss did not improve for 10 epochs.
Generate examples Step 572, Loss 0.6478635419101615
Epoch: 115
Negative Examples
['a', 'aababa', 'aba', 'ba', 'baccbaba', 'bbcababa', 'bcababa']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 115: No counterexamples found, skipped.
Accuracy at epoch 115: 0.533203125, total training samples: 960
Early stopping at epoch 536. Loss did not improve for 10 epochs.
Generate examples Step 536, Loss 0.5918555266364327
Epoch: 116
Negative Examples
['a', 'aba', 'accbaba', 'ba', 'bacbaba', 'bacccaba', 'bbcababa', 'caba', 'cababa', 'ccbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacccaba', 'abaabbba', 'abcba']
Pos Pos Pos
Train Epoch 0, Loss 1.086734414100647
Train Epoch 1, Loss 0.7623478770256042
Train Epoch 2, Loss 0.5034107565879822
Accuracy at epoch 116: 0.625, total training samples: 963
Early stopping at epoch 446. Loss did not improve for 10 epochs.
Generate examples Step 446, Loss 0.3344056072084429
Epoch: 117
Negative Examples
['a', 'aba']
Neg Neg
Positive Examples
['abcbaba', 'ba', 'baba', 'babaaaba', 'babababa', 'bbaccba', 'bbbababa', 'bbccaba', 'bcaaba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abcbaba', 'bcaabcbc', 'baacbaa', 'ba', 'baacccc', 'bacacbb', 'babaaaba', 'caacbbba', 'baacbaab', 'babababa', 'babcbbcc', 'bcaabbca', 'bbaccba', 'aaabccac', 'aababcbc', 'bbbababa', 'bccaabbb', 'cbbcbbbb', 'bbccaba', 'bcbcbaac', 'aaaacaab', 'bcaaba', 'abbcbba', 'cacbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.45816856622695923
Train Epoch 1, Loss 0.2278248518705368
Train Epoch 2, Loss 0.11773360520601273
Accuracy at epoch 117: 0.484375, total training samples: 987
Early stopping at epoch 653. Loss did not improve for 10 epochs.
Generate examples Step 653, Loss 1.1853793679938769
Epoch: 118
Negative Examples
['a', 'aaacbba', 'abbba', 'ba', 'baabbba', 'bacbbba', 'bba', 'bbaabbba', 'bbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baabbba', 'bacccaba', 'babbacba', 'bacbbba', 'abcba', 'abaaccba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 1.8636161088943481
Train Epoch 1, Loss 1.4948863983154297
Train Epoch 2, Loss 1.1560015678405762
Accuracy at epoch 118: 0.578125, total training samples: 993
Early stopping at epoch 677. Loss did not improve for 10 epochs.
Generate examples Step 677, Loss 0.5285009377166233
Epoch: 119
Negative Examples
['ba', 'baacaba', 'bba']
Neg Neg Neg
Positive Examples
['aacaba', 'aaccba', 'accba', 'baaccba', 'bbaabcba', 'bbaacaba', 'bbba', 'cbba']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baacaba', 'bacacaba', 'abaaaaba', 'aacaba', 'abbbbabc', 'bccbacb', 'aaccba', 'abbcaaca', 'baacbacb', 'accba', 'aababcac', 'bcbbcacb', 'bbaabcba', 'cbcbabab', 'ccabcbca', 'bbaacaba', 'cabcbab', 'bcbcbcaa', 'bbba', 'aacbbbac', 'ccabab', 'cbba', 'cccabac', 'ccbacca']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.4855963736772537
Train Epoch 1, Loss 0.3887822926044464
Train Epoch 2, Loss 0.34145548939704895
Accuracy at epoch 119: 0.591796875, total training samples: 1017
Early stopping at epoch 450. Loss did not improve for 10 epochs.
Generate examples Step 450, Loss 0.36320116944165026
Epoch: 120
Negative Examples
['a']
Neg
Positive Examples
['aba', 'ababa', 'abacaba', 'babacaba', 'babccaba', 'bacaba', 'bbbababa', 'bbbcbaba', 'bbcbaba', 'caba', 'ccaba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aba', 'baabbcca', 'abaacbba', 'babacaba', 'bbabbccc', 'bababcaa', 'babccaba', 'abbacbcc', 'baacbcbc', 'bbbababa', 'cabbccba', 'caacccba', 'bbbcbaba', 'bbcbacc', 'ccacacba', 'bbcbaba', 'aacabbba', 'babaacca', 'caba', 'acbacbaa', 'bccbcca', 'ccaba', 'acacaacc', 'cbccccac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.4519614726305008
Train Epoch 1, Loss 0.22292885184288025
Train Epoch 2, Loss 0.11528921127319336
Accuracy at epoch 120: 0.5234375, total training samples: 1041
Early stopping at epoch 619. Loss did not improve for 10 epochs.
Generate examples Step 619, Loss 1.1914058089256288
Epoch: 121
Negative Examples
['a', 'acba', 'ba', 'baacba', 'bbbcacba', 'bbccacba', 'bccacba', 'cacba', 'cba', 'ccacba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baacba', 'bacba', 'bacccaba']
Pos Pos Pos
Train Epoch 0, Loss 1.977547287940979
Train Epoch 1, Loss 1.578107476234436
Train Epoch 2, Loss 1.1970558166503906
Accuracy at epoch 121: 0.513671875, total training samples: 1044
Early stopping at epoch 612. Loss did not improve for 10 epochs.
Generate examples Step 612, Loss 0.8689934272945025
Epoch: 122
Negative Examples
['a', 'acba', 'acbaaba', 'ba', 'babbacba', 'cba', 'cbaaba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acbacba', 'babcacba', 'bacbacba', 'bbacba']
Pos Pos Pos Pos
Counterexamples
['babbacba', 'abaaaaba', 'abacba', 'acbacba', 'accbcbb', 'cbccbcb', 'babcacba', 'abcbabcb', 'bbaaabac', 'bacbacba', 'abbabccc', 'ca', 'bbacba', 'bbbaccac', 'cbaabca']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5661771297454834
Train Epoch 1, Loss 0.48772045969963074
Train Epoch 2, Loss 0.4409574270248413
Accuracy at epoch 122: 0.650390625, total training samples: 1059
Early stopping at epoch 551. Loss did not improve for 10 epochs.
Generate examples Step 551, Loss 0.4652329244367454
Epoch: 123
Negative Examples
['a', 'bacbcba']
Neg Neg
Positive Examples
['aba', 'acbaaba', 'baaba', 'babaaba', 'bacbaaba', 'bbbcaba', 'cbbcaaba', 'cbcbaaba']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aba', 'babbccca', 'abbbaaaa', 'acbaaba', 'bbacaba', 'bcbcbbba', 'babaaba', 'acccaaa', 'caaccbba', 'bacbaaba', 'baabcbb', 'bbaaaba', 'bbbcaba', 'cccabba', 'bbbbbac', 'cbbcaaba', 'bcacbbaa', 'bbabaacc', 'cbcbaaba', 'cbbaaba', 'cbcbaac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5241752564907074
Train Epoch 1, Loss 0.251904733479023
Train Epoch 2, Loss 0.1272827833890915
Accuracy at epoch 123: 0.46875, total training samples: 1080
Early stopping at epoch 632. Loss did not improve for 10 epochs.
Generate examples Step 632, Loss 1.3022066399949421
Epoch: 124
Negative Examples
['a', 'aaacba', 'aacba', 'aaccba', 'acba', 'accba', 'ba', 'baaacba', 'baaccba', 'cba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaacba', 'baacccba', 'abbbacba', 'baaccba', 'baccaba', 'abcba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 1.7074198722839355
Train Epoch 1, Loss 1.3303837776184082
Train Epoch 2, Loss 0.9870412945747375
Accuracy at epoch 124: 0.587890625, total training samples: 1086
Early stopping at epoch 474. Loss did not improve for 10 epochs.
Generate examples Step 474, Loss 0.6128350516996885
Epoch: 125
Negative Examples
['a', 'ba', 'baaaba', 'ccba']
Neg Neg Neg Neg
Positive Examples
['aacba', 'baacba', 'bbaccba', 'bbbacaba', 'bbbcccba']
Pos Pos Pos Pos Pos
Counterexamples
['baaaba', 'abccaba', 'baacaba', 'aacba', 'cccabbcc', 'acbcbcca', 'bbaccba', 'aaccbcac', 'baaacbcc', 'bbbacaba', 'ccbbabbb', 'cbccaaaa', 'bbbcccba', 'acacbacb', 'acabacac']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5779216289520264
Train Epoch 1, Loss 0.48450908064842224
Train Epoch 2, Loss 0.4245564043521881
Accuracy at epoch 125: 0.6171875, total training samples: 1101
Early stopping at epoch 358. Loss did not improve for 10 epochs.
Generate examples Step 358, Loss 0.344178768751681
Epoch: 126
Negative Examples
['a', 'aba', 'acabb', 'bababb', 'bbabcbb', 'cba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['ababa', 'ba', 'bbabcba', 'bcbaba', 'caba', 'cbaba', 'cbcba']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ba', 'abaaccbb', 'abcccc', 'bbabcba', 'cabbaaaa', 'cababc', 'bcbaba', 'cbbbacaa', 'abcbcac', 'caba', 'cbbaabcb', 'aacacacb', 'cbaba', 'caacaa', 'ababccb', 'cbcba', 'bbabca', 'cbccbaac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.27727428264915943
Train Epoch 1, Loss 0.16290316730737686
Train Epoch 2, Loss 0.09934144467115402
Accuracy at epoch 126: 0.5078125, total training samples: 1119
Early stopping at epoch 631. Loss did not improve for 10 epochs.
Generate examples Step 631, Loss 0.9435105718012098
Epoch: 127
Negative Examples
['a', 'aaacba', 'aaccba', 'acacba', 'acccba', 'ba', 'baaaaba', 'baaccba', 'bacacba', 'bbaaccba', 'bbacccba', 'ccba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaaaba', 'bacacaba', 'abccba', 'baaccba', 'babbaba', 'abcacba', 'bacacba', 'babbacba', 'abaaacba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 1.5844745635986328
Train Epoch 1, Loss 1.2191147804260254
Train Epoch 2, Loss 0.897176206111908
Accuracy at epoch 127: 0.546875, total training samples: 1128
Early stopping at epoch 474. Loss did not improve for 10 epochs.
Generate examples Step 474, Loss 0.460209312407594
Epoch: 128
Negative Examples
['a', 'ba', 'cba']
Neg Neg Neg
Positive Examples
['aacaba', 'acaaba', 'baaaba', 'bacaaba', 'baccaba', 'bbaaaaba', 'bbaacaba', 'caaba', 'cbacacba', 'ccba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aacaba', 'abaaacbc', 'bbbaccac', 'acaaba', 'ccccca', 'ccabbb', 'bbaaaaba', 'cbaccbab', 'abbccba', 'bbaacaba', 'cbccacb', 'bbabb', 'caaba', 'bcaccacc', 'caacbccb', 'cbacacba', 'baccbc', 'bbbccbaa', 'ccba', 'bbcbcaca', 'bbcbbccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.2988836318254471
Train Epoch 1, Loss 0.14559772983193398
Train Epoch 2, Loss 0.07786322943866253
Accuracy at epoch 128: 0.494140625, total training samples: 1149
Early stopping at epoch 645. Loss did not improve for 10 epochs.
Generate examples Step 645, Loss 1.3878839957086664
Epoch: 129
Negative Examples
['acba', 'accacba', 'ba', 'cacba', 'cba', 'ccacba']
Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 129: No counterexamples found, skipped.
Accuracy at epoch 129: 0.50390625, total training samples: 1149
Early stopping at epoch 640. Loss did not improve for 10 epochs.
Generate examples Step 640, Loss 1.5587690975662327
Epoch: 130
Negative Examples
['a', 'acba', 'accacba', 'ba', 'bbccacba', 'bccacba', 'caacba', 'cba', 'ccacba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 130: No counterexamples found, skipped.
Accuracy at epoch 130: 0.509765625, total training samples: 1149
Early stopping at epoch 652. Loss did not improve for 10 epochs.
Generate examples Step 652, Loss 1.7001510774559851
Epoch: 131
Negative Examples
['a', 'acba', 'accacba', 'ba', 'bacaacba', 'baccacba', 'cacba', 'cba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacaacba', 'babbbbba', 'baaaacba', 'baccacba', 'abccba', 'baaaccba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 2.2003705501556396
Train Epoch 1, Loss 1.7939866781234741
Train Epoch 2, Loss 1.3915899991989136
Accuracy at epoch 131: 0.46484375, total training samples: 1155
Early stopping at epoch 555. Loss did not improve for 10 epochs.
Generate examples Step 555, Loss 0.7242078631580304
Epoch: 132
Negative Examples
['a', 'acba', 'acbacba', 'ba', 'bacba', 'bacbacba', 'baccacba', 'bbabacba', 'cba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacba', 'abaaba', 'baccacba', 'baccacba', 'abccba', 'bacabbba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 1.0438584089279175
Train Epoch 1, Loss 0.7097139954566956
Train Epoch 2, Loss 0.45134422183036804
Accuracy at epoch 132: 0.5703125, total training samples: 1161
Early stopping at epoch 516. Loss did not improve for 10 epochs.
Generate examples Step 516, Loss 0.462003562275169
Epoch: 133
Negative Examples
['bacccba']
Neg
Positive Examples
['aaba', 'acacba', 'acba', 'ba', 'baaba', 'baabaaba', 'babacba', 'bacacba', 'caaba', 'cacba', 'cba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['bacccba', 'baabbcba', 'abcbbaba', 'aaba', 'cbcabcca', 'cccbcacb', 'acacba', 'ccccab', 'bcbbcaab', 'acba', 'cacaabc', 'babbbcac', 'ba', 'bacabbbb', 'abacaacc', 'baabaaba', 'bcaaaacc', 'bbcbbcb', 'babacba', 'bbcaacbc', 'caacaaab', 'caaba', 'cbcbaab', 'caa', 'cacba', 'abbaaaab', 'ccbbaab', 'cba', 'ccaaaaba', 'babbabaa']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5980728268623352
Train Epoch 1, Loss 0.4354395419359207
Train Epoch 2, Loss 0.35195185244083405
Accuracy at epoch 133: 0.578125, total training samples: 1191
Early stopping at epoch 471. Loss did not improve for 10 epochs.
Generate examples Step 471, Loss 0.41476533151531625
Epoch: 134
Negative Examples
['aba', 'abacbaca', 'baba', 'bcba']
Neg Neg Neg Neg
Positive Examples
['abaababa', 'abbcbaba', 'ba', 'baababa', 'bacbcba']
Pos Pos Pos Pos Pos
Counterexamples
['baba', 'baaaacba', 'abcbbaba', 'abaababa', 'cccacbcc', 'acabbaab', 'abbcbaba', 'cacabcba', 'acbbaab', 'ba', 'babbbbbb', 'baccabb', 'baababa', 'cbacacba', 'ccbabacb', 'bacbcba', 'abaaabaa', 'cbcacaac']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.31288576126098633
Train Epoch 1, Loss 0.27336781565099955
Train Epoch 2, Loss 0.2467464478686452
Accuracy at epoch 134: 0.701171875, total training samples: 1209
Early stopping at epoch 467. Loss did not improve for 10 epochs.
Generate examples Step 467, Loss 0.35828486685123706
Epoch: 135
Negative Examples
['aabbaca', 'ababa', 'accbaca']
Neg Neg Neg
Positive Examples
['aaba', 'aba', 'abbaba', 'ba', 'baba', 'babbcba', 'cababa', 'cabbaba']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababa', 'abcbbaba', 'abacaba', 'aaba', 'acaaabbb', 'caabcabc', 'aba', 'baacaaa', 'abcccbba', 'abbaba', 'aaacbaaa', 'ccabba', 'ba', 'ba', 'bacbb', 'cababa', 'ccababab', 'aacaca', 'cabbaba', 'bcbbbbcb', 'abbabacc']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.48512884974479675
Train Epoch 1, Loss 0.38638633489608765
Train Epoch 2, Loss 0.3459186479449272
Accuracy at epoch 135: 0.56640625, total training samples: 1230
Early stopping at epoch 450. Loss did not improve for 10 epochs.
Generate examples Step 450, Loss 0.5745776992150791
Epoch: 136
Negative Examples
['a', 'baba', 'cbaba']
Neg Neg Neg
Positive Examples
['abababa', 'abcba', 'ba', 'bababa', 'babcba', 'bcba', 'cababa', 'cba']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baba', 'bacaba', 'bacaaba', 'abababa', 'bcabcc', 'abbaabcb', 'ba', 'abaabbc', 'baaacbb', 'bababa', 'ccabbacb', 'bcccbcca', 'babcba', 'accaabac', 'ccaacacc', 'bcba', 'aabbbacc', 'caaccbac', 'cababa', 'cabb', 'bbaacaba', 'cba', 'acaaa', 'cabbaacc']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5442010760307312
Train Epoch 1, Loss 0.4656536132097244
Train Epoch 2, Loss 0.42790597677230835
Accuracy at epoch 136: 0.65625, total training samples: 1254
Early stopping at epoch 534. Loss did not improve for 10 epochs.
Generate examples Step 534, Loss 0.5763763351139621
Epoch: 137
Negative Examples
['a', 'aba']
Neg Neg
Positive Examples
['aacbaba', 'accbaba', 'ba', 'baaababa', 'baccbaba', 'baccbcba', 'caccbcba', 'cbaba', 'cbcba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aacbaba', 'cbcacbbb', 'aabaccba', 'accbaba', 'babacaac', 'cbcbaa', 'ba', 'abccaacc', 'baccacbb', 'baaababa', 'cccbaab', 'abacbcbb', 'baccbaba', 'accacbba', 'bccabaca', 'baccbcba', 'cabcbabb', 'cccbcbba', 'caccbcba', 'cbbaccca', 'cccbcabc', 'cbaba', 'bccaaccc', 'bbaccccb', 'cbcba', 'acbbb', 'cbbcaccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.3643937483429909
Train Epoch 1, Loss 0.17348453775048256
Train Epoch 2, Loss 0.08860054612159729
Accuracy at epoch 137: 0.501953125, total training samples: 1281
Early stopping at epoch 692. Loss did not improve for 10 epochs.
Generate examples Step 692, Loss 1.3851225325383494
Epoch: 138
Negative Examples
['a', 'aabaacba', 'aacba', 'acba', 'ba', 'baacba', 'cba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baacba', 'babbacba', 'abbbcba']
Pos Pos Pos
Train Epoch 0, Loss 1.8418091535568237
Train Epoch 1, Loss 1.3556972742080688
Train Epoch 2, Loss 0.9255919456481934
Accuracy at epoch 138: 0.5625, total training samples: 1284
Early stopping at epoch 502. Loss did not improve for 10 epochs.
Generate examples Step 502, Loss 0.609989634091053
Epoch: 139
Negative Examples
['a', 'ba', 'cba']
Neg Neg Neg
Positive Examples
['abbacba', 'acba', 'bacba', 'bacbacba', 'cbacba']
Pos Pos Pos Pos Pos
Counterexamples
['abbacba', 'abcabaac', 'bbbabacc', 'acba', 'ccacccaa', 'bbccbbac', 'bacbacba', 'cabcbac', 'bcbcbab', 'cbacba', 'aaccbca', 'bccaabbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.34320592880249023
Train Epoch 1, Loss 0.20612187683582306
Train Epoch 2, Loss 0.12292969971895218
Accuracy at epoch 139: 0.490234375, total training samples: 1296
Early stopping at epoch 681. Loss did not improve for 10 epochs.
Generate examples Step 681, Loss 1.2926807522424155
Epoch: 140
Negative Examples
['a', 'aabaacba', 'aacba', 'abaacba', 'acba', 'ba', 'baacba', 'cba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaacba', 'baccaba', 'abbbbbba', 'baacba', 'abaacaba', 'babbaba']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 2.0677359104156494
Train Epoch 1, Loss 1.7052024602890015
Train Epoch 2, Loss 1.3648570775985718
Accuracy at epoch 140: 0.541015625, total training samples: 1302
Early stopping at epoch 581. Loss did not improve for 10 epochs.
Generate examples Step 581, Loss 0.46958763254774394
Epoch: 141
Negative Examples
['abaaaba', 'acba', 'ba', 'cba', 'ccba']
Neg Neg Neg Neg Neg
Positive Examples
['aabaacba', 'aacba', 'abaacba', 'abbacaba', 'baacba', 'bacaba', 'bbaaaba']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaaaba', 'babbba', 'abbbccba', 'aabaacba', 'bccccabb', 'cbbbaccc', 'aacba', 'cbbbabb', 'aacbcbca', 'abbacaba', 'ccabbcb', 'ccbcbacc', 'bbaaaba', 'ccaccbab', 'ccbbcaca']
Pos Pos Pos Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.5498903393745422
Train Epoch 1, Loss 0.4771752953529358
Train Epoch 2, Loss 0.4567359685897827
Accuracy at epoch 141: 0.6484375, total training samples: 1317
Early stopping at epoch 532. Loss did not improve for 10 epochs.
Generate examples Step 532, Loss 0.41528735792100097
Epoch: 142
Negative Examples
['a', 'accca']
Neg Neg
Positive Examples
['aaaba', 'aacba', 'abcba', 'acaba', 'ba', 'caaaba', 'cba', 'cbacaba', 'cbaccba', 'cbaccbb', 'ccba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaba', 'ccccbcca', 'bcacccb', 'aacba', 'ccbbbbbb', 'bcbcbca', 'acaba', 'cccaaccb', 'bccbabbc', 'ba', 'abbbaacc', 'ababbacc', 'caaaba', 'cbabbbb', 'ccabcba', 'cba', 'caccaacb', 'cabbaccc', 'cbacaba', 'bbcbabbc', 'bccaacba', 'cbaccba', 'cabababc', 'bccaaacc', 'cbaccbb', 'abcbacba', 'cbbbacbc', 'ccba', 'bcaccabb', 'aacacbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.49802710115909576
Train Epoch 1, Loss 0.22649669647216797
Train Epoch 2, Loss 0.10349849238991737
Accuracy at epoch 142: 0.50390625, total training samples: 1347
Early stopping at epoch 622. Loss did not improve for 10 epochs.
Generate examples Step 622, Loss 1.0521442088422577
Epoch: 143
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'bacbba', 'bba', 'bbaacbba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 143: No counterexamples found, skipped.
Accuracy at epoch 143: 0.509765625, total training samples: 1347
Early stopping at epoch 665. Loss did not improve for 10 epochs.
Generate examples Step 665, Loss 0.939832975825032
Epoch: 144
Negative Examples
['a', 'ba', 'baccbba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'bbbacbba', 'bccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 144: No counterexamples found, skipped.
Accuracy at epoch 144: 0.4921875, total training samples: 1347
Early stopping at epoch 665. Loss did not improve for 10 epochs.
Generate examples Step 665, Loss 1.2234298727354846
Epoch: 145
Negative Examples
['a', 'acbba', 'accbba', 'ba', 'baacbba', 'bba', 'bbbacbba', 'bbccbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 145: No counterexamples found, skipped.
Accuracy at epoch 145: 0.490234375, total training samples: 1347
Early stopping at epoch 641. Loss did not improve for 10 epochs.
Generate examples Step 641, Loss 1.16629366200661
Epoch: 146
Negative Examples
['a', 'acbba', 'ba', 'bacbba', 'baccbba', 'bba', 'bbacbba', 'bbbacbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 146: No counterexamples found, skipped.
Accuracy at epoch 146: 0.470703125, total training samples: 1347
Early stopping at epoch 679. Loss did not improve for 10 epochs.
Generate examples Step 679, Loss 1.2348776580656275
Epoch: 147
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'bba', 'bbacbba', 'bbaccbba', 'bbbccbba', 'bccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 147: No counterexamples found, skipped.
Accuracy at epoch 147: 0.501953125, total training samples: 1347
Early stopping at epoch 673. Loss did not improve for 10 epochs.
Generate examples Step 673, Loss 1.323274065019115
Epoch: 148
Negative Examples
['a', 'aacbba', 'ba', 'bacbba', 'bba', 'bbaccbba', 'bbbacbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 148: No counterexamples found, skipped.
Accuracy at epoch 148: 0.501953125, total training samples: 1347
Early stopping at epoch 612. Loss did not improve for 10 epochs.
Generate examples Step 612, Loss 0.9794629771402181
Epoch: 149
Negative Examples
['a', 'abacbba', 'acbba', 'accbba', 'ba', 'baacbba', 'bacbba', 'bba', 'bbbacbba', 'bbbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 149: No counterexamples found, skipped.
Accuracy at epoch 149: 0.49609375, total training samples: 1347
Early stopping at epoch 575. Loss did not improve for 10 epochs.
Generate examples Step 575, Loss 0.8150133760645986
Epoch: 150
Negative Examples
['a', 'abaacbba', 'ba', 'bacbba', 'bba', 'bbaacbba', 'bbacbba', 'bbbccbba', 'bbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 150: No counterexamples found, skipped.
Accuracy at epoch 150: 0.462890625, total training samples: 1347
Early stopping at epoch 715. Loss did not improve for 10 epochs.
Generate examples Step 715, Loss 1.4022790109478562
Epoch: 151
Negative Examples
['aacbba', 'ba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 151: No counterexamples found, skipped.
Accuracy at epoch 151: 0.5, total training samples: 1347
Early stopping at epoch 691. Loss did not improve for 10 epochs.
Generate examples Step 691, Loss 1.4885711933491548
Epoch: 152
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'ba', 'baccbba', 'bba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 152: No counterexamples found, skipped.
Accuracy at epoch 152: 0.513671875, total training samples: 1347
Early stopping at epoch 606. Loss did not improve for 10 epochs.
Generate examples Step 606, Loss 0.9820041879397837
Epoch: 153
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'baacbba', 'bba', 'bbaacbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 153: No counterexamples found, skipped.
Accuracy at epoch 153: 0.4921875, total training samples: 1347
Early stopping at epoch 662. Loss did not improve for 10 epochs.
Generate examples Step 662, Loss 1.4618080706977556
Epoch: 154
Negative Examples
['a', 'acbba', 'accbba', 'ba', 'bacbba', 'bba', 'bbacbba', 'bbbacbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 154: No counterexamples found, skipped.
Accuracy at epoch 154: 0.484375, total training samples: 1347
Early stopping at epoch 678. Loss did not improve for 10 epochs.
Generate examples Step 678, Loss 1.3792512006893074
Epoch: 155
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'bbacbba', 'bbbacbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 155: No counterexamples found, skipped.
Accuracy at epoch 155: 0.533203125, total training samples: 1347
Early stopping at epoch 597. Loss did not improve for 10 epochs.
Generate examples Step 597, Loss 0.9730580082306495
Epoch: 156
Negative Examples
['a', 'abaacbba', 'ba', 'baccbba', 'bba', 'bbbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 156: No counterexamples found, skipped.
Accuracy at epoch 156: 0.46875, total training samples: 1347
Early stopping at epoch 669. Loss did not improve for 10 epochs.
Generate examples Step 669, Loss 1.1558668500451899
Epoch: 157
Negative Examples
['a', 'accbba', 'ba', 'baacbba', 'bacbba', 'baccbba', 'bba', 'bbaacbba', 'bbbacbba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 157: No counterexamples found, skipped.
Accuracy at epoch 157: 0.501953125, total training samples: 1347
Early stopping at epoch 668. Loss did not improve for 10 epochs.
Generate examples Step 668, Loss 1.2217863953167012
Epoch: 158
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'bacbba', 'baccbba', 'bba', 'bbaccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 158: No counterexamples found, skipped.
Accuracy at epoch 158: 0.4921875, total training samples: 1347
Early stopping at epoch 678. Loss did not improve for 10 epochs.
Generate examples Step 678, Loss 1.2886806362156313
Epoch: 159
Negative Examples
['a', 'accbba', 'baccbba', 'bba', 'bbaccbba', 'bbbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 159: No counterexamples found, skipped.
Accuracy at epoch 159: 0.5, total training samples: 1347
Early stopping at epoch 651. Loss did not improve for 10 epochs.
Generate examples Step 651, Loss 1.1058055599408647
Epoch: 160
Negative Examples
['a', 'ba', 'baacbba', 'baccbba', 'bba', 'bbacbba', 'bbbacbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 160: No counterexamples found, skipped.
Accuracy at epoch 160: 0.46875, total training samples: 1347
Early stopping at epoch 659. Loss did not improve for 10 epochs.
Generate examples Step 659, Loss 1.376285922346693
Epoch: 161
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'baccbba', 'bba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 161: No counterexamples found, skipped.
Accuracy at epoch 161: 0.474609375, total training samples: 1347
Early stopping at epoch 613. Loss did not improve for 10 epochs.
Generate examples Step 613, Loss 0.8676020244822051
Epoch: 162
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'baacbba', 'bba', 'bbaccbba', 'bbbacbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 162: No counterexamples found, skipped.
Accuracy at epoch 162: 0.45703125, total training samples: 1347
Early stopping at epoch 639. Loss did not improve for 10 epochs.
Generate examples Step 639, Loss 1.1090416125953197
Epoch: 163
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'baccbba', 'bba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 163: No counterexamples found, skipped.
Accuracy at epoch 163: 0.48828125, total training samples: 1347
Early stopping at epoch 652. Loss did not improve for 10 epochs.
Generate examples Step 652, Loss 1.073066049932886
Epoch: 164
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'ba', 'bba', 'bbbacbba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 164: No counterexamples found, skipped.
Accuracy at epoch 164: 0.501953125, total training samples: 1347
Early stopping at epoch 662. Loss did not improve for 10 epochs.
Generate examples Step 662, Loss 1.1600399356262356
Epoch: 165
Negative Examples
['a', 'aacbba', 'baacbba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 165: No counterexamples found, skipped.
Accuracy at epoch 165: 0.533203125, total training samples: 1347
Early stopping at epoch 630. Loss did not improve for 10 epochs.
Generate examples Step 630, Loss 1.1705358001962896
Epoch: 166
Negative Examples
['a', 'acbba', 'accbba', 'ba', 'bacbba', 'baccbba', 'bba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 166: No counterexamples found, skipped.
Accuracy at epoch 166: 0.49609375, total training samples: 1347
Early stopping at epoch 625. Loss did not improve for 10 epochs.
Generate examples Step 625, Loss 1.0074743171469471
Epoch: 167
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'bacbba', 'baccbba', 'bba', 'bbaccbba', 'bbbacbba', 'bbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 167: No counterexamples found, skipped.
Accuracy at epoch 167: 0.494140625, total training samples: 1347
Early stopping at epoch 593. Loss did not improve for 10 epochs.
Generate examples Step 593, Loss 0.7660391747650473
Epoch: 168
Negative Examples
['a', 'acbba', 'accbba', 'baacbba', 'bacbba', 'bba', 'bbaacbba', 'bbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 168: No counterexamples found, skipped.
Accuracy at epoch 168: 0.484375, total training samples: 1347
Early stopping at epoch 616. Loss did not improve for 10 epochs.
Generate examples Step 616, Loss 0.9930362834729484
Epoch: 169
Negative Examples
['a', 'ba', 'bacbba', 'baccbba', 'bba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'bccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 169: No counterexamples found, skipped.
Accuracy at epoch 169: 0.484375, total training samples: 1347
Early stopping at epoch 652. Loss did not improve for 10 epochs.
Generate examples Step 652, Loss 1.0701443013516905
Epoch: 170
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'baacbba', 'bba', 'bbaccbba', 'bbbccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 170: No counterexamples found, skipped.
Accuracy at epoch 170: 0.498046875, total training samples: 1347
Early stopping at epoch 696. Loss did not improve for 10 epochs.
Generate examples Step 696, Loss 1.3646838169870965
Epoch: 171
Negative Examples
['acbba', 'accbba', 'ba', 'bba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 171: No counterexamples found, skipped.
Accuracy at epoch 171: 0.5, total training samples: 1347
Early stopping at epoch 650. Loss did not improve for 10 epochs.
Generate examples Step 650, Loss 0.997746075078639
Epoch: 172
Negative Examples
['a', 'aaacbba', 'aacbba', 'ba', 'bba', 'bbacbba', 'bbaccbba', 'bccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 172: No counterexamples found, skipped.
Accuracy at epoch 172: 0.525390625, total training samples: 1347
Early stopping at epoch 660. Loss did not improve for 10 epochs.
Generate examples Step 660, Loss 1.09610907113498
Epoch: 173
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'baacbba', 'bba', 'bbacbba', 'bbaccbba', 'bbbacbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 173: No counterexamples found, skipped.
Accuracy at epoch 173: 0.50390625, total training samples: 1347
Early stopping at epoch 655. Loss did not improve for 10 epochs.
Generate examples Step 655, Loss 1.112254859289018
Epoch: 174
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'baacbba', 'baccbba', 'bbaacbba', 'bbacbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 174: No counterexamples found, skipped.
Accuracy at epoch 174: 0.52734375, total training samples: 1347
Early stopping at epoch 597. Loss did not improve for 10 epochs.
Generate examples Step 597, Loss 1.0174515852362018
Epoch: 175
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'bacbba', 'bba', 'bbaacbba', 'bbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 175: No counterexamples found, skipped.
Accuracy at epoch 175: 0.484375, total training samples: 1347
Early stopping at epoch 630. Loss did not improve for 10 epochs.
Generate examples Step 630, Loss 1.130026859734591
Epoch: 176
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'bba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 176: No counterexamples found, skipped.
Accuracy at epoch 176: 0.53125, total training samples: 1347
Early stopping at epoch 648. Loss did not improve for 10 epochs.
Generate examples Step 648, Loss 1.3647406040611914
Epoch: 177
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'bba', 'bbaacbba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 177: No counterexamples found, skipped.
Accuracy at epoch 177: 0.505859375, total training samples: 1347
Early stopping at epoch 729. Loss did not improve for 10 epochs.
Generate examples Step 729, Loss 1.5567230843517879
Epoch: 178
Negative Examples
['a', 'aacbba', 'ba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'bbbacbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 178: No counterexamples found, skipped.
Accuracy at epoch 178: 0.451171875, total training samples: 1347
Early stopping at epoch 662. Loss did not improve for 10 epochs.
Generate examples Step 662, Loss 1.0190883417294936
Epoch: 179
Negative Examples
['a', 'abaccbba', 'accbba', 'ba', 'bbacbba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 179: No counterexamples found, skipped.
Accuracy at epoch 179: 0.494140625, total training samples: 1347
Early stopping at epoch 676. Loss did not improve for 10 epochs.
Generate examples Step 676, Loss 1.1224692754414445
Epoch: 180
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'ba', 'bacbba', 'bba', 'bbbacbba', 'bbbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 180: No counterexamples found, skipped.
Accuracy at epoch 180: 0.47265625, total training samples: 1347
Early stopping at epoch 633. Loss did not improve for 10 epochs.
Generate examples Step 633, Loss 1.0836071120075623
Epoch: 181
Negative Examples
['a', 'ba', 'bacbba', 'baccbba', 'bba', 'bbacbba', 'bbaccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 181: No counterexamples found, skipped.
Accuracy at epoch 181: 0.509765625, total training samples: 1347
Early stopping at epoch 672. Loss did not improve for 10 epochs.
Generate examples Step 672, Loss 1.144161120389227
Epoch: 182
Negative Examples
['a', 'ba', 'baacbba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'bccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 182: No counterexamples found, skipped.
Accuracy at epoch 182: 0.494140625, total training samples: 1347
Early stopping at epoch 689. Loss did not improve for 10 epochs.
Generate examples Step 689, Loss 1.2724172447902569
Epoch: 183
Negative Examples
['a', 'acbba', 'accbba', 'ba', 'bba', 'bbaacbba', 'bbaccbba', 'bbbccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 183: No counterexamples found, skipped.
Accuracy at epoch 183: 0.521484375, total training samples: 1347
Early stopping at epoch 629. Loss did not improve for 10 epochs.
Generate examples Step 629, Loss 1.077303537868318
Epoch: 184
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'bba', 'bbbacbba', 'bbbccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 184: No counterexamples found, skipped.
Accuracy at epoch 184: 0.474609375, total training samples: 1347
Early stopping at epoch 641. Loss did not improve for 10 epochs.
Generate examples Step 641, Loss 1.1119289471538638
Epoch: 185
Negative Examples
['a', 'ba', 'bacbba', 'baccbba', 'bba', 'bbacbba', 'bbbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 185: No counterexamples found, skipped.
Accuracy at epoch 185: 0.509765625, total training samples: 1347
Early stopping at epoch 684. Loss did not improve for 10 epochs.
Generate examples Step 684, Loss 1.5716087398737886
Epoch: 186
Negative Examples
['a', 'aacbba', 'ba', 'bba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 186: No counterexamples found, skipped.
Accuracy at epoch 186: 0.4921875, total training samples: 1347
Early stopping at epoch 698. Loss did not improve for 10 epochs.
Generate examples Step 698, Loss 1.4225188311248038
Epoch: 187
Negative Examples
['a', 'accbba', 'bacbba', 'bba', 'bbacbba', 'bbaccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 187: No counterexamples found, skipped.
Accuracy at epoch 187: 0.498046875, total training samples: 1347
Early stopping at epoch 683. Loss did not improve for 10 epochs.
Generate examples Step 683, Loss 1.2939155360585766
Epoch: 188
Negative Examples
['a', 'ba', 'baacbba', 'bacbba', 'bba', 'bbccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 188: No counterexamples found, skipped.
Accuracy at epoch 188: 0.48046875, total training samples: 1347
Early stopping at epoch 669. Loss did not improve for 10 epochs.
Generate examples Step 669, Loss 1.0514370975209706
Epoch: 189
Negative Examples
['aacbba', 'acbba', 'accbba', 'ba', 'baccbba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'bbbacbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 189: No counterexamples found, skipped.
Accuracy at epoch 189: 0.498046875, total training samples: 1347
Early stopping at epoch 681. Loss did not improve for 10 epochs.
Generate examples Step 681, Loss 1.2008827328681946
Epoch: 190
Negative Examples
['aacbba', 'ba', 'baccbba', 'bba', 'bbaacbba', 'bbbacbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 190: No counterexamples found, skipped.
Accuracy at epoch 190: 0.458984375, total training samples: 1347
Early stopping at epoch 624. Loss did not improve for 10 epochs.
Generate examples Step 624, Loss 1.1229529528617859
Epoch: 191
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'baacbba', 'bacbba', 'bba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 191: No counterexamples found, skipped.
Accuracy at epoch 191: 0.51171875, total training samples: 1347
Early stopping at epoch 614. Loss did not improve for 10 epochs.
Generate examples Step 614, Loss 0.842237642577024
Epoch: 192
Negative Examples
['a', 'ba', 'bacbba', 'bba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 192: No counterexamples found, skipped.
Accuracy at epoch 192: 0.458984375, total training samples: 1347
Early stopping at epoch 714. Loss did not improve for 10 epochs.
Generate examples Step 714, Loss 1.3340782720725852
Epoch: 193
Negative Examples
['aacbba', 'accbba', 'ba', 'bba', 'bbaacbba', 'bbacbba', 'bbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 193: No counterexamples found, skipped.
Accuracy at epoch 193: 0.5078125, total training samples: 1347
Early stopping at epoch 650. Loss did not improve for 10 epochs.
Generate examples Step 650, Loss 1.144918562263571
Epoch: 194
Negative Examples
['a', 'acbba', 'accbba', 'ba', 'baacbba', 'bacbba', 'bbacbba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 194: No counterexamples found, skipped.
Accuracy at epoch 194: 0.509765625, total training samples: 1347
Early stopping at epoch 673. Loss did not improve for 10 epochs.
Generate examples Step 673, Loss 1.2141822370471163
Epoch: 195
Negative Examples
['acbba', 'ba', 'baacbba', 'bba', 'bbaccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 195: No counterexamples found, skipped.
Accuracy at epoch 195: 0.53515625, total training samples: 1347
Early stopping at epoch 669. Loss did not improve for 10 epochs.
Generate examples Step 669, Loss 1.208964150699217
Epoch: 196
Negative Examples
['a', 'acbba', 'accbba', 'ba', 'baacbba', 'bba', 'bbaacbba', 'bbacbba', 'bbbacbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 196: No counterexamples found, skipped.
Accuracy at epoch 196: 0.517578125, total training samples: 1347
Early stopping at epoch 663. Loss did not improve for 10 epochs.
Generate examples Step 663, Loss 1.44372006568564
Epoch: 197
Negative Examples
['a', 'accbba', 'ba', 'bacbba', 'bba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 197: No counterexamples found, skipped.
Accuracy at epoch 197: 0.509765625, total training samples: 1347
Early stopping at epoch 629. Loss did not improve for 10 epochs.
Generate examples Step 629, Loss 1.179944865381907
Epoch: 198
Negative Examples
['a', 'abacbba', 'accbba', 'ba', 'baacbba', 'bacbba', 'baccbba', 'bba', 'bbaccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 198: No counterexamples found, skipped.
Accuracy at epoch 198: 0.48828125, total training samples: 1347
Early stopping at epoch 648. Loss did not improve for 10 epochs.
Generate examples Step 648, Loss 1.1085389619248307
Epoch: 199
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'bacbba', 'baccbba', 'bba', 'bbaacbba', 'bbacbba', 'bbbacbba', 'bbccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 199: No counterexamples found, skipped.
Accuracy at epoch 199: 0.45703125, total training samples: 1347
Early stopping at epoch 633. Loss did not improve for 10 epochs.
Generate examples Step 633, Loss 0.7864159265245173
Epoch: 200
Negative Examples
['aaccbba', 'acbba', 'ba', 'bacbba', 'baccbba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 200: No counterexamples found, skipped.
Accuracy at epoch 200: 0.494140625, total training samples: 1347
Early stopping at epoch 649. Loss did not improve for 10 epochs.
Generate examples Step 649, Loss 1.075703728382404
Epoch: 201
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'ba', 'bacbba', 'bba', 'bbaacbba', 'bbbccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 201: No counterexamples found, skipped.
Accuracy at epoch 201: 0.51953125, total training samples: 1347
Early stopping at epoch 691. Loss did not improve for 10 epochs.
Generate examples Step 691, Loss 1.4386770857896418
Epoch: 202
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'bba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 202: No counterexamples found, skipped.
Accuracy at epoch 202: 0.474609375, total training samples: 1347
Early stopping at epoch 695. Loss did not improve for 10 epochs.
Generate examples Step 695, Loss 1.642376852446589
Epoch: 203
Negative Examples
['a', 'aacbba', 'ba', 'bba', 'bbacbba', 'bbaccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 203: No counterexamples found, skipped.
Accuracy at epoch 203: 0.51953125, total training samples: 1347
Early stopping at epoch 671. Loss did not improve for 10 epochs.
Generate examples Step 671, Loss 1.2980903482862882
Epoch: 204
Negative Examples
['a', 'acbba', 'bba', 'bbacbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 204: No counterexamples found, skipped.
Accuracy at epoch 204: 0.490234375, total training samples: 1347
Early stopping at epoch 678. Loss did not improve for 10 epochs.
Generate examples Step 678, Loss 1.2565287316495084
Epoch: 205
Negative Examples
['a', 'ba', 'baacbba', 'bacbba', 'baccbba', 'bba', 'bbaacbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 205: No counterexamples found, skipped.
Accuracy at epoch 205: 0.51953125, total training samples: 1347
Early stopping at epoch 661. Loss did not improve for 10 epochs.
Generate examples Step 661, Loss 1.4276240631892962
Epoch: 206
Negative Examples
['a', 'accbba', 'ba', 'bba', 'bbaacbba', 'bbacbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 206: No counterexamples found, skipped.
Accuracy at epoch 206: 0.490234375, total training samples: 1347
Early stopping at epoch 674. Loss did not improve for 10 epochs.
Generate examples Step 674, Loss 1.2114511680603028
Epoch: 207
Negative Examples
['a', 'accbba', 'ba', 'baacbba', 'baccbba', 'bba', 'bbaccbba', 'bbbccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 207: No counterexamples found, skipped.
Accuracy at epoch 207: 0.505859375, total training samples: 1347
Early stopping at epoch 678. Loss did not improve for 10 epochs.
Generate examples Step 678, Loss 1.1403600112679078
Epoch: 208
Negative Examples
['acbba', 'bacbba', 'bba', 'bbacbba', 'bbaccbba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 208: No counterexamples found, skipped.
Accuracy at epoch 208: 0.54296875, total training samples: 1347
Early stopping at epoch 655. Loss did not improve for 10 epochs.
Generate examples Step 655, Loss 1.0783063409531988
Epoch: 209
Negative Examples
['a', 'acbba', 'ba', 'bacbba', 'baccbba', 'bba', 'bbaccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 209: No counterexamples found, skipped.
Accuracy at epoch 209: 0.478515625, total training samples: 1347
Early stopping at epoch 590. Loss did not improve for 10 epochs.
Generate examples Step 590, Loss 1.05935799218273
Epoch: 210
Negative Examples
['a', 'abaccbba', 'acbba', 'ba', 'bba', 'bbaacbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 210: No counterexamples found, skipped.
Accuracy at epoch 210: 0.484375, total training samples: 1347
Early stopping at epoch 636. Loss did not improve for 10 epochs.
Generate examples Step 636, Loss 1.1279876277061234
Epoch: 211
Negative Examples
['a', 'aaacbba', 'acbba', 'accbba', 'ba', 'baacbba', 'bba', 'bbaacbba', 'bbbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 211: No counterexamples found, skipped.
Accuracy at epoch 211: 0.509765625, total training samples: 1347
Early stopping at epoch 582. Loss did not improve for 10 epochs.
Generate examples Step 582, Loss 0.8155440133493106
Epoch: 212
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'baaacbba', 'bacbba', 'baccbba', 'bba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 212: No counterexamples found, skipped.
Accuracy at epoch 212: 0.509765625, total training samples: 1347
Early stopping at epoch 685. Loss did not improve for 10 epochs.
Generate examples Step 685, Loss 1.0187343938531404
Epoch: 213
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'ba', 'baacbba', 'baccbba', 'bbacbba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 213: No counterexamples found, skipped.
Accuracy at epoch 213: 0.47265625, total training samples: 1347
Early stopping at epoch 695. Loss did not improve for 10 epochs.
Generate examples Step 695, Loss 1.1664281668162895
Epoch: 214
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 214: No counterexamples found, skipped.
Accuracy at epoch 214: 0.49609375, total training samples: 1347
Early stopping at epoch 734. Loss did not improve for 10 epochs.
Generate examples Step 734, Loss 1.33009489604405
Epoch: 215
Negative Examples
['accbba', 'ba', 'bacbba', 'bba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 215: No counterexamples found, skipped.
Accuracy at epoch 215: 0.517578125, total training samples: 1347
Early stopping at epoch 711. Loss did not improve for 10 epochs.
Generate examples Step 711, Loss 1.4353603134664257
Epoch: 216
Negative Examples
['a', 'ba', 'bba', 'bbaacbba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 216: No counterexamples found, skipped.
Accuracy at epoch 216: 0.517578125, total training samples: 1347
Early stopping at epoch 632. Loss did not improve for 10 epochs.
Generate examples Step 632, Loss 1.1094736946514243
Epoch: 217
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'ba', 'baacbba', 'bacbba', 'bba', 'bbacbba', 'bccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 217: No counterexamples found, skipped.
Accuracy at epoch 217: 0.4921875, total training samples: 1347
Early stopping at epoch 670. Loss did not improve for 10 epochs.
Generate examples Step 670, Loss 1.1255942597296837
Epoch: 218
Negative Examples
['aacbba', 'accbba', 'ba', 'baacbba', 'bacbba', 'bba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 218: No counterexamples found, skipped.
Accuracy at epoch 218: 0.490234375, total training samples: 1347
Early stopping at epoch 644. Loss did not improve for 10 epochs.
Generate examples Step 644, Loss 1.106029859993809
Epoch: 219
Negative Examples
['a', 'ba', 'baacbba', 'bacbba', 'baccbba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 219: No counterexamples found, skipped.
Accuracy at epoch 219: 0.490234375, total training samples: 1347
Early stopping at epoch 673. Loss did not improve for 10 epochs.
Generate examples Step 673, Loss 1.190860525879025
Epoch: 220
Negative Examples
['a', 'acbba', 'baacbba', 'bacbba', 'bba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 220: No counterexamples found, skipped.
Accuracy at epoch 220: 0.509765625, total training samples: 1347
Early stopping at epoch 652. Loss did not improve for 10 epochs.
Generate examples Step 652, Loss 1.4156578560152975
Epoch: 221
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'bba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 221: No counterexamples found, skipped.
Accuracy at epoch 221: 0.498046875, total training samples: 1347
Early stopping at epoch 690. Loss did not improve for 10 epochs.
Generate examples Step 690, Loss 1.2708865700858374
Epoch: 222
Negative Examples
['a', 'accbba', 'ba', 'bacbba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 222: No counterexamples found, skipped.
Accuracy at epoch 222: 0.529296875, total training samples: 1347
Early stopping at epoch 722. Loss did not improve for 10 epochs.
Generate examples Step 722, Loss 1.2425911219617967
Epoch: 223
Negative Examples
['a', 'ba', 'baccbba', 'bba', 'bbacbba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 223: No counterexamples found, skipped.
Accuracy at epoch 223: 0.515625, total training samples: 1347
Early stopping at epoch 680. Loss did not improve for 10 epochs.
Generate examples Step 680, Loss 1.6059331204222513
Epoch: 224
Negative Examples
['a', 'aacbba', 'baacbba', 'bacbba', 'baccbba', 'bba', 'bbaccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 224: No counterexamples found, skipped.
Accuracy at epoch 224: 0.46875, total training samples: 1347
Early stopping at epoch 629. Loss did not improve for 10 epochs.
Generate examples Step 629, Loss 1.061258550106533
Epoch: 225
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'baccbba', 'bba', 'bbaacbba', 'bbaccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 225: No counterexamples found, skipped.
Accuracy at epoch 225: 0.4765625, total training samples: 1347
Early stopping at epoch 700. Loss did not improve for 10 epochs.
Generate examples Step 700, Loss 1.7138956764795301
Epoch: 226
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'bacbba', 'baccbba', 'bba', 'bbacbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 226: No counterexamples found, skipped.
Accuracy at epoch 226: 0.494140625, total training samples: 1347
Early stopping at epoch 644. Loss did not improve for 10 epochs.
Generate examples Step 644, Loss 1.2083150144695312
Epoch: 227
Negative Examples
['a', 'acbba', 'bba', 'bbbacbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 227: No counterexamples found, skipped.
Accuracy at epoch 227: 0.51171875, total training samples: 1347
Early stopping at epoch 682. Loss did not improve for 10 epochs.
Generate examples Step 682, Loss 1.296277732835158
Epoch: 228
Negative Examples
['a', 'acbba', 'ba', 'baccbba', 'bba', 'bbbacbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 228: No counterexamples found, skipped.
Accuracy at epoch 228: 0.517578125, total training samples: 1347
Early stopping at epoch 669. Loss did not improve for 10 epochs.
Generate examples Step 669, Loss 0.9928903243434963
Epoch: 229
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'ba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 229: No counterexamples found, skipped.
Accuracy at epoch 229: 0.451171875, total training samples: 1347
Early stopping at epoch 672. Loss did not improve for 10 epochs.
Generate examples Step 672, Loss 1.2477660838951858
Epoch: 230
Negative Examples
['a', 'ba', 'baacbba', 'baccbba', 'bba', 'bbaacbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 230: No counterexamples found, skipped.
Accuracy at epoch 230: 0.48828125, total training samples: 1347
Early stopping at epoch 625. Loss did not improve for 10 epochs.
Generate examples Step 625, Loss 1.0082047025616558
Epoch: 231
Negative Examples
['a', 'aaacbba', 'abacbba', 'acbba', 'ba', 'bba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 231: No counterexamples found, skipped.
Accuracy at epoch 231: 0.5, total training samples: 1347
Early stopping at epoch 643. Loss did not improve for 10 epochs.
Generate examples Step 643, Loss 1.1445629196877805
Epoch: 232
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'bacbba', 'bba', 'bbaccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 232: No counterexamples found, skipped.
Accuracy at epoch 232: 0.53515625, total training samples: 1347
Early stopping at epoch 677. Loss did not improve for 10 epochs.
Generate examples Step 677, Loss 1.0408173388963604
Epoch: 233
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'ba', 'bba', 'bbacbba', 'bbaccbba', 'bbbacbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 233: No counterexamples found, skipped.
Accuracy at epoch 233: 0.4765625, total training samples: 1347
Early stopping at epoch 680. Loss did not improve for 10 epochs.
Generate examples Step 680, Loss 1.2394684121647246
Epoch: 234
Negative Examples
['a', 'aacbba', 'acbba', 'baacbba', 'bacbba', 'bba', 'bbbacbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 234: No counterexamples found, skipped.
Accuracy at epoch 234: 0.533203125, total training samples: 1347
Early stopping at epoch 691. Loss did not improve for 10 epochs.
Generate examples Step 691, Loss 1.1748051748454915
Epoch: 235
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'baacbba', 'baccbba', 'bba', 'bbacbba', 'bbccbba', 'bccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 235: No counterexamples found, skipped.
Accuracy at epoch 235: 0.49609375, total training samples: 1347
Early stopping at epoch 663. Loss did not improve for 10 epochs.
Generate examples Step 663, Loss 1.0338992442352226
Epoch: 236
Negative Examples
['aacbba', 'ba', 'bacbba', 'baccbba', 'bbaacbba', 'bbaccbba', 'bbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 236: No counterexamples found, skipped.
Accuracy at epoch 236: 0.533203125, total training samples: 1347
Early stopping at epoch 578. Loss did not improve for 10 epochs.
Generate examples Step 578, Loss 1.130631984309623
Epoch: 237
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'bba', 'cbba']
Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 237: No counterexamples found, skipped.
Accuracy at epoch 237: 0.517578125, total training samples: 1347
Early stopping at epoch 610. Loss did not improve for 10 epochs.
Generate examples Step 610, Loss 0.9483904741009401
Epoch: 238
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'baccbba', 'bba', 'bbaacbba', 'bbbacbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 238: No counterexamples found, skipped.
Accuracy at epoch 238: 0.474609375, total training samples: 1347
Early stopping at epoch 645. Loss did not improve for 10 epochs.
Generate examples Step 645, Loss 1.189260962485529
Epoch: 239
Negative Examples
['a', 'aacbba', 'ba', 'baacbba', 'bacbba', 'bba', 'bbaccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 239: No counterexamples found, skipped.
Accuracy at epoch 239: 0.505859375, total training samples: 1347
Early stopping at epoch 676. Loss did not improve for 10 epochs.
Generate examples Step 676, Loss 1.2196103092310524
Epoch: 240
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'baccbba', 'bba', 'bbaacbba', 'bbacbba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 240: No counterexamples found, skipped.
Accuracy at epoch 240: 0.466796875, total training samples: 1347
Early stopping at epoch 641. Loss did not improve for 10 epochs.
Generate examples Step 641, Loss 1.2959590790984787
Epoch: 241
Negative Examples
['a', 'acbba', 'ba', 'bba', 'bbacbba', 'bbaccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 241: No counterexamples found, skipped.
Accuracy at epoch 241: 0.494140625, total training samples: 1347
Early stopping at epoch 652. Loss did not improve for 10 epochs.
Generate examples Step 652, Loss 1.1377183298422038
Epoch: 242
Negative Examples
['a', 'acbba', 'accbba', 'ba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 242: No counterexamples found, skipped.
Accuracy at epoch 242: 0.48828125, total training samples: 1347
Early stopping at epoch 672. Loss did not improve for 10 epochs.
Generate examples Step 672, Loss 1.4103452622624775
Epoch: 243
Negative Examples
['a', 'ba', 'bba', 'bbaccbba', 'bbbccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 243: No counterexamples found, skipped.
Accuracy at epoch 243: 0.474609375, total training samples: 1347
Early stopping at epoch 678. Loss did not improve for 10 epochs.
Generate examples Step 678, Loss 1.2374333366203027
Epoch: 244
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'ba', 'bba', 'bbaacbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 244: No counterexamples found, skipped.
Accuracy at epoch 244: 0.486328125, total training samples: 1347
Early stopping at epoch 669. Loss did not improve for 10 epochs.
Generate examples Step 669, Loss 1.178230114214456
Epoch: 245
Negative Examples
['a', 'accbba', 'ba', 'bacbba', 'baccbba', 'bba', 'bbaccbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 245: No counterexamples found, skipped.
Accuracy at epoch 245: 0.505859375, total training samples: 1347
Early stopping at epoch 599. Loss did not improve for 10 epochs.
Generate examples Step 599, Loss 1.1814482732613882
Epoch: 246
Negative Examples
['a', 'acbba', 'accbba', 'ba', 'baccbba', 'bba', 'bbbacbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 246: No counterexamples found, skipped.
Accuracy at epoch 246: 0.49609375, total training samples: 1347
Early stopping at epoch 674. Loss did not improve for 10 epochs.
Generate examples Step 674, Loss 1.1002642961784646
Epoch: 247
Negative Examples
['a', 'acbba', 'bacbba', 'bba', 'bbaacbba', 'bbaccbba', 'bbbccbba', 'bbccbba', 'bccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 247: No counterexamples found, skipped.
Accuracy at epoch 247: 0.5, total training samples: 1347
Early stopping at epoch 677. Loss did not improve for 10 epochs.
Generate examples Step 677, Loss 1.2305141100137986
Epoch: 248
Negative Examples
['a', 'accbba', 'baacbba', 'bacbba', 'bba', 'bbacbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 248: No counterexamples found, skipped.
Accuracy at epoch 248: 0.494140625, total training samples: 1347
Early stopping at epoch 654. Loss did not improve for 10 epochs.
Generate examples Step 654, Loss 1.2495720230896055
Epoch: 249
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'bacbba', 'bba', 'bbbacbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 249: No counterexamples found, skipped.
Accuracy at epoch 249: 0.458984375, total training samples: 1347
Early stopping at epoch 664. Loss did not improve for 10 epochs.
Generate examples Step 664, Loss 1.181104852830557
Epoch: 250
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'bba', 'bbacbba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 250: No counterexamples found, skipped.
Accuracy at epoch 250: 0.494140625, total training samples: 1347
Early stopping at epoch 675. Loss did not improve for 10 epochs.
Generate examples Step 675, Loss 1.2945623128900867
Epoch: 251
Negative Examples
['a', 'accbba', 'baccbba', 'bba', 'bbaacbba', 'bbbccbba', 'bccbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 251: No counterexamples found, skipped.
Accuracy at epoch 251: 0.515625, total training samples: 1347
Early stopping at epoch 710. Loss did not improve for 10 epochs.
Generate examples Step 710, Loss 1.10107552552525
Epoch: 252
Negative Examples
['a', 'ba', 'baacbba', 'bacbba', 'bba', 'bbaacbba', 'bbbacbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 252: No counterexamples found, skipped.
Accuracy at epoch 252: 0.537109375, total training samples: 1347
Early stopping at epoch 626. Loss did not improve for 10 epochs.
Generate examples Step 626, Loss 1.007821961643213
Epoch: 253
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'bacbba', 'bbbacbba', 'bbbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 253: No counterexamples found, skipped.
Accuracy at epoch 253: 0.521484375, total training samples: 1347
Early stopping at epoch 675. Loss did not improve for 10 epochs.
Generate examples Step 675, Loss 1.1939017643942635
Epoch: 254
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'baacbba', 'bba', 'bbacbba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 254: No counterexamples found, skipped.
Accuracy at epoch 254: 0.5234375, total training samples: 1347
Early stopping at epoch 676. Loss did not improve for 10 epochs.
Generate examples Step 676, Loss 1.3250310877929585
Epoch: 255
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'baacbba', 'bacbba', 'baccbba', 'bbaccbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 255: No counterexamples found, skipped.
Accuracy at epoch 255: 0.525390625, total training samples: 1347
Early stopping at epoch 673. Loss did not improve for 10 epochs.
Generate examples Step 673, Loss 1.5962293466403858
Epoch: 256
Negative Examples
['a', 'aacbba', 'ba', 'baacbba', 'bba', 'bbbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 256: No counterexamples found, skipped.
Accuracy at epoch 256: 0.50390625, total training samples: 1347
Early stopping at epoch 675. Loss did not improve for 10 epochs.
Generate examples Step 675, Loss 1.1440840007285396
Epoch: 257
Negative Examples
['a', 'acbba', 'accbba', 'ba', 'baccbba', 'bba', 'bbaacbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 257: No counterexamples found, skipped.
Accuracy at epoch 257: 0.490234375, total training samples: 1347
Early stopping at epoch 678. Loss did not improve for 10 epochs.
Generate examples Step 678, Loss 1.3085074075367384
Epoch: 258
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'baccbba', 'bba', 'bbacbba', 'bbaccbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 258: No counterexamples found, skipped.
Accuracy at epoch 258: 0.47265625, total training samples: 1347
Early stopping at epoch 684. Loss did not improve for 10 epochs.
Generate examples Step 684, Loss 1.1303128536600267
Epoch: 259
Negative Examples
['a', 'ba', 'baacbba', 'bacbba', 'baccbba', 'bbacbba', 'bbbacbba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 259: No counterexamples found, skipped.
Accuracy at epoch 259: 0.494140625, total training samples: 1347
Early stopping at epoch 708. Loss did not improve for 10 epochs.
Generate examples Step 708, Loss 1.4264723287819134
Epoch: 260
Negative Examples
['a', 'aacbba', 'ba', 'baacbba', 'bba', 'bbbacbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 260: No counterexamples found, skipped.
Accuracy at epoch 260: 0.521484375, total training samples: 1347
Early stopping at epoch 628. Loss did not improve for 10 epochs.
Generate examples Step 628, Loss 1.177704443139378
Epoch: 261
Negative Examples
['a', 'aacbba', 'acbba', 'bacbba', 'bba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 261: No counterexamples found, skipped.
Accuracy at epoch 261: 0.517578125, total training samples: 1347
Early stopping at epoch 729. Loss did not improve for 10 epochs.
Generate examples Step 729, Loss 1.5794556011892344
Epoch: 262
Negative Examples
['accbba', 'ba', 'baacbba', 'bba', 'bbaacbba', 'bbbacbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 262: No counterexamples found, skipped.
Accuracy at epoch 262: 0.478515625, total training samples: 1347
Early stopping at epoch 672. Loss did not improve for 10 epochs.
Generate examples Step 672, Loss 1.1542815163089475
Epoch: 263
Negative Examples
['aacbba', 'acbba', 'accbba', 'ba', 'baacbba', 'bacbba', 'bba', 'bbaacbba', 'bbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 263: No counterexamples found, skipped.
Accuracy at epoch 263: 0.51171875, total training samples: 1347
Early stopping at epoch 633. Loss did not improve for 10 epochs.
Generate examples Step 633, Loss 1.0192631621278048
Epoch: 264
Negative Examples
['a', 'ba', 'bacbba', 'baccbba', 'bba', 'bbaacbba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 264: No counterexamples found, skipped.
Accuracy at epoch 264: 0.529296875, total training samples: 1347
Early stopping at epoch 654. Loss did not improve for 10 epochs.
Generate examples Step 654, Loss 1.078414777126021
Epoch: 265
Negative Examples
['a', 'acbba', 'ba', 'bacbba', 'baccbba', 'bba', 'bbaacbba', 'bbacbba', 'bbbacbba', 'bbbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 265: No counterexamples found, skipped.
Accuracy at epoch 265: 0.482421875, total training samples: 1347
Early stopping at epoch 625. Loss did not improve for 10 epochs.
Generate examples Step 625, Loss 1.053105485801118
Epoch: 266
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'bacbba', 'baccbba', 'bbaccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 266: No counterexamples found, skipped.
Accuracy at epoch 266: 0.505859375, total training samples: 1347
Early stopping at epoch 693. Loss did not improve for 10 epochs.
Generate examples Step 693, Loss 1.4053829076997828
Epoch: 267
Negative Examples
['a', 'ba', 'baacbba', 'bacbba', 'bba', 'bbbccbba', 'bbccbba', 'bccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 267: No counterexamples found, skipped.
Accuracy at epoch 267: 0.529296875, total training samples: 1347
Early stopping at epoch 693. Loss did not improve for 10 epochs.
Generate examples Step 693, Loss 1.2328778775865132
Epoch: 268
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'bacbba', 'bba', 'bbaacbba', 'bbacbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 268: No counterexamples found, skipped.
Accuracy at epoch 268: 0.458984375, total training samples: 1347
Early stopping at epoch 667. Loss did not improve for 10 epochs.
Generate examples Step 667, Loss 1.4534086217423399
Epoch: 269
Negative Examples
['a', 'accbba', 'ba', 'bba', 'bbbacbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 269: No counterexamples found, skipped.
Accuracy at epoch 269: 0.521484375, total training samples: 1347
Early stopping at epoch 723. Loss did not improve for 10 epochs.
Generate examples Step 723, Loss 1.3518122487305277
Epoch: 270
Negative Examples
['aacbba', 'ba', 'baacbba', 'bacbba', 'bba', 'bbbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 270: No counterexamples found, skipped.
Accuracy at epoch 270: 0.48828125, total training samples: 1347
Early stopping at epoch 707. Loss did not improve for 10 epochs.
Generate examples Step 707, Loss 1.790117380140865
Epoch: 271
Negative Examples
['a', 'ba', 'bacbba', 'baccbba', 'bba', 'bbaccbba', 'bbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 271: No counterexamples found, skipped.
Accuracy at epoch 271: 0.5078125, total training samples: 1347
Early stopping at epoch 632. Loss did not improve for 10 epochs.
Generate examples Step 632, Loss 1.181180045126362
Epoch: 272
Negative Examples
['a', 'acbba', 'ba', 'bacbba', 'bba', 'bbacbba', 'bbbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 272: No counterexamples found, skipped.
Accuracy at epoch 272: 0.494140625, total training samples: 1347
Early stopping at epoch 636. Loss did not improve for 10 epochs.
Generate examples Step 636, Loss 1.0104073579869053
Epoch: 273
Negative Examples
['a', 'abaccbba', 'ba', 'bacbba', 'baccbba', 'bbbccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 273: No counterexamples found, skipped.
Accuracy at epoch 273: 0.5, total training samples: 1347
Early stopping at epoch 656. Loss did not improve for 10 epochs.
Generate examples Step 656, Loss 1.2919228496979724
Epoch: 274
Negative Examples
['a', 'acbba', 'ba', 'baccbba', 'bba', 'bbaacbba', 'bbbacbba', 'bbbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 274: No counterexamples found, skipped.
Accuracy at epoch 274: 0.484375, total training samples: 1347
Early stopping at epoch 640. Loss did not improve for 10 epochs.
Generate examples Step 640, Loss 1.1385555181785976
Epoch: 275
Negative Examples
['a', 'acbba', 'ba', 'bba', 'bbaacbba', 'bbbccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 275: No counterexamples found, skipped.
Accuracy at epoch 275: 0.498046875, total training samples: 1347
Early stopping at epoch 652. Loss did not improve for 10 epochs.
Generate examples Step 652, Loss 1.0484985815985828
Epoch: 276
Negative Examples
['a', 'abacbba', 'acbba', 'ba', 'baccbba', 'bba', 'bbbacbba', 'bbbccbba', 'bccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 276: No counterexamples found, skipped.
Accuracy at epoch 276: 0.484375, total training samples: 1347
Early stopping at epoch 693. Loss did not improve for 10 epochs.
Generate examples Step 693, Loss 1.3645683125906787
Epoch: 277
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'bacbba', 'bba', 'bbaacbba', 'bbbacbba', 'bbbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 277: No counterexamples found, skipped.
Accuracy at epoch 277: 0.482421875, total training samples: 1347
Early stopping at epoch 698. Loss did not improve for 10 epochs.
Generate examples Step 698, Loss 1.215226790253526
Epoch: 278
Negative Examples
['acbba', 'accbba', 'ba', 'baacbba', 'bacbba', 'baccbba', 'bba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 278: No counterexamples found, skipped.
Accuracy at epoch 278: 0.513671875, total training samples: 1347
Early stopping at epoch 670. Loss did not improve for 10 epochs.
Generate examples Step 670, Loss 1.3127370005925971
Epoch: 279
Negative Examples
['a', 'ba', 'bacbba', 'bba', 'bbaacbba', 'bbaccbba', 'bbbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 279: No counterexamples found, skipped.
Accuracy at epoch 279: 0.490234375, total training samples: 1347
Early stopping at epoch 615. Loss did not improve for 10 epochs.
Generate examples Step 615, Loss 1.0386635008957479
Epoch: 280
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'baccbba', 'bba', 'bbaacbba', 'bbbccbba', 'bccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 280: No counterexamples found, skipped.
Accuracy at epoch 280: 0.4921875, total training samples: 1347
Early stopping at epoch 684. Loss did not improve for 10 epochs.
Generate examples Step 684, Loss 1.202566668151939
Epoch: 281
Negative Examples
['a', 'aacbba', 'acbba', 'accbba', 'ba', 'bacbba', 'bba', 'bbaccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 281: No counterexamples found, skipped.
Accuracy at epoch 281: 0.458984375, total training samples: 1347
Early stopping at epoch 710. Loss did not improve for 10 epochs.
Generate examples Step 710, Loss 1.43538544483158
Epoch: 282
Negative Examples
['a', 'aacbba', 'ba', 'baacbba', 'bba', 'bbaccbba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 282: No counterexamples found, skipped.
Accuracy at epoch 282: 0.544921875, total training samples: 1347
Early stopping at epoch 623. Loss did not improve for 10 epochs.
Generate examples Step 623, Loss 0.9365712177868073
Epoch: 283
Negative Examples
['a', 'abaacbba', 'ba', 'baaacbba', 'babacbba', 'bba', 'bbaccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 283: No counterexamples found, skipped.
Accuracy at epoch 283: 0.49609375, total training samples: 1347
Early stopping at epoch 694. Loss did not improve for 10 epochs.
Generate examples Step 694, Loss 1.108885214911948
Epoch: 284
Negative Examples
['a', 'acbba', 'accbba', 'baccbba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'bbbacbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 284: No counterexamples found, skipped.
Accuracy at epoch 284: 0.517578125, total training samples: 1347
Early stopping at epoch 686. Loss did not improve for 10 epochs.
Generate examples Step 686, Loss 1.3420637665530653
Epoch: 285
Negative Examples
['a', 'acbba', 'ba', 'bacbba', 'bba', 'bbaacbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 285: No counterexamples found, skipped.
Accuracy at epoch 285: 0.501953125, total training samples: 1347
Early stopping at epoch 716. Loss did not improve for 10 epochs.
Generate examples Step 716, Loss 1.5201759032458275
Epoch: 286
Negative Examples
['a', 'aacbba', 'accbba', 'ba', 'bba', 'bbaacbba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 286: No counterexamples found, skipped.
Accuracy at epoch 286: 0.517578125, total training samples: 1347
Early stopping at epoch 681. Loss did not improve for 10 epochs.
Generate examples Step 681, Loss 1.13695578832081
Epoch: 287
Negative Examples
['a', 'aacbba', 'ba', 'baccbba', 'bbacbba', 'bbaccbba', 'bbbccbba', 'bbccbba', 'bccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 287: No counterexamples found, skipped.
Accuracy at epoch 287: 0.45703125, total training samples: 1347
Early stopping at epoch 689. Loss did not improve for 10 epochs.
Generate examples Step 689, Loss 1.252399450281392
Epoch: 288
Negative Examples
['a', 'aacbba', 'acbba', 'baacbba', 'bba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 288: No counterexamples found, skipped.
Accuracy at epoch 288: 0.478515625, total training samples: 1347
Early stopping at epoch 679. Loss did not improve for 10 epochs.
Generate examples Step 679, Loss 1.2543377873652122
Epoch: 289
Negative Examples
['a', 'aacbba', 'baacbba', 'baccbba', 'bba', 'bbaacbba', 'bbacbba', 'bbaccbba', 'bbbacbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 289: No counterexamples found, skipped.
Accuracy at epoch 289: 0.517578125, total training samples: 1347
Early stopping at epoch 679. Loss did not improve for 10 epochs.
Generate examples Step 679, Loss 1.294662394330782
Epoch: 290
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'baacbba', 'bba', 'bbacbba', 'bbbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 290: No counterexamples found, skipped.
Accuracy at epoch 290: 0.521484375, total training samples: 1347
Early stopping at epoch 680. Loss did not improve for 10 epochs.
Generate examples Step 680, Loss 1.4199202360035565
Epoch: 291
Negative Examples
['a', 'aacbba', 'acbba', 'ba', 'bba', 'bbaacbba', 'bbbacbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 291: No counterexamples found, skipped.
Accuracy at epoch 291: 0.458984375, total training samples: 1347
Early stopping at epoch 645. Loss did not improve for 10 epochs.
Generate examples Step 645, Loss 1.0905157112669281
Epoch: 292
Negative Examples
['a', 'aacbba', 'acbba', 'baacbba', 'baccbba', 'bba', 'bbaacbba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 292: No counterexamples found, skipped.
Accuracy at epoch 292: 0.478515625, total training samples: 1347
Early stopping at epoch 681. Loss did not improve for 10 epochs.
Generate examples Step 681, Loss 1.461502993910893
Epoch: 293
Negative Examples
['a', 'aacbba', 'ba', 'bacbba', 'baccbba', 'bba', 'bbaacbba', 'bbbccbba', 'bbccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 293: No counterexamples found, skipped.
Accuracy at epoch 293: 0.474609375, total training samples: 1347
Early stopping at epoch 628. Loss did not improve for 10 epochs.
Generate examples Step 628, Loss 1.0651009091322676
Epoch: 294
Negative Examples
['a', 'acbba', 'ba', 'baacbba', 'bacbba', 'bba', 'bbaacbba', 'bbaccbba', 'bbccbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 294: No counterexamples found, skipped.
Accuracy at epoch 294: 0.474609375, total training samples: 1347
Early stopping at epoch 641. Loss did not improve for 10 epochs.
Generate examples Step 641, Loss 1.1677833141010499
Epoch: 295
Negative Examples
['a', 'acbba', 'accbba', 'ba', 'bba', 'bbacbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 295: No counterexamples found, skipped.
Accuracy at epoch 295: 0.51171875, total training samples: 1347
Early stopping at epoch 723. Loss did not improve for 10 epochs.
Generate examples Step 723, Loss 1.315659585331685
Epoch: 296
Negative Examples
['acbba', 'ba', 'bacbba', 'bba', 'bbacbba', 'bbaccbba', 'bbbacbba', 'bbbccbba', 'bbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 296: No counterexamples found, skipped.
Accuracy at epoch 296: 0.494140625, total training samples: 1347
Early stopping at epoch 701. Loss did not improve for 10 epochs.
Generate examples Step 701, Loss 1.2825457873167816
Epoch: 297
Negative Examples
['acbba', 'accbba', 'ba', 'baccbba', 'bba', 'bbacbba', 'bbbccbba', 'bbccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 297: No counterexamples found, skipped.
Accuracy at epoch 297: 0.515625, total training samples: 1347
Early stopping at epoch 620. Loss did not improve for 10 epochs.
Generate examples Step 620, Loss 1.0999775248641168
Epoch: 298
Negative Examples
['a', 'ba', 'bba', 'bbacbba', 'bbbacbba', 'bccbba', 'cbba', 'ccbba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 298: No counterexamples found, skipped.
Accuracy at epoch 298: 0.49609375, total training samples: 1347
Early stopping at epoch 674. Loss did not improve for 10 epochs.
Generate examples Step 674, Loss 1.197413701304683
Epoch: 299
Negative Examples
['a', 'aacbba', 'ba', 'baacbba', 'bba', 'bbaacbba', 'bbbacbba', 'bbccbba', 'bccbba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 299: No counterexamples found, skipped.
Accuracy at epoch 299: 0.498046875, total training samples: 1347
Pos train / Tot train = 309 / 1347
