Train Epoch 0, Loss 0.7013667300343513
Train Epoch 1, Loss 0.6479287147521973
Train Epoch 2, Loss 0.6059842109680176
Pretrained on 128 supervised examples, Eval Acc: 0.734375
Early stopping at epoch 261. Loss did not improve for 10 epochs.
Generate examples Step 261, Loss 0.2739494807506336
Epoch: 0
Negative Examples
['abaabaca', 'aca', 'acabaca', 'b', 'bbcbb', 'bcca', 'cbabbcbb', 'cbcbbccc', 'cc', 'ccbacc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['a', 'bbaabcac', 'bbcabcba', 'bcaa']
Pos Pos Pos Pos
Counterexamples
['a', 'bbaabcac', 'bbcabcba', 'bcaa']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7614204287528992
Train Epoch 1, Loss 0.6293704509735107
Train Epoch 2, Loss 0.5178309679031372
Accuracy at epoch 0: 0.60546875, total training samples: 4
Early stopping at epoch 298. Loss did not improve for 10 epochs.
Generate examples Step 298, Loss 0.2945937886584961
Epoch: 1
Negative Examples
['baba', 'babc', 'cabbcba', 'cbbaba', 'cbbabc', 'ccbbaba']
Neg Neg Neg Neg Neg Neg
Positive Examples
['aabc', 'abc', 'abccba', 'ba', 'babaaba', 'bcbaaba', 'cacbbabc', 'ccbaaac']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baba', 'aabc', 'abc', 'ba', 'babaaba', 'bcbaaba', 'cacbbabc', 'ccbaaac']
Pos Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.8158470988273621
Train Epoch 1, Loss 0.7730221748352051
Train Epoch 2, Loss 0.7377082109451294
Accuracy at epoch 1: 0.572265625, total training samples: 12
Early stopping at epoch 235. Loss did not improve for 10 epochs.
Generate examples Step 235, Loss 0.2864021158824533
Epoch: 2
Negative Examples
['a', 'aaabcba', 'aaccbcba', 'abacba', 'baccba', 'bbcba', 'bcbc', 'bcccbabc', 'c']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aba', 'abc', 'baba', 'bc', 'cbbaaba']
Pos Pos Pos Pos Pos
Counterexamples
['abacba', 'baccba', 'aba', 'abc', 'bc', 'cbbaaba']
Pos Pos Neg Neg Neg Neg
Train Epoch 0, Loss 0.7413462996482849
Train Epoch 1, Loss 0.7106154561042786
Train Epoch 2, Loss 0.6843926906585693
Accuracy at epoch 2: 0.640625, total training samples: 18
Early stopping at epoch 289. Loss did not improve for 10 epochs.
Generate examples Step 289, Loss 0.2934281866098272
Epoch: 3
Negative Examples
['a', 'abc', 'babc', 'bacbcba', 'bbcbaba', 'bc', 'c', 'cbabcbc', 'cccbabc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aba', 'ba', 'baba', 'bbbbaba', 'bcbc']
Pos Pos Pos Pos Pos
Counterexamples
['aba', 'ba', 'bbbbaba', 'bcbc']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7061960697174072
Train Epoch 1, Loss 0.6033293604850769
Train Epoch 2, Loss 0.5115554928779602
Accuracy at epoch 3: 0.5234375, total training samples: 22
Early stopping at epoch 267. Loss did not improve for 10 epochs.
Generate examples Step 267, Loss 0.33802163934529716
Epoch: 4
Negative Examples
['aaababa', 'aaabc', 'aabc', 'ababcbc', 'abbbcaba', 'accacba', 'accbbabc', 'bba', 'bc', 'bcbba', 'bccba', 'c', 'cababc', 'cabacba', 'cba', 'ccba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abbbcaba']
Pos
Train Epoch 0, Loss 0.8994837403297424
Train Epoch 1, Loss 0.746742308139801
Train Epoch 2, Loss 0.6195084452629089
Accuracy at epoch 4: 0.56640625, total training samples: 23
Early stopping at epoch 272. Loss did not improve for 10 epochs.
Generate examples Step 272, Loss 0.2839327469850198
Epoch: 5
Negative Examples
['a', 'abcbc', 'ba', 'baba', 'babc', 'bbbbabc', 'bc', 'bcba', 'c', 'caacba', 'cabbabc', 'cabbcba', 'cbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acbc', 'baaba', 'cbcbaabc']
Pos Pos Pos
Counterexamples
['baba', 'acbc', 'cbcbaabc']
Pos Neg Neg
Train Epoch 0, Loss 0.7472473978996277
Train Epoch 1, Loss 0.7100480198860168
Train Epoch 2, Loss 0.6763246655464172
Accuracy at epoch 5: 0.59765625, total training samples: 26
Early stopping at epoch 207. Loss did not improve for 10 epochs.
Generate examples Step 207, Loss 0.2614930926893766
Epoch: 6
Negative Examples
['a', 'aacc', 'abbcbc', 'abcba', 'abcbc', 'acbabc', 'ba', 'baca', 'bcacba', 'bcbc', 'c', 'cbacbc', 'cbbcbcbc', 'ccbbbaca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bcba']
Pos
Counterexamples
['abcba', 'bcba']
Pos Neg
Train Epoch 0, Loss 0.7224112749099731
Train Epoch 1, Loss 0.7016374468803406
Train Epoch 2, Loss 0.6843522191047668
Accuracy at epoch 6: 0.607421875, total training samples: 28
Early stopping at epoch 204. Loss did not improve for 10 epochs.
Generate examples Step 204, Loss 0.24949265362285986
Epoch: 7
Negative Examples
['aaabcbb', 'aabb', 'ababcbb', 'b', 'ba', 'bacc', 'bb', 'bcbb', 'bcbc', 'bccc', 'ca', 'cbc', 'cca', 'ccaaacba', 'ccabcca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 7: No counterexamples found, skipped.
Accuracy at epoch 7: 0.654296875, total training samples: 28
Early stopping at epoch 211. Loss did not improve for 10 epochs.
Generate examples Step 211, Loss 0.26888139046869186
Epoch: 8
Negative Examples
['aaca', 'abacbb', 'bbabaaba', 'bbcbaca', 'bcbcccbb', 'bcca', 'bccbb', 'c', 'cacbccbb', 'caccc', 'cba', 'cbc', 'cbccca', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baacba']
Pos
Counterexamples
[]

Round 8: No counterexamples found, skipped.
Accuracy at epoch 8: 0.646484375, total training samples: 28
Early stopping at epoch 268. Loss did not improve for 10 epochs.
Generate examples Step 268, Loss 0.27057319375440536
Epoch: 9
Negative Examples
['a', 'aabbbcbc', 'ababc', 'abaca', 'abc', 'acba', 'accbbcba', 'ba', 'bbbcba', 'bbcba', 'bbccbc', 'caabca', 'cba', 'cbabcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['cacba']
Pos
Counterexamples
['cacba']
Neg
Train Epoch 0, Loss 0.69338059425354
Train Epoch 1, Loss 0.5864763259887695
Train Epoch 2, Loss 0.4897545874118805
Accuracy at epoch 9: 0.4921875, total training samples: 29
Early stopping at epoch 337. Loss did not improve for 10 epochs.
Generate examples Step 337, Loss 0.3567317091148986
Epoch: 10
Negative Examples
['a', 'aabbba', 'abbabc', 'abbba', 'abcbbbba', 'bababa', 'baccaba', 'bbaba', 'bbacbba', 'bbba', 'bbbbbbba', 'bcabbbc', 'caacaba', 'caba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baccaba']
Pos
Train Epoch 0, Loss 0.8807716369628906
Train Epoch 1, Loss 0.739961564540863
Train Epoch 2, Loss 0.6223129034042358
Accuracy at epoch 10: 0.61328125, total training samples: 30
Early stopping at epoch 211. Loss did not improve for 10 epochs.
Generate examples Step 211, Loss 0.26935426258253603
Epoch: 11
Negative Examples
['aacbc', 'abcba', 'aca', 'acacaca', 'ba', 'bba', 'bbbabc', 'bbcabb', 'bc', 'bcbbabba', 'bcc', 'ca', 'cba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abcba']
Pos
Train Epoch 0, Loss 0.759316623210907
Train Epoch 1, Loss 0.6403428912162781
Train Epoch 2, Loss 0.5371825695037842
Accuracy at epoch 11: 0.658203125, total training samples: 31
Early stopping at epoch 264. Loss did not improve for 10 epochs.
Generate examples Step 264, Loss 0.2639267468789838
Epoch: 12
Negative Examples
[]

Positive Examples
['a', 'aba', 'abcca', 'acbbabc', 'ba', 'babaca', 'baca', 'bbabcba', 'bc', 'bcba', 'bcbc', 'cabaca', 'cabcbc']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['a', 'aba', 'abcca', 'acbbabc', 'ba', 'babaca', 'baca', 'bbabcba', 'bc', 'bcba', 'bcbc', 'cabaca', 'cabcbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.8220912218093872
Train Epoch 1, Loss 0.7245568037033081
Train Epoch 2, Loss 0.6392908096313477
Accuracy at epoch 12: 0.578125, total training samples: 44
Early stopping at epoch 351. Loss did not improve for 10 epochs.
Generate examples Step 351, Loss 0.28442867468534544
Epoch: 13
Negative Examples
['a', 'aaba', 'abc', 'ba', 'bba', 'bbc', 'bcba', 'cba', 'cbcba', 'ccbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabccba', 'accacaba', 'cabbba']
Pos Pos Pos
Counterexamples
['aabccba', 'accacaba', 'cabbba']
Neg Neg Neg
Train Epoch 0, Loss 0.7152066826820374
Train Epoch 1, Loss 0.6123246550559998
Train Epoch 2, Loss 0.5202314257621765
Accuracy at epoch 13: 0.482421875, total training samples: 47
Early stopping at epoch 283. Loss did not improve for 10 epochs.
Generate examples Step 283, Loss 0.3159430916880218
Epoch: 14
Negative Examples
['a', 'aaaabc', 'aaabcbba', 'aabcbba', 'abba', 'accba', 'bba', 'bbbc', 'bbc', 'bcbbc', 'cabcbbc', 'caccaba', 'cbbabcbc', 'cbbc', 'cbbcbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba']
Pos
Train Epoch 0, Loss 0.9850282669067383
Train Epoch 1, Loss 0.8278751373291016
Train Epoch 2, Loss 0.6950031518936157
Accuracy at epoch 14: 0.541015625, total training samples: 48
Early stopping at epoch 257. Loss did not improve for 10 epochs.
Generate examples Step 257, Loss 0.2727883933473003
Epoch: 15
Negative Examples
['a', 'aabbbcba', 'aabc', 'aba', 'ba', 'baabc', 'baacbc', 'babc', 'bacba', 'bc', 'bcbccba', 'c']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bba', 'bccba', 'cbabbba', 'ccbc']
Pos Pos Pos Pos
Counterexamples
['bacba', 'bba', 'bccba', 'cbabbba', 'ccbc']
Pos Neg Neg Neg Neg
Train Epoch 0, Loss 0.7896490097045898
Train Epoch 1, Loss 0.7460212707519531
Train Epoch 2, Loss 0.7132212519645691
Accuracy at epoch 15: 0.587890625, total training samples: 53
Early stopping at epoch 224. Loss did not improve for 10 epochs.
Generate examples Step 224, Loss 0.23484442121452756
Epoch: 16
Negative Examples
['aabaccc', 'abacb', 'aca', 'accaaacc', 'b', 'baababc', 'bacb', 'bacc', 'bb', 'bbaaca', 'bbacb', 'bbb', 'ca', 'cabcba', 'cbbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 16: No counterexamples found, skipped.
Accuracy at epoch 16: 0.58203125, total training samples: 53
Early stopping at epoch 217. Loss did not improve for 10 epochs.
Generate examples Step 217, Loss 0.2895035771054959
Epoch: 17
Negative Examples
['a', 'aacacabc', 'aacbc', 'abca', 'ba', 'bbbbbc', 'bcacc', 'bcbc', 'cacbaba', 'cacc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bbaaba', 'bbba', 'cbbbbaba']
Pos Pos Pos
Counterexamples
['bbaaba', 'bbba', 'cbbbbaba']
Neg Neg Neg
Train Epoch 0, Loss 0.7197475433349609
Train Epoch 1, Loss 0.6284162402153015
Train Epoch 2, Loss 0.5438266396522522
Accuracy at epoch 17: 0.5, total training samples: 56
Early stopping at epoch 327. Loss did not improve for 10 epochs.
Generate examples Step 327, Loss 0.41761426164246185
Epoch: 18
Negative Examples
['a', 'aaacaba', 'aaba', 'abacbc', 'abcccba', 'acbba', 'acbbc', 'ba', 'bbaaccba', 'bbcaaba', 'bbcbbba', 'bcbaba', 'cabbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abcccba']
Pos
Train Epoch 0, Loss 0.9479965567588806
Train Epoch 1, Loss 0.8117262125015259
Train Epoch 2, Loss 0.6971936225891113
Accuracy at epoch 18: 0.56640625, total training samples: 57
Early stopping at epoch 218. Loss did not improve for 10 epochs.
Generate examples Step 218, Loss 0.30716816815611436
Epoch: 19
Negative Examples
['aba', 'acabb', 'ba', 'baabc', 'bbaba', 'bbcabb', 'bc', 'bcba', 'c', 'cabbbc', 'ccbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bbacba', 'cabc']
Pos Pos
Counterexamples
['bbacba', 'cabc']
Neg Neg
Train Epoch 0, Loss 0.6996335983276367
Train Epoch 1, Loss 0.6028361320495605
Train Epoch 2, Loss 0.5156270265579224
Accuracy at epoch 19: 0.486328125, total training samples: 59
Early stopping at epoch 268. Loss did not improve for 10 epochs.
Generate examples Step 268, Loss 0.326164321496141
Epoch: 20
Negative Examples
['aaccbaba', 'aba', 'abba', 'abcaccba', 'abcba', 'acabc', 'ba', 'bacccba', 'bbba', 'bbc', 'bcba', 'bcbaaba', 'caba', 'cabba', 'cabbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'abcaccba', 'abcba', 'bacccba']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.9542930126190186
Train Epoch 1, Loss 0.8266890048980713
Train Epoch 2, Loss 0.7185019254684448
Accuracy at epoch 20: 0.595703125, total training samples: 63
Early stopping at epoch 215. Loss did not improve for 10 epochs.
Generate examples Step 215, Loss 0.28094880172499903
Epoch: 21
Negative Examples
['a', 'aaaccbb', 'aabc', 'ba', 'bacca', 'bbacacca', 'bbacba', 'bbcba', 'bcba', 'bcbc', 'bcbcca', 'ca', 'cba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 21: No counterexamples found, skipped.
Accuracy at epoch 21: 0.5703125, total training samples: 63
Early stopping at epoch 240. Loss did not improve for 10 epochs.
Generate examples Step 240, Loss 0.23299919990088436
Epoch: 22
Negative Examples
['aacc', 'abcbaaba', 'b', 'baacc', 'bb', 'bc', 'cacaacca', 'cacb', 'cb', 'cbb', 'cbbaba', 'cbbbaca', 'cc', 'cca', 'cccbacba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 22: No counterexamples found, skipped.
Accuracy at epoch 22: 0.59765625, total training samples: 63
Early stopping at epoch 207. Loss did not improve for 10 epochs.
Generate examples Step 207, Loss 0.2939761918611251
Epoch: 23
Negative Examples
['a', 'aaaaccca', 'abbbccbc', 'abbcba', 'acba', 'ba', 'baabcbc', 'bca', 'c', 'caabacbc', 'cabc', 'cabcaba', 'ccacbca', 'ccacbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 23: No counterexamples found, skipped.
Accuracy at epoch 23: 0.576171875, total training samples: 63
Early stopping at epoch 222. Loss did not improve for 10 epochs.
Generate examples Step 222, Loss 0.27668319381940526
Epoch: 24
Negative Examples
['aba', 'acbccbc', 'b', 'ba', 'bbaaaca', 'bbbccbc', 'bbcbccca', 'bc', 'bcbaaaca', 'c', 'cba', 'ccbbccbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 24: No counterexamples found, skipped.
Accuracy at epoch 24: 0.583984375, total training samples: 63
Early stopping at epoch 241. Loss did not improve for 10 epochs.
Generate examples Step 241, Loss 0.2701747459936733
Epoch: 25
Negative Examples
['aababc', 'abcbb', 'acccbbca', 'b', 'baaabb', 'bacabcba', 'bacca', 'bb', 'bbacabb', 'bbbaaccc', 'c', 'ca', 'caabbabc', 'cbc', 'cccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 25: No counterexamples found, skipped.
Accuracy at epoch 25: 0.583984375, total training samples: 63
Early stopping at epoch 303. Loss did not improve for 10 epochs.
Generate examples Step 303, Loss 0.2603637865303378
Epoch: 26
Negative Examples
['a', 'accbb', 'ba', 'baaca', 'baba', 'bacca', 'bbbaacbb', 'bbbbcaca', 'bbc', 'bbcba', 'bca', 'bcbbcba', 'cba', 'cbaacba', 'cbbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baba']
Pos
Train Epoch 0, Loss 0.766791582107544
Train Epoch 1, Loss 0.6737536191940308
Train Epoch 2, Loss 0.5910747051239014
Accuracy at epoch 26: 0.62109375, total training samples: 64
Early stopping at epoch 283. Loss did not improve for 10 epochs.
Generate examples Step 283, Loss 0.23418662329794657
Epoch: 27
Negative Examples
['aaccb', 'abb', 'acaacca', 'accc', 'b', 'baabb', 'baabccc', 'baccb', 'bb', 'bbaabb', 'bcababc', 'bccb', 'bccc', 'cabcbb', 'cca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['cbacba']
Pos
Counterexamples
['cbacba']
Neg
Train Epoch 0, Loss 0.7648564577102661
Train Epoch 1, Loss 0.6496111750602722
Train Epoch 2, Loss 0.545656144618988
Accuracy at epoch 27: 0.568359375, total training samples: 65
Early stopping at epoch 240. Loss did not improve for 10 epochs.
Generate examples Step 240, Loss 0.25271913916243555
Epoch: 28
Negative Examples
['aaabb', 'aba', 'abacc', 'abcbb', 'acbc', 'ba', 'baaca', 'babcca', 'bacbc', 'bcabcba', 'bccaaaba', 'c', 'caacba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 28: No counterexamples found, skipped.
Accuracy at epoch 28: 0.5625, total training samples: 65
Early stopping at epoch 207. Loss did not improve for 10 epochs.
Generate examples Step 207, Loss 0.23097153222904757
Epoch: 29
Negative Examples
['aaabb', 'aabacba', 'aaca', 'aba', 'ababb', 'abc', 'aca', 'bc', 'ca', 'cabcba', 'cabcbc', 'cbbabbc', 'cbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abcbc']
Pos
Counterexamples
['abcbc']
Neg
Train Epoch 0, Loss 0.7070223689079285
Train Epoch 1, Loss 0.6148537993431091
Train Epoch 2, Loss 0.534010648727417
Accuracy at epoch 29: 0.47265625, total training samples: 66
Early stopping at epoch 331. Loss did not improve for 10 epochs.
Generate examples Step 331, Loss 0.35491216568702677
Epoch: 30
Negative Examples
['a', 'aaacbca', 'aba', 'abba', 'abbbcaba', 'baba', 'bba', 'bcccba', 'ccba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acaba', 'babcbba', 'caabcbba', 'cbba']
Pos Pos Pos Pos
Counterexamples
['abba', 'abbbcaba', 'baba', 'acaba', 'babcbba', 'caabcbba', 'cbba']
Pos Pos Pos Neg Neg Neg Neg
Train Epoch 0, Loss 0.7429250478744507
Train Epoch 1, Loss 0.7233017683029175
Train Epoch 2, Loss 0.7082962393760681
Accuracy at epoch 30: 0.462890625, total training samples: 73
Early stopping at epoch 249. Loss did not improve for 10 epochs.
Generate examples Step 249, Loss 0.26949551498889923
Epoch: 31
Negative Examples
['a', 'aaca', 'aacbca', 'ababba', 'abbacca', 'abbbb', 'babba', 'bba', 'bcacba', 'bcbaaca', 'bcbbbca', 'bcbcaba', 'caccba', 'cbcc', 'ccaabc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 31: No counterexamples found, skipped.
Accuracy at epoch 31: 0.509765625, total training samples: 73
Early stopping at epoch 254. Loss did not improve for 10 epochs.
Generate examples Step 254, Loss 0.2845412129864973
Epoch: 32
Negative Examples
['a', 'aaaba', 'aaabba', 'aaccbcba', 'aba', 'abbccca', 'ba', 'baaba', 'baaca', 'bcabba', 'ca', 'caca', 'ccaaaba', 'ccba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaba']
Pos
Train Epoch 0, Loss 0.7989190816879272
Train Epoch 1, Loss 0.7039220333099365
Train Epoch 2, Loss 0.6199432611465454
Accuracy at epoch 32: 0.59375, total training samples: 74
Early stopping at epoch 207. Loss did not improve for 10 epochs.
Generate examples Step 207, Loss 0.2556416684618363
Epoch: 33
Negative Examples
['abb', 'abc', 'abcca', 'b', 'bbcbb', 'bbcca', 'bc', 'bcacbcca', 'bcbc', 'ca', 'cbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['a', 'abaca', 'abcba', 'bbbababa']
Pos Pos Pos Pos
Counterexamples
['a', 'abaca', 'bbbababa']
Neg Neg Neg
Train Epoch 0, Loss 0.7527435421943665
Train Epoch 1, Loss 0.6766507625579834
Train Epoch 2, Loss 0.608593225479126
Accuracy at epoch 33: 0.474609375, total training samples: 77
Early stopping at epoch 278. Loss did not improve for 10 epochs.
Generate examples Step 278, Loss 0.27341643867740495
Epoch: 34
Negative Examples
['a', 'aabaabba', 'aba', 'ababa', 'acbaaba', 'babca', 'bba', 'cba', 'cbcaba', 'ccba', 'cccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bcaba', 'bcbba']
Pos Pos
Counterexamples
['ababa', 'bcaba', 'bcbba']
Pos Neg Neg
Train Epoch 0, Loss 0.7485844492912292
Train Epoch 1, Loss 0.721411406993866
Train Epoch 2, Loss 0.6986653804779053
Accuracy at epoch 34: 0.490234375, total training samples: 80
Early stopping at epoch 220. Loss did not improve for 10 epochs.
Generate examples Step 220, Loss 0.2397323386296967
Epoch: 35
Negative Examples
['a', 'acaccbc', 'acbc', 'accbabb', 'b', 'ba', 'bacc', 'c', 'cbcbcca', 'cbcca', 'ccababb', 'ccbc', 'ccbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 35: No counterexamples found, skipped.
Accuracy at epoch 35: 0.4765625, total training samples: 80
Early stopping at epoch 292. Loss did not improve for 10 epochs.
Generate examples Step 292, Loss 0.33368387533536137
Epoch: 36
Negative Examples
['aaba', 'aacaaaba', 'aacbbba', 'abbba', 'acbaaba', 'ba', 'baaba', 'bbbaba', 'bbbbaaba', 'bcacaba', 'caaaaba', 'cbabaaba', 'cbbbcaba', 'cbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaba']
Pos
Train Epoch 0, Loss 0.7201589345932007
Train Epoch 1, Loss 0.6349862813949585
Train Epoch 2, Loss 0.5586624145507812
Accuracy at epoch 36: 0.578125, total training samples: 81
Early stopping at epoch 234. Loss did not improve for 10 epochs.
Generate examples Step 234, Loss 0.24601456909737687
Epoch: 37
Negative Examples
['acacabb', 'b', 'babcbb', 'bacc', 'bbbcca', 'bbccbc', 'bbccc', 'ca', 'cacccb', 'cb', 'cbbcbb', 'cc', 'ccbabacb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baaabaca', 'cba']
Pos Pos
Counterexamples
['baaabaca', 'cba']
Neg Neg
Train Epoch 0, Loss 0.7112289667129517
Train Epoch 1, Loss 0.6272052526473999
Train Epoch 2, Loss 0.5501391291618347
Accuracy at epoch 37: 0.515625, total training samples: 83
Early stopping at epoch 298. Loss did not improve for 10 epochs.
Generate examples Step 298, Loss 0.27561083221714633
Epoch: 38
Negative Examples
['abaca', 'abccacc', 'ba', 'baaca', 'babacabc', 'bacbbbc', 'bbacbba', 'cabbbaba', 'cacbacba', 'cacbcbba', 'cbcaacba', 'cbcaba', 'cbccacc', 'ccacba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acabba', 'ccbbba']
Pos Pos
Counterexamples
['acabba', 'ccbbba']
Neg Neg
Train Epoch 0, Loss 0.715729296207428
Train Epoch 1, Loss 0.6125129461288452
Train Epoch 2, Loss 0.5220115184783936
Accuracy at epoch 38: 0.498046875, total training samples: 85
Early stopping at epoch 207. Loss did not improve for 10 epochs.
Generate examples Step 207, Loss 0.2270173033556113
Epoch: 39
Negative Examples
['a', 'aabbaa', 'aba', 'acbcbaba', 'ba', 'babba', 'babca', 'bacacca', 'bbbbcca', 'bbcbabca', 'bcba', 'caca', 'cba', 'cbcbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 39: No counterexamples found, skipped.
Accuracy at epoch 39: 0.525390625, total training samples: 85
Early stopping at epoch 261. Loss did not improve for 10 epochs.
Generate examples Step 261, Loss 0.24066463101456184
Epoch: 40
Negative Examples
['a', 'aaba', 'aba', 'ababc', 'abba', 'abc', 'acbc', 'accacc', 'ba', 'bbc', 'c', 'caabccba', 'cabba', 'ccaacc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba']
Pos
Train Epoch 0, Loss 0.9511401057243347
Train Epoch 1, Loss 0.8216565251350403
Train Epoch 2, Loss 0.7111904621124268
Accuracy at epoch 40: 0.5390625, total training samples: 86
Early stopping at epoch 220. Loss did not improve for 10 epochs.
Generate examples Step 220, Loss 0.23485026525426234
Epoch: 41
Negative Examples
['a', 'aaabaccc', 'abcba', 'aca', 'b', 'bbaca', 'bbbb', 'bbcc', 'bcba', 'c', 'cba', 'cbbabc', 'cbbbbc', 'cbbcbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abcba']
Pos
Train Epoch 0, Loss 0.8152546882629395
Train Epoch 1, Loss 0.7188721299171448
Train Epoch 2, Loss 0.6346544027328491
Accuracy at epoch 41: 0.576171875, total training samples: 87
Early stopping at epoch 227. Loss did not improve for 10 epochs.
Generate examples Step 227, Loss 0.21988264445150107
Epoch: 42
Negative Examples
['aabacb', 'abcbbcc', 'acb', 'bacaccca', 'bacc', 'bb', 'bbb', 'bbcbb', 'bcbbbcbc', 'bcca', 'ca', 'caa', 'cbbcaa', 'ccacbccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baca']
Pos
Counterexamples
['baca']
Neg
Train Epoch 0, Loss 0.7659231424331665
Train Epoch 1, Loss 0.6778535842895508
Train Epoch 2, Loss 0.599420964717865
Accuracy at epoch 42: 0.51171875, total training samples: 88
Early stopping at epoch 209. Loss did not improve for 10 epochs.
Generate examples Step 209, Loss 0.2043980074780328
Epoch: 43
Negative Examples
['aabaca', 'abcab', 'acabcbc', 'acb', 'babaca', 'babbcca', 'bb', 'bbbaccc', 'bc', 'bca', 'cacbacca', 'cbbcbc', 'cbcbcba', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 43: No counterexamples found, skipped.
Accuracy at epoch 43: 0.515625, total training samples: 88
Early stopping at epoch 219. Loss did not improve for 10 epochs.
Generate examples Step 219, Loss 0.27114997959949755
Epoch: 44
Negative Examples
['a', 'acacaba', 'acccbbc', 'ba', 'babbabc', 'bcaababa', 'cbbbacbc', 'ccba']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabcacba', 'acbbabba', 'baabbc', 'bbbba', 'cbcba']
Pos Pos Pos Pos Pos
Counterexamples
['aabcacba', 'acbbabba', 'baabbc', 'bbbba', 'cbcba']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7068055868148804
Train Epoch 1, Loss 0.6277227401733398
Train Epoch 2, Loss 0.553836464881897
Accuracy at epoch 44: 0.48828125, total training samples: 93
Early stopping at epoch 284. Loss did not improve for 10 epochs.
Generate examples Step 284, Loss 0.3052697072949326
Epoch: 45
Negative Examples
['aaaaba', 'aabcbba', 'aba', 'ba', 'baba', 'bbababa', 'bbacbaba', 'bbba', 'bc', 'caabc', 'cabbbaba', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baba']
Pos
Train Epoch 0, Loss 0.974492609500885
Train Epoch 1, Loss 0.8587161302566528
Train Epoch 2, Loss 0.7594478726387024
Accuracy at epoch 45: 0.564453125, total training samples: 94
Early stopping at epoch 239. Loss did not improve for 10 epochs.
Generate examples Step 239, Loss 0.25870059300214054
Epoch: 46
Negative Examples
['a', 'aabacabc', 'aabb', 'aacabba', 'acca', 'ba', 'bba', 'bbcaba', 'bcbacba', 'bcbbcba', 'cbcca', 'cccbca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baba']
Pos
Counterexamples
[]

Round 46: No counterexamples found, skipped.
Accuracy at epoch 46: 0.576171875, total training samples: 94
Early stopping at epoch 179. Loss did not improve for 10 epochs.
Generate examples Step 179, Loss 0.22911953868137466
Epoch: 47
Negative Examples
['a', 'aabcabb', 'acbc', 'b', 'bb', 'bbabb', 'bca', 'bcacc', 'bcc', 'bcccc', 'ca', 'cba', 'ccaabcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 47: No counterexamples found, skipped.
Accuracy at epoch 47: 0.544921875, total training samples: 94
Early stopping at epoch 302. Loss did not improve for 10 epochs.
Generate examples Step 302, Loss 0.29037261825583555
Epoch: 48
Negative Examples
['a', 'aabc', 'aba', 'acbcbaba', 'ba', 'baabbcba', 'cba', 'cbcba', 'ccbccba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acaba', 'cbaba']
Pos Pos
Counterexamples
['baabbcba', 'acaba', 'cbaba']
Pos Neg Neg
Train Epoch 0, Loss 0.8007779717445374
Train Epoch 1, Loss 0.7606135010719299
Train Epoch 2, Loss 0.7298917174339294
Accuracy at epoch 48: 0.603515625, total training samples: 97
Early stopping at epoch 253. Loss did not improve for 10 epochs.
Generate examples Step 253, Loss 0.22307574244465414
Epoch: 49
Negative Examples
['a', 'aa', 'baab', 'babb', 'babca', 'bbababcc', 'bbcac', 'bc', 'c', 'caa', 'caaababc', 'cabbca', 'cacabaaa', 'cacccc', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 49: No counterexamples found, skipped.
Accuracy at epoch 49: 0.56640625, total training samples: 97
Early stopping at epoch 215. Loss did not improve for 10 epochs.
Generate examples Step 215, Loss 0.24453328377394765
Epoch: 50
Negative Examples
['a', 'aababba', 'aabccbc', 'abcbc', 'ba', 'babc', 'bbcba', 'bbcbba', 'bca', 'bccaca', 'c']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bbcaaba']
Pos
Counterexamples
['bbcaaba']
Neg
Train Epoch 0, Loss 0.6973940134048462
Train Epoch 1, Loss 0.6291773915290833
Train Epoch 2, Loss 0.5646987557411194
Accuracy at epoch 50: 0.494140625, total training samples: 98
Early stopping at epoch 261. Loss did not improve for 10 epochs.
Generate examples Step 261, Loss 0.3414042148426289
Epoch: 51
Negative Examples
['a', 'aabbc', 'abbbaba', 'ba', 'babacbbc', 'bba', 'bbaba', 'bbbccbba', 'bc', 'bcaaba', 'bcbba', 'caabc', 'caacbbba', 'cabbc', 'cbbc', 'ccbccba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abbbaba']
Pos
Train Epoch 0, Loss 0.9363937377929688
Train Epoch 1, Loss 0.844132125377655
Train Epoch 2, Loss 0.7638958692550659
Accuracy at epoch 51: 0.587890625, total training samples: 99
Early stopping at epoch 270. Loss did not improve for 10 epochs.
Generate examples Step 270, Loss 0.2669666755903251
Epoch: 52
Negative Examples
['a', 'abbba', 'abc', 'abcccbbc', 'abccccca', 'accbcabc', 'ba', 'bbaca', 'bbca', 'bcbcbcba', 'cbaccabc', 'cbcbba', 'ccbabc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaba']
Pos
Counterexamples
['aaba']
Neg
Train Epoch 0, Loss 0.7162156105041504
Train Epoch 1, Loss 0.6461778879165649
Train Epoch 2, Loss 0.5803146958351135
Accuracy at epoch 52: 0.5, total training samples: 100
Early stopping at epoch 229. Loss did not improve for 10 epochs.
Generate examples Step 229, Loss 0.32634912962498874
Epoch: 53
Negative Examples
['a', 'aabbaaca', 'abaabba', 'abbabba', 'ba', 'babbaba', 'baccacba', 'bbcbcaba', 'bc', 'bcbacca', 'bccbcca', 'c']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['babbaba', 'baccacba']
Pos Pos
Train Epoch 0, Loss 0.9104868173599243
Train Epoch 1, Loss 0.8270565867424011
Train Epoch 2, Loss 0.75370854139328
Accuracy at epoch 53: 0.611328125, total training samples: 102
Early stopping at epoch 264. Loss did not improve for 10 epochs.
Generate examples Step 264, Loss 0.27211940423497616
Epoch: 54
Negative Examples
['a', 'aaabcbc', 'aabcca', 'aba', 'abbbbcca', 'acbbaca', 'accbcbc', 'ba', 'bc', 'cbc', 'ccbaba', 'ccbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acbbaba']
Pos
Counterexamples
['acbbaba']
Neg
Train Epoch 0, Loss 0.705082893371582
Train Epoch 1, Loss 0.6373231410980225
Train Epoch 2, Loss 0.5752900838851929
Accuracy at epoch 54: 0.517578125, total training samples: 103
Early stopping at epoch 242. Loss did not improve for 10 epochs.
Generate examples Step 242, Loss 0.2599201994668309
Epoch: 55
Negative Examples
['aa', 'aababc', 'abbcabbc', 'b', 'ba', 'baabacbb', 'bacbc', 'bc', 'bcbabcbc', 'bcc', 'cabc', 'cacca', 'cbc', 'ccaca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 55: No counterexamples found, skipped.
Accuracy at epoch 55: 0.505859375, total training samples: 103
Early stopping at epoch 163. Loss did not improve for 10 epochs.
Generate examples Step 163, Loss 0.25943356043681864
Epoch: 56
Negative Examples
['aacbcaca', 'b', 'baacbbab', 'babaa', 'babbbcca', 'bacb', 'bbacbaaa', 'bcaacaa', 'bcbb', 'bcc', 'caccba', 'ccbbbaca', 'cccacaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 56: No counterexamples found, skipped.
Accuracy at epoch 56: 0.5390625, total training samples: 103
Early stopping at epoch 196. Loss did not improve for 10 epochs.
Generate examples Step 196, Loss 0.24536790384858997
Epoch: 57
Negative Examples
['a', 'aaa', 'aabcbc', 'aabccbc', 'abcc', 'acbaca', 'acbb', 'accccaab', 'baaacca', 'babbabc', 'bbaab', 'bcba', 'bccc', 'ca', 'cc', 'ccbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 57: No counterexamples found, skipped.
Accuracy at epoch 57: 0.509765625, total training samples: 103
Early stopping at epoch 189. Loss did not improve for 10 epochs.
Generate examples Step 189, Loss 0.3074452051990911
Epoch: 58
Negative Examples
['a', 'aacaca', 'abaca', 'acbababc', 'acbccc', 'ba', 'baabbcba', 'babcca', 'bacccba', 'bbaaaa', 'bc', 'bca', 'bcc', 'cabbba', 'cbcbacc', 'cbcca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baabbcba', 'bacccba']
Pos Pos
Train Epoch 0, Loss 0.8767071962356567
Train Epoch 1, Loss 0.7963622808456421
Train Epoch 2, Loss 0.7249939441680908
Accuracy at epoch 58: 0.625, total training samples: 105
Early stopping at epoch 185. Loss did not improve for 10 epochs.
Generate examples Step 185, Loss 0.2379684199889501
Epoch: 59
Negative Examples
['aaacbc', 'aaca', 'abaabbb', 'abca', 'abcbcbb', 'b', 'babc', 'bacbaabb', 'bacbb', 'bb', 'bbcbbba', 'c', 'cbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baba']
Pos
Counterexamples
[]

Round 59: No counterexamples found, skipped.
Accuracy at epoch 59: 0.595703125, total training samples: 105
Early stopping at epoch 203. Loss did not improve for 10 epochs.
Generate examples Step 203, Loss 0.22999180685363563
Epoch: 60
Negative Examples
['ababbbb', 'abb', 'accaaaca', 'ba', 'bb', 'bbacb', 'bcacbc', 'bcccbb', 'ca', 'cacbabc', 'cbcabcbb', 'cbcbabba', 'ccccbbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 60: No counterexamples found, skipped.
Accuracy at epoch 60: 0.59765625, total training samples: 105
Early stopping at epoch 186. Loss did not improve for 10 epochs.
Generate examples Step 186, Loss 0.23041356311124914
Epoch: 61
Negative Examples
['aaacaacc', 'abbbaaca', 'acacbccc', 'acbb', 'accb', 'baabaccc', 'bb', 'bbba', 'bbc', 'bcaabb', 'c', 'caaaaaca', 'cca', 'ccaabcc', 'ccbbbacc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 61: No counterexamples found, skipped.
Accuracy at epoch 61: 0.58203125, total training samples: 105
Early stopping at epoch 190. Loss did not improve for 10 epochs.
Generate examples Step 190, Loss 0.2287145543316896
Epoch: 62
Negative Examples
['aababb', 'ababbca', 'abbc', 'abbcca', 'abca', 'accc', 'b', 'ba', 'bbbbba', 'bbc', 'bc', 'cababc', 'cabbbbb', 'ccacb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['cbcbcba']
Pos
Counterexamples
['cbcbcba']
Neg
Train Epoch 0, Loss 0.7120811343193054
Train Epoch 1, Loss 0.6456116437911987
Train Epoch 2, Loss 0.5847100019454956
Accuracy at epoch 62: 0.501953125, total training samples: 106
Early stopping at epoch 206. Loss did not improve for 10 epochs.
Generate examples Step 206, Loss 0.2793257076671158
Epoch: 63
Negative Examples
['a', 'aa', 'abbca', 'acabca', 'ba', 'bbabb', 'bbbcbbc', 'bc', 'bcccbca', 'c', 'ca', 'caacbbc', 'cabbbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 63: No counterexamples found, skipped.
Accuracy at epoch 63: 0.484375, total training samples: 106
Early stopping at epoch 266. Loss did not improve for 10 epochs.
Generate examples Step 266, Loss 0.382567416192887
Epoch: 64
Negative Examples
['a', 'aaaccabc', 'aaba', 'aabaaba', 'abbcacba', 'abcbc', 'acaacbc', 'acabbbba', 'bbbc', 'bbccbaba', 'bccbbba', 'cba', 'ccbacba', 'cccbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 64: No counterexamples found, skipped.
Accuracy at epoch 64: 0.50390625, total training samples: 106
Early stopping at epoch 192. Loss did not improve for 10 epochs.
Generate examples Step 192, Loss 0.27685974962970755
Epoch: 65
Negative Examples
['a', 'aaacbcc', 'aba', 'abbcaba', 'abc', 'accba', 'ba', 'baba', 'bc', 'bcbbaba', 'ca', 'caacbabc', 'ccaca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baba']
Pos
Train Epoch 0, Loss 0.8736627101898193
Train Epoch 1, Loss 0.7947500944137573
Train Epoch 2, Loss 0.7237219214439392
Accuracy at epoch 65: 0.6171875, total training samples: 107
Early stopping at epoch 214. Loss did not improve for 10 epochs.
Generate examples Step 214, Loss 0.24752503612706828
Epoch: 66
Negative Examples
['a', 'aacbaba', 'abc', 'acaabba', 'acbaba', 'b', 'babbaccb', 'bacbbba', 'bca', 'bcbacbc', 'c', 'cbc', 'ccabbccc', 'ccacabba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacbbba']
Pos
Train Epoch 0, Loss 0.7309036254882812
Train Epoch 1, Loss 0.6739317178726196
Train Epoch 2, Loss 0.6217575669288635
Accuracy at epoch 66: 0.505859375, total training samples: 108
Early stopping at epoch 247. Loss did not improve for 10 epochs.
Generate examples Step 247, Loss 0.2409969126024554
Epoch: 67
Negative Examples
['aaaabbbb', 'aaabb', 'baacbb', 'babcbacc', 'bbacb', 'cb']
Neg Neg Neg Neg Neg Neg
Positive Examples
['a', 'aababa', 'ababc', 'ba', 'baacbbbc', 'bbcabbba', 'bc', 'cababa']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['a', 'aababa', 'ababc', 'ba', 'baacbbbc', 'bbcabbba', 'bc', 'cababa']
Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7740445137023926
Train Epoch 1, Loss 0.7198517322540283
Train Epoch 2, Loss 0.6693664193153381
Accuracy at epoch 67: 0.59765625, total training samples: 116
Early stopping at epoch 180. Loss did not improve for 10 epochs.
Generate examples Step 180, Loss 0.2743540840912919
Epoch: 68
Negative Examples
['aabca', 'aca', 'accbb', 'bbabbbb', 'bbb', 'bc', 'bcccb', 'c', 'cabb', 'cbbba', 'cbca', 'ccbaaaca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baaabcba']
Pos
Counterexamples
['baaabcba']
Neg
Train Epoch 0, Loss 0.7137120366096497
Train Epoch 1, Loss 0.6471468210220337
Train Epoch 2, Loss 0.5852097272872925
Accuracy at epoch 68: 0.4765625, total training samples: 117
Early stopping at epoch 268. Loss did not improve for 10 epochs.
Generate examples Step 268, Loss 0.3566053146323307
Epoch: 69
Negative Examples
['a', 'aababa', 'aabbba', 'aacbba', 'aaccaaba', 'abbbbbba', 'ba', 'babcabba', 'babcabca', 'babcba', 'bacbba', 'bbba', 'bccabbba', 'cbbacaba', 'ccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abbbbbba']
Pos
Train Epoch 0, Loss 0.8732889890670776
Train Epoch 1, Loss 0.8028885722160339
Train Epoch 2, Loss 0.7407094240188599
Accuracy at epoch 69: 0.625, total training samples: 118
Early stopping at epoch 166. Loss did not improve for 10 epochs.
Generate examples Step 166, Loss 0.27622143094411156
Epoch: 70
Negative Examples
['a', 'aaabcbc', 'aababbca', 'aabbbb', 'b', 'ba', 'bbcacca', 'bcbaaaca', 'c', 'cababa', 'ccc', 'ccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['accbcbba', 'bbba']
Pos Pos
Counterexamples
['accbcbba', 'bbba']
Neg Neg
Train Epoch 0, Loss 0.7209458351135254
Train Epoch 1, Loss 0.6622966527938843
Train Epoch 2, Loss 0.6071345806121826
Accuracy at epoch 70: 0.50390625, total training samples: 120
Early stopping at epoch 139. Loss did not improve for 10 epochs.
Generate examples Step 139, Loss 0.260350460346256
Epoch: 71
Negative Examples
['aa', 'b', 'ba', 'baa', 'bb', 'bbbacab', 'bbbcb', 'bbc', 'bc', 'c', 'ca', 'cb', 'cba', 'cca', 'ccbbacc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 71: No counterexamples found, skipped.
Accuracy at epoch 71: 0.4765625, total training samples: 120
Early stopping at epoch 180. Loss did not improve for 10 epochs.
Generate examples Step 180, Loss 0.25446251899168637
Epoch: 72
Negative Examples
['aacaabc', 'abba', 'abbca', 'acac', 'b', 'ba', 'baa', 'baacc', 'baca', 'cacbc', 'cbabbbc', 'cbbabbac', 'cbbb', 'cbcabcbc', 'ccabcb', 'ccbcc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba']
Pos
Train Epoch 0, Loss 0.8451333045959473
Train Epoch 1, Loss 0.7715644836425781
Train Epoch 2, Loss 0.7052891254425049
Accuracy at epoch 72: 0.611328125, total training samples: 121
Early stopping at epoch 235. Loss did not improve for 10 epochs.
Generate examples Step 235, Loss 0.28545642038018015
Epoch: 73
Negative Examples
['aacaba', 'abcababc', 'acbb', 'ba', 'bacaca', 'bbaaacc', 'bbaaca', 'bbcba', 'bbcbaba', 'bbcbccc', 'bca', 'caabb', 'cbc', 'ccaacba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabacba', 'caba']
Pos Pos
Counterexamples
['aaabacba', 'caba']
Neg Neg
Train Epoch 0, Loss 0.724044919013977
Train Epoch 1, Loss 0.6608589887619019
Train Epoch 2, Loss 0.6020539402961731
Accuracy at epoch 73: 0.513671875, total training samples: 123
Early stopping at epoch 207. Loss did not improve for 10 epochs.
Generate examples Step 207, Loss 0.29142107255756855
Epoch: 74
Negative Examples
['a', 'aba', 'abaaca', 'acca', 'ba', 'babbaca', 'bbcaba', 'bbcc', 'bc', 'bcabb', 'bccacac', 'ca', 'cabbc', 'cabbca', 'cbabca', 'cbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 74: No counterexamples found, skipped.
Accuracy at epoch 74: 0.49609375, total training samples: 123
Early stopping at epoch 202. Loss did not improve for 10 epochs.
Generate examples Step 202, Loss 0.34831860162354455
Epoch: 75
Negative Examples
['a', 'aaa', 'abacacbc', 'abca', 'aca', 'b', 'babbaa', 'baccc', 'bbcc', 'bcccca', 'caa', 'cbbabca', 'cbcbcaaa', 'ccaa', 'ccbaaaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 75: No counterexamples found, skipped.
Accuracy at epoch 75: 0.498046875, total training samples: 123
Early stopping at epoch 190. Loss did not improve for 10 epochs.
Generate examples Step 190, Loss 0.29625505224572424
Epoch: 76
Negative Examples
['a', 'aa', 'aabbb', 'aca', 'ba', 'bbaabc', 'bbbcca', 'bc', 'bcbcba', 'bcbcc', 'caab', 'cababaaa', 'cba', 'cbbb', 'cccacaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 76: No counterexamples found, skipped.
Accuracy at epoch 76: 0.5, total training samples: 123
Early stopping at epoch 189. Loss did not improve for 10 epochs.
Generate examples Step 189, Loss 0.3090253698198419
Epoch: 77
Negative Examples
['a', 'aaba', 'abbc', 'abc', 'acbccc', 'ba', 'baa', 'bbabac', 'bbabbcaa', 'c', 'cba', 'cbbccbca', 'cbca', 'ccbca', 'ccccabaa', 'cccccbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 77: No counterexamples found, skipped.
Accuracy at epoch 77: 0.509765625, total training samples: 123
Early stopping at epoch 150. Loss did not improve for 10 epochs.
Generate examples Step 150, Loss 0.27358433229244306
Epoch: 78
Negative Examples
['a', 'aabc', 'aaccaccc', 'acaacb', 'acacc', 'acbacba', 'b', 'bacbcaa', 'caabcba', 'cbcba', 'cc', 'cca', 'cccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 78: No counterexamples found, skipped.
Accuracy at epoch 78: 0.53125, total training samples: 123
Early stopping at epoch 160. Loss did not improve for 10 epochs.
Generate examples Step 160, Loss 0.2712982219198476
Epoch: 79
Negative Examples
['a', 'aaa', 'ababbb', 'abcabcc', 'abccacb', 'acc', 'bca', 'bcbaa', 'bcbbaaba', 'cacababb', 'cacabba', 'cacabbbc', 'cacb', 'cacbb', 'cb', 'cbcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 79: No counterexamples found, skipped.
Accuracy at epoch 79: 0.55859375, total training samples: 123
Early stopping at epoch 197. Loss did not improve for 10 epochs.
Generate examples Step 197, Loss 0.29041205662669556
Epoch: 80
Negative Examples
['a', 'aaabbb', 'abaa', 'abacc', 'acacbba', 'baacca', 'baccbab', 'bba', 'bca', 'ca', 'caaabca', 'caabba', 'cab', 'caccbc', 'cbcbbca', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 80: No counterexamples found, skipped.
Accuracy at epoch 80: 0.515625, total training samples: 123
Early stopping at epoch 168. Loss did not improve for 10 epochs.
Generate examples Step 168, Loss 0.31288658479261683
Epoch: 81
Negative Examples
['aacaaac', 'aba', 'abaabbba', 'abba', 'abbbbbbc', 'abc', 'bacaa', 'bacac', 'bbcac', 'bcbaab', 'c', 'caabbba', 'caccc', 'cbcaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaabbba', 'abba', 'bacac']
Pos Pos Pos
Train Epoch 0, Loss 0.8710383772850037
Train Epoch 1, Loss 0.8090810775756836
Train Epoch 2, Loss 0.755046546459198
Accuracy at epoch 81: 0.498046875, total training samples: 126
Early stopping at epoch 166. Loss did not improve for 10 epochs.
Generate examples Step 166, Loss 0.27736719247109876
Epoch: 82
Negative Examples
['aacbc', 'aca', 'accca', 'b', 'bacb', 'bc', 'bcbb', 'c', 'cb', 'cbcca', 'ccacb', 'cccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aacba', 'baacabba', 'cbba']
Pos Pos Pos
Counterexamples
['aacba', 'baacabba', 'cbba']
Neg Neg Neg
Train Epoch 0, Loss 0.7201571464538574
Train Epoch 1, Loss 0.6636777520179749
Train Epoch 2, Loss 0.6105456948280334
Accuracy at epoch 82: 0.494140625, total training samples: 129
Early stopping at epoch 150. Loss did not improve for 10 epochs.
Generate examples Step 150, Loss 0.29878989789659616
Epoch: 83
Negative Examples
['aaabcbba', 'aaaca', 'aabbbcac', 'acbbc', 'acccb', 'b', 'babcb', 'baccb', 'c', 'caacbc', 'cbacba', 'cbb', 'cbbababc', 'cbbcbcb', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 83: No counterexamples found, skipped.
Accuracy at epoch 83: 0.544921875, total training samples: 129
Early stopping at epoch 159. Loss did not improve for 10 epochs.
Generate examples Step 159, Loss 0.25774528719484807
Epoch: 84
Negative Examples
['aaaabcb', 'aaabcc', 'aaacab', 'aabbc', 'aacbbcab', 'ab', 'abccac', 'acabbaa', 'ba', 'bac', 'bbab', 'bbbbba', 'bc', 'bcb', 'bcbaccbc', 'caacbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abccac']
Pos
Train Epoch 0, Loss 0.9116834402084351
Train Epoch 1, Loss 0.8438283801078796
Train Epoch 2, Loss 0.782421350479126
Accuracy at epoch 84: 0.494140625, total training samples: 130
Early stopping at epoch 190. Loss did not improve for 10 epochs.
Generate examples Step 190, Loss 0.3432542708219658
Epoch: 85
Negative Examples
['aacbacbc', 'acbaab', 'acbbca', 'b', 'ba', 'bbbbcbb', 'c', 'ca', 'caaaab', 'cabca', 'cacbac', 'cacbaca', 'ccaccbcb', 'ccbbab', 'cccaabcc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 85: No counterexamples found, skipped.
Accuracy at epoch 85: 0.5, total training samples: 130
Early stopping at epoch 189. Loss did not improve for 10 epochs.
Generate examples Step 189, Loss 0.27871974063547034
Epoch: 86
Negative Examples
['a', 'aaaacbb', 'abbccbcc', 'acbcaacb', 'accabba', 'b', 'baaacc', 'baacbb', 'bba', 'bbbb', 'bca', 'cbba', 'cbbca', 'cc', 'ccbcabcc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 86: No counterexamples found, skipped.
Accuracy at epoch 86: 0.48828125, total training samples: 130
Early stopping at epoch 140. Loss did not improve for 10 epochs.
Generate examples Step 140, Loss 0.2978806981803678
Epoch: 87
Negative Examples
['a', 'aaacacab', 'aac', 'ab', 'abababb', 'ac', 'accacaab', 'accbaa', 'bcccbb', 'c', 'cabbcc', 'cabca', 'cacaaa', 'cba', 'cc', 'cca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 87: No counterexamples found, skipped.
Accuracy at epoch 87: 0.51953125, total training samples: 130
Early stopping at epoch 154. Loss did not improve for 10 epochs.
Generate examples Step 154, Loss 0.3180265113230675
Epoch: 88
Negative Examples
['a', 'aaa', 'aacabcbc', 'aacbbb', 'acaaccb', 'acbca', 'b', 'bacc', 'bba', 'bcacbba', 'bcccaaa', 'cc', 'ccba', 'cccbaaab', 'cccbaacb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 88: No counterexamples found, skipped.
Accuracy at epoch 88: 0.515625, total training samples: 130
Early stopping at epoch 138. Loss did not improve for 10 epochs.
Generate examples Step 138, Loss 0.2916371730162943
Epoch: 89
Negative Examples
['ac', 'acaaabaa', 'acbcbba', 'b', 'bab', 'babbbbbc', 'bc', 'bcaac', 'c', 'cacc', 'cbbac', 'ccab', 'ccacb', 'ccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 89: No counterexamples found, skipped.
Accuracy at epoch 89: 0.509765625, total training samples: 130
Early stopping at epoch 144. Loss did not improve for 10 epochs.
Generate examples Step 144, Loss 0.3267107809412068
Epoch: 90
Negative Examples
['aa', 'aacbabbb', 'abac', 'abcac', 'acccaca', 'baab', 'baacaab', 'bacccb', 'bbaacc', 'bbac', 'c', 'cb', 'cbcaa', 'ccbbc', 'ccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abac', 'abcac']
Pos Pos
Train Epoch 0, Loss 0.726011335849762
Train Epoch 1, Loss 0.6725437641143799
Train Epoch 2, Loss 0.6211217045783997
Accuracy at epoch 90: 0.623046875, total training samples: 132
Early stopping at epoch 181. Loss did not improve for 10 epochs.
Generate examples Step 181, Loss 0.27593368275479957
Epoch: 91
Negative Examples
['aabb', 'abbabacb', 'acb', 'b', 'bbbccb', 'cabcb', 'cca', 'ccacbabb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['a', 'bcbb', 'bcca', 'cc', 'ccc', 'cccbabc']
Pos Pos Pos Pos Pos Pos
Counterexamples
['a', 'bcbb', 'bcca', 'cc', 'ccc', 'cccbabc']
Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7169792056083679
Train Epoch 1, Loss 0.666166365146637
Train Epoch 2, Loss 0.6205883026123047
Accuracy at epoch 91: 0.537109375, total training samples: 138
Early stopping at epoch 167. Loss did not improve for 10 epochs.
Generate examples Step 167, Loss 0.30509784675779794
Epoch: 92
Negative Examples
['aab', 'aabc', 'abbcaab', 'abccab', 'acbab', 'baaabbbc', 'bc', 'bcabc', 'c', 'cac', 'cbca', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['caaac', 'cbbbcbaa', 'cbbc']
Pos Pos Pos
Counterexamples
['caaac', 'cbbbcbaa', 'cbbc']
Neg Neg Neg
Train Epoch 0, Loss 0.7040885090827942
Train Epoch 1, Loss 0.6515233516693115
Train Epoch 2, Loss 0.6026839017868042
Accuracy at epoch 92: 0.51171875, total training samples: 141
Early stopping at epoch 153. Loss did not improve for 10 epochs.
Generate examples Step 153, Loss 0.299102374872604
Epoch: 93
Negative Examples
['a', 'aa', 'abcccaa', 'acaa', 'b', 'bac', 'bbacabbc', 'bc', 'c', 'cab', 'cacbccbc', 'cbaccaaa', 'cbbbaccb', 'cbcc', 'ccaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 93: No counterexamples found, skipped.
Accuracy at epoch 93: 0.515625, total training samples: 141
Early stopping at epoch 161. Loss did not improve for 10 epochs.
Generate examples Step 161, Loss 0.3488293573444272
Epoch: 94
Negative Examples
['a', 'aabccaa', 'abbbca', 'ac', 'acbcbca', 'accabac', 'ba', 'bacbca', 'ca', 'cac', 'cbaccccc', 'cc', 'ccbbcc', 'cccaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 94: No counterexamples found, skipped.
Accuracy at epoch 94: 0.466796875, total training samples: 141
Early stopping at epoch 154. Loss did not improve for 10 epochs.
Generate examples Step 154, Loss 0.31876607268087326
Epoch: 95
Negative Examples
['aa', 'aaabb', 'aac', 'aacaacb', 'aacaca', 'abbaabcc', 'abca', 'acbab', 'bbbbc', 'bbbcbac', 'bcaaaa', 'bcba', 'ca', 'cac', 'cbcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 95: No counterexamples found, skipped.
Accuracy at epoch 95: 0.50390625, total training samples: 141
Early stopping at epoch 149. Loss did not improve for 10 epochs.
Generate examples Step 149, Loss 0.3102026031414668
Epoch: 96
Negative Examples
['a', 'aacca', 'ab', 'abababcb', 'accbbb', 'b', 'ba', 'babacacc', 'babc', 'bc', 'bcccb', 'bcccbaa', 'cbaaa', 'ccabc', 'ccb', 'cccaabcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 96: No counterexamples found, skipped.
Accuracy at epoch 96: 0.48828125, total training samples: 141
Early stopping at epoch 169. Loss did not improve for 10 epochs.
Generate examples Step 169, Loss 0.31966085293713736
Epoch: 97
Negative Examples
['aaaabb', 'aaabaac', 'aca', 'b', 'baacccac', 'bba', 'bcccaacb', 'ca', 'cacbb', 'cacbc', 'cbb', 'cbba', 'cc', 'cca', 'ccbacab', 'ccbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baacccac']
Pos
Train Epoch 0, Loss 0.8446378707885742
Train Epoch 1, Loss 0.7783799171447754
Train Epoch 2, Loss 0.7165749073028564
Accuracy at epoch 97: 0.5859375, total training samples: 142
Early stopping at epoch 200. Loss did not improve for 10 epochs.
Generate examples Step 200, Loss 0.2797088529636611
Epoch: 98
Negative Examples
['a', 'aaaba', 'aaababba', 'abbb', 'abca', 'acc', 'accc', 'b', 'baabb', 'bacaccc', 'bbbbabcc', 'bbcbbcb', 'bcabbba', 'bcc', 'c', 'ccabbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 98: No counterexamples found, skipped.
Accuracy at epoch 98: 0.56640625, total training samples: 142
Early stopping at epoch 176. Loss did not improve for 10 epochs.
Generate examples Step 176, Loss 0.2587658847119175
Epoch: 99
Negative Examples
['a', 'aab', 'aabb', 'ba', 'baa', 'bb', 'bbc', 'bcabccc', 'bcb', 'bcbbccc', 'ca', 'cbcba', 'cc', 'ccabbbc', 'ccbbabb', 'ccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 99: No counterexamples found, skipped.
Accuracy at epoch 99: 0.5859375, total training samples: 142
Early stopping at epoch 143. Loss did not improve for 10 epochs.
Generate examples Step 143, Loss 0.2767926589068439
Epoch: 100
Negative Examples
['a', 'aabbcb', 'aabc', 'aabccbb', 'aac', 'b', 'ba', 'baa', 'bbbcb', 'bcbacaba', 'caabaca', 'cbccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 100: No counterexamples found, skipped.
Accuracy at epoch 100: 0.60546875, total training samples: 142
Early stopping at epoch 202. Loss did not improve for 10 epochs.
Generate examples Step 202, Loss 0.34659569237032545
Epoch: 101
Negative Examples
['aabbb', 'accacaa', 'bab', 'bababcab', 'bc', 'bccbbcab', 'bcccbbc', 'caa', 'caaccacc', 'cba', 'cbbaa', 'cbc', 'cbcca', 'cca', 'ccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 101: No counterexamples found, skipped.
Accuracy at epoch 101: 0.56640625, total training samples: 142
Early stopping at epoch 164. Loss did not improve for 10 epochs.
Generate examples Step 164, Loss 0.28558636036786167
Epoch: 102
Negative Examples
['acaabc', 'accba', 'b', 'ba', 'baca', 'bccc', 'c', 'ca', 'caabacc', 'cabbc', 'cacaabc', 'cbbb', 'cbcc', 'ccacc', 'ccbabcb', 'cccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 102: No counterexamples found, skipped.
Accuracy at epoch 102: 0.55859375, total training samples: 142
Early stopping at epoch 180. Loss did not improve for 10 epochs.
Generate examples Step 180, Loss 0.27233033614922625
Epoch: 103
Negative Examples
['a', 'aabcbcb', 'ab', 'ababbbb', 'abc', 'acaacc', 'acca', 'b', 'baaba', 'baacb', 'babcc', 'bccb', 'cb', 'cc', 'ccacaacb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaba']
Pos
Train Epoch 0, Loss 0.8067276477813721
Train Epoch 1, Loss 0.7466025948524475
Train Epoch 2, Loss 0.6931681036949158
Accuracy at epoch 103: 0.689453125, total training samples: 143
Early stopping at epoch 138. Loss did not improve for 10 epochs.
Generate examples Step 138, Loss 0.2897345307919619
Epoch: 104
Negative Examples
['aaa', 'aab', 'abaacbcb', 'aca', 'cabb', 'caca', 'cb', 'ccbbcb']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['a', 'abbbbac', 'ba', 'bbbcc', 'bc', 'c', 'cbbbb']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['a', 'abbbbac', 'ba', 'bbbcc', 'bc', 'c', 'cbbbb']
Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7191332578659058
Train Epoch 1, Loss 0.6756182909011841
Train Epoch 2, Loss 0.6343730092048645
Accuracy at epoch 104: 0.498046875, total training samples: 150
Early stopping at epoch 170. Loss did not improve for 10 epochs.
Generate examples Step 170, Loss 0.2898395505913517
Epoch: 105
Negative Examples
['aaaba', 'aaacaa', 'acaaaaca', 'accca', 'babaab', 'babbab', 'bc', 'bca', 'c', 'caabbbbc', 'cabcbaab', 'cbabb', 'cbaca', 'cbcccbbc', 'cc', 'cccaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 105: No counterexamples found, skipped.
Accuracy at epoch 105: 0.5, total training samples: 150
Early stopping at epoch 231. Loss did not improve for 10 epochs.
Generate examples Step 231, Loss 0.4135681604003084
Epoch: 106
Negative Examples
['aacbccaa', 'abccba', 'acabbbc', 'ba', 'bacaccaa', 'bcabcba', 'bcbacbc', 'bccaacba', 'caacbac', 'caba', 'cacabba', 'caccba', 'cbabbabc', 'cbbcbc', 'cbcbabaa', 'ccccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abccba']
Pos
Train Epoch 0, Loss 0.7403950691223145
Train Epoch 1, Loss 0.680778443813324
Train Epoch 2, Loss 0.6257422566413879
Accuracy at epoch 106: 0.71484375, total training samples: 151
Early stopping at epoch 166. Loss did not improve for 10 epochs.
Generate examples Step 166, Loss 0.29395043903482176
Epoch: 107
Negative Examples
['abbcabbb', 'ac', 'acabcb', 'b', 'baacbcca', 'babcc', 'bcabbbcb', 'bccc', 'c', 'caaabbb', 'ccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abbbc', 'ba', 'bcbcac']
Pos Pos Pos
Counterexamples
['abbbc', 'ba', 'bcbcac']
Neg Neg Neg
Train Epoch 0, Loss 0.7038097381591797
Train Epoch 1, Loss 0.6606989502906799
Train Epoch 2, Loss 0.61912602186203
Accuracy at epoch 107: 0.57421875, total training samples: 154
Early stopping at epoch 219. Loss did not improve for 10 epochs.
Generate examples Step 219, Loss 0.36909429105845365
Epoch: 108
Negative Examples
['aaa', 'aaaabca', 'abbbcbc', 'baacbcba', 'bacacbba', 'bba', 'bcacacac', 'bcbac', 'caa', 'caba', 'cabbba', 'cbaacac', 'cbbbcca', 'cbccbc', 'ccaabaa', 'ccacbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 108: No counterexamples found, skipped.
Accuracy at epoch 108: 0.494140625, total training samples: 154
Early stopping at epoch 154. Loss did not improve for 10 epochs.
Generate examples Step 154, Loss 0.2781324803829193
Epoch: 109
Negative Examples
['aacb', 'abbc', 'acacbcab', 'acb', 'ba', 'bbb', 'bbbb', 'bbccbaba', 'bcb', 'ca', 'caaba', 'caacc', 'cb', 'cbbba', 'cbbcbc', 'cccbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 109: No counterexamples found, skipped.
Accuracy at epoch 109: 0.55859375, total training samples: 154
Early stopping at epoch 212. Loss did not improve for 10 epochs.
Generate examples Step 212, Loss 0.2870193486202491
Epoch: 110
Negative Examples
['aabcab', 'abbabbaa', 'acababa', 'acbbaaba', 'baabacc', 'bac', 'bbbbcca', 'bbca', 'bccc', 'caaacaca', 'cbbbcbb', 'cbc', 'cbca', 'ccaabac', 'ccaacbb', 'ccaccbca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 110: No counterexamples found, skipped.
Accuracy at epoch 110: 0.564453125, total training samples: 154
Early stopping at epoch 146. Loss did not improve for 10 epochs.
Generate examples Step 146, Loss 0.293623811330925
Epoch: 111
Negative Examples
['aaa', 'aababbbb', 'ab', 'abcab', 'b', 'bab', 'babbaba', 'bbc', 'bcb', 'c', 'cbaab', 'cbccb', 'ccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['cbcaccba']
Pos
Counterexamples
['babbaba', 'cbcaccba']
Pos Neg
Train Epoch 0, Loss 0.731751561164856
Train Epoch 1, Loss 0.7069560289382935
Train Epoch 2, Loss 0.6860086917877197
Accuracy at epoch 111: 0.52734375, total training samples: 156
Early stopping at epoch 151. Loss did not improve for 10 epochs.
Generate examples Step 151, Loss 0.31502858647390414
Epoch: 112
Negative Examples
['aaab', 'aabcbbcc', 'abab', 'abbaac', 'ac', 'acaaa', 'acbaac', 'b', 'babbab', 'bacabcbb', 'bb', 'bba', 'bbbb', 'caaccccb', 'cacacbaa', 'ccac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 112: No counterexamples found, skipped.
Accuracy at epoch 112: 0.576171875, total training samples: 156
Early stopping at epoch 170. Loss did not improve for 10 epochs.
Generate examples Step 170, Loss 0.2849524324400383
Epoch: 113
Negative Examples
['a', 'aaaacaaa', 'ab', 'abac', 'acbbbb', 'accc', 'b', 'baabab', 'babbabbb', 'baca', 'bbbaacbc', 'ca', 'cb', 'ccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ccba']
Pos
Counterexamples
['abac', 'ccba']
Pos Neg
Train Epoch 0, Loss 0.7436705827713013
Train Epoch 1, Loss 0.7148333191871643
Train Epoch 2, Loss 0.6877737641334534
Accuracy at epoch 113: 0.57421875, total training samples: 158
Early stopping at epoch 146. Loss did not improve for 10 epochs.
Generate examples Step 146, Loss 0.3460766839332321
Epoch: 114
Negative Examples
['a', 'abab', 'ac', 'b', 'baa', 'babcb', 'baccc', 'bbbacccb', 'bbbbc', 'bccb', 'cabbaa', 'cac', 'cbbcbcb', 'ccaab', 'ccaccacb', 'ccbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 114: No counterexamples found, skipped.
Accuracy at epoch 114: 0.583984375, total training samples: 158
Early stopping at epoch 162. Loss did not improve for 10 epochs.
Generate examples Step 162, Loss 0.29874828384697805
Epoch: 115
Negative Examples
['aaacca', 'abacba', 'acc', 'b', 'ba', 'bac', 'bbaabcc', 'bbb', 'bbbbbbcc', 'bcbabbb', 'c', 'cabcbcac', 'cbcabcbb', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abacba']
Pos
Train Epoch 0, Loss 0.7009956240653992
Train Epoch 1, Loss 0.6347618699073792
Train Epoch 2, Loss 0.5707010626792908
Accuracy at epoch 115: 0.59765625, total training samples: 159
Early stopping at epoch 171. Loss did not improve for 10 epochs.
Generate examples Step 171, Loss 0.303057691558849
Epoch: 116
Negative Examples
['accabbca', 'b', 'bababb', 'bbbabbca', 'bbbaca', 'bcb', 'bcc', 'ca']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['a', 'aaccccbc', 'accb', 'accbbba', 'bacbcba', 'bc', 'c']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['a', 'aaccccbc', 'accb', 'accbbba', 'bacbcba', 'bc', 'c']
Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7345042824745178
Train Epoch 1, Loss 0.688213050365448
Train Epoch 2, Loss 0.6452293395996094
Accuracy at epoch 116: 0.5625, total training samples: 166
Early stopping at epoch 163. Loss did not improve for 10 epochs.
Generate examples Step 163, Loss 0.3104644944755042
Epoch: 117
Negative Examples
['aa', 'aaba', 'aabbcab', 'ab', 'abb', 'abc', 'baa', 'bacb', 'bbbaabba', 'bcaccba', 'c', 'cbabaacb', 'cbc', 'cbccbba', 'ccabaabb', 'ccacaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 117: No counterexamples found, skipped.
Accuracy at epoch 117: 0.580078125, total training samples: 166
Early stopping at epoch 142. Loss did not improve for 10 epochs.
Generate examples Step 142, Loss 0.31832572838643214
Epoch: 118
Negative Examples
['a', 'aaacbac', 'aabb', 'abbacba', 'abcb', 'aca', 'accac', 'b', 'bb', 'bbbbbaaa', 'cabacaaa', 'cac', 'cacb', 'cb', 'cbbabcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 118: No counterexamples found, skipped.
Accuracy at epoch 118: 0.64453125, total training samples: 166
Early stopping at epoch 179. Loss did not improve for 10 epochs.
Generate examples Step 179, Loss 0.33935844401518506
Epoch: 119
Negative Examples
['aaba', 'aac', 'abacbca', 'abb', 'abcabb', 'bbacabbb', 'bbbcacb', 'bca', 'bccbbb', 'ca', 'caabcacc', 'caacacc', 'cbbcabc', 'ccbb', 'cccab', 'cccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 119: No counterexamples found, skipped.
Accuracy at epoch 119: 0.609375, total training samples: 166
Early stopping at epoch 172. Loss did not improve for 10 epochs.
Generate examples Step 172, Loss 0.3224976198866188
Epoch: 120
Negative Examples
['aabcaaa', 'abcaab', 'acca', 'acccacba', 'b', 'babb', 'bacbaac', 'bcacbbcb', 'bcccc', 'cabca', 'cbaaccca', 'cbc', 'cbcaccaa', 'cbcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abaa']
Pos
Counterexamples
['abaa']
Neg
Train Epoch 0, Loss 0.7151299118995667
Train Epoch 1, Loss 0.6503879427909851
Train Epoch 2, Loss 0.5917831659317017
Accuracy at epoch 120: 0.517578125, total training samples: 167
Early stopping at epoch 152. Loss did not improve for 10 epochs.
Generate examples Step 152, Loss 0.3085862298806508
Epoch: 121
Negative Examples
['a', 'abbbca', 'ac', 'b', 'ba', 'bb', 'bbabaa', 'bc', 'bcaac', 'bcbcbaab', 'bcccaab', 'c', 'cac', 'cacc', 'ccbbc', 'ccccbbca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 121: No counterexamples found, skipped.
Accuracy at epoch 121: 0.50390625, total training samples: 167
Early stopping at epoch 181. Loss did not improve for 10 epochs.
Generate examples Step 181, Loss 0.447036339358969
Epoch: 122
Negative Examples
['a', 'aa', 'aabbaaa', 'aac', 'accbccbc', 'acccbca', 'bacc', 'baccbbc', 'baccca', 'bcbbbaa', 'c', 'caaa', 'caca', 'cacbba', 'cbbbaaca', 'ccaabbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 122: No counterexamples found, skipped.
Accuracy at epoch 122: 0.5, total training samples: 167
Early stopping at epoch 149. Loss did not improve for 10 epochs.
Generate examples Step 149, Loss 0.3463402146100998
Epoch: 123
Negative Examples
['a', 'aaaaa', 'aac', 'b', 'bacac', 'bacccba', 'bbccbbb', 'bcccb', 'bcccbcc', 'bcccccaa', 'cbababac', 'ccbccbcb', 'ccc', 'cccccab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacac', 'bacccba']
Pos Pos
Train Epoch 0, Loss 0.8259903192520142
Train Epoch 1, Loss 0.7595611810684204
Train Epoch 2, Loss 0.6991573572158813
Accuracy at epoch 123: 0.576171875, total training samples: 169
Early stopping at epoch 155. Loss did not improve for 10 epochs.
Generate examples Step 155, Loss 0.3234653889368742
Epoch: 124
Negative Examples
['abbcc', 'acbbcb', 'accc', 'b', 'baacbcc', 'bcccacc', 'c', 'ca', 'cabcac', 'cac', 'cba', 'cbaacb', 'cbabcab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bcaaa']
Pos
Counterexamples
['bcaaa']
Neg
Train Epoch 0, Loss 0.6938619017601013
Train Epoch 1, Loss 0.6373956203460693
Train Epoch 2, Loss 0.5847156047821045
Accuracy at epoch 124: 0.515625, total training samples: 170
Early stopping at epoch 167. Loss did not improve for 10 epochs.
Generate examples Step 167, Loss 0.3300424778745288
Epoch: 125
Negative Examples
['aaacb', 'abaabb', 'b', 'ba', 'babc', 'bbbbcbaa', 'bcccbaac', 'bcccccc', 'c', 'ca', 'caabcc', 'cacab', 'cacbc', 'ccabbac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 125: No counterexamples found, skipped.
Accuracy at epoch 125: 0.455078125, total training samples: 170
Early stopping at epoch 148. Loss did not improve for 10 epochs.
Generate examples Step 148, Loss 0.29648356249668456
Epoch: 126
Negative Examples
['a', 'aababb', 'abbacbb', 'acb', 'acbabb', 'b', 'bacac', 'bbaacba', 'bcacb', 'cab', 'cacbb', 'caccc', 'cb', 'cbb', 'cccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacac']
Pos
Train Epoch 0, Loss 0.7539935111999512
Train Epoch 1, Loss 0.6757690906524658
Train Epoch 2, Loss 0.600298285484314
Accuracy at epoch 126: 0.6015625, total training samples: 171
Early stopping at epoch 200. Loss did not improve for 10 epochs.
Generate examples Step 200, Loss 0.30956453025637576
Epoch: 127
Negative Examples
['abbaab', 'ba', 'baaa', 'bbbcbb', 'bbcbacc', 'bbcbbaa', 'bbcbbaab', 'bcba', 'bcbcbabc', 'c', 'ca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaacaa', 'ababba', 'abbcaa', 'acabaa', 'bccbcaa']
Pos Pos Pos Pos Pos
Counterexamples
['aaacaa', 'ababba', 'abbcaa', 'acabaa', 'bccbcaa']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7442182302474976
Train Epoch 1, Loss 0.6765772104263306
Train Epoch 2, Loss 0.6181390881538391
Accuracy at epoch 127: 0.53125, total training samples: 176
Early stopping at epoch 152. Loss did not improve for 10 epochs.
Generate examples Step 152, Loss 0.29639821952464535
Epoch: 128
Negative Examples
['aacbacac', 'aacccb', 'abccabc', 'acabbb', 'acbacacb', 'acc', 'b', 'baa', 'baacbc', 'bbbaabb', 'bcaaabac', 'ca', 'caacb', 'cb', 'ccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 128: No counterexamples found, skipped.
Accuracy at epoch 128: 0.486328125, total training samples: 176
Early stopping at epoch 161. Loss did not improve for 10 epochs.
Generate examples Step 161, Loss 0.2993675818045934
Epoch: 129
Negative Examples
['aaabc', 'aaabcb', 'aaba', 'abb', 'accabab', 'ba', 'babcb', 'bacb', 'bacc', 'bbcca', 'bcbc', 'bccaca', 'caca', 'cacbbb', 'cbcbcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 129: No counterexamples found, skipped.
Accuracy at epoch 129: 0.505859375, total training samples: 176
Early stopping at epoch 149. Loss did not improve for 10 epochs.
Generate examples Step 149, Loss 0.3235527954498927
Epoch: 130
Negative Examples
['a', 'aaac', 'abbbca', 'ba', 'baab', 'bcab', 'c', 'cabaaab', 'cabccca', 'cacbca', 'cacbcb', 'cc', 'ccabbaab', 'ccbcaa', 'ccc', 'ccccaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 130: No counterexamples found, skipped.
Accuracy at epoch 130: 0.505859375, total training samples: 176
Early stopping at epoch 151. Loss did not improve for 10 epochs.
Generate examples Step 151, Loss 0.3207120140524287
Epoch: 131
Negative Examples
['a', 'aaabaccb', 'aabcbca', 'ab', 'abbccab', 'aca', 'baab', 'bbccb', 'bca', 'caaabbc', 'caab', 'cac', 'cbaac', 'ccaaabbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 131: No counterexamples found, skipped.
Accuracy at epoch 131: 0.46875, total training samples: 176
Early stopping at epoch 159. Loss did not improve for 10 epochs.
Generate examples Step 159, Loss 0.3159091003239155
Epoch: 132
Negative Examples
['a', 'aaa', 'acacccc', 'b', 'ba', 'baaaabbb', 'baac', 'bab', 'bbbaabbb', 'bc', 'bcbacab', 'cabacca', 'ccabbabb', 'cccab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baac']
Pos
Train Epoch 0, Loss 0.883766233921051
Train Epoch 1, Loss 0.809580385684967
Train Epoch 2, Loss 0.7450480461120605
Accuracy at epoch 132: 0.5859375, total training samples: 177
Early stopping at epoch 188. Loss did not improve for 10 epochs.
Generate examples Step 188, Loss 0.3061833063130656
Epoch: 133
Negative Examples
['aa', 'abbaa', 'abbbc', 'acacbb', 'baa', 'bbbc', 'bbcb', 'bcaccba', 'bcbabcca', 'bcbb', 'bccb', 'cbaaa', 'cbabacc', 'cca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['cbabcbc']
Pos
Counterexamples
['cbabcbc']
Neg
Train Epoch 0, Loss 0.6938120722770691
Train Epoch 1, Loss 0.6340357065200806
Train Epoch 2, Loss 0.5789107084274292
Accuracy at epoch 133: 0.490234375, total training samples: 178
Early stopping at epoch 145. Loss did not improve for 10 epochs.
Generate examples Step 145, Loss 0.29748637259823
Epoch: 134
Negative Examples
['aabccbca', 'ababcacc', 'abc', 'bbb', 'bbc', 'bc', 'cabbaca', 'cac', 'cacb', 'caccbbc', 'cb', 'cbbc', 'cbc', 'cca', 'ccabccab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 134: No counterexamples found, skipped.
Accuracy at epoch 134: 0.5390625, total training samples: 178
Early stopping at epoch 159. Loss did not improve for 10 epochs.
Generate examples Step 159, Loss 0.34392891023308036
Epoch: 135
Negative Examples
['aababc', 'ab', 'abbcbcb', 'abcc', 'acab', 'acbbb', 'ba', 'baaabab', 'baca', 'bbbcccb', 'bbcc', 'ca', 'caacca', 'cabbcbb', 'cbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 135: No counterexamples found, skipped.
Accuracy at epoch 135: 0.451171875, total training samples: 178
Early stopping at epoch 145. Loss did not improve for 10 epochs.
Generate examples Step 145, Loss 0.31396606442046493
Epoch: 136
Negative Examples
['a', 'aa', 'ab', 'abab', 'abcbca', 'baabcc', 'bcac', 'bcb', 'c', 'cababcc', 'cb', 'cbbcb', 'ccbbbc', 'ccbca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 136: No counterexamples found, skipped.
Accuracy at epoch 136: 0.4921875, total training samples: 178
Early stopping at epoch 265. Loss did not improve for 10 epochs.
Generate examples Step 265, Loss 0.4233252329933912
Epoch: 137
Negative Examples
['ac', 'accaacbc', 'bac', 'bbaba', 'c', 'cabaaba', 'cababcc', 'cabbbaac', 'cabcbcaa', 'cbaaa', 'cbabaabc', 'cbbbabac', 'cbbbc', 'cbbcbbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 137: No counterexamples found, skipped.
Accuracy at epoch 137: 0.5078125, total training samples: 178
Early stopping at epoch 150. Loss did not improve for 10 epochs.
Generate examples Step 150, Loss 0.36018332542962583
Epoch: 138
Negative Examples
['a', 'acba', 'accc', 'b', 'ba', 'babcbbaa', 'bb', 'bbcaab', 'bcab', 'bcba', 'bcccacab', 'cbb', 'ccca', 'cccab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 138: No counterexamples found, skipped.
Accuracy at epoch 138: 0.513671875, total training samples: 178
Early stopping at epoch 142. Loss did not improve for 10 epochs.
Generate examples Step 142, Loss 0.3016905226073899
Epoch: 139
Negative Examples
['aaa', 'ab', 'abab', 'abbcc', 'b', 'baa', 'bacbcbb', 'baccc', 'bc', 'bcabbcbc', 'c', 'ca', 'cacb', 'cbaacbac', 'ccbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 139: No counterexamples found, skipped.
Accuracy at epoch 139: 0.49609375, total training samples: 178
Early stopping at epoch 169. Loss did not improve for 10 epochs.
Generate examples Step 169, Loss 0.37174116110100464
Epoch: 140
Negative Examples
['a', 'aabcbcbc', 'aacbac', 'ac', 'acacaaaa', 'b', 'ba', 'baacb', 'bbcbaa', 'bcb', 'c', 'caaa', 'cbcaaa', 'cbcbcab', 'ccbcbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 140: No counterexamples found, skipped.
Accuracy at epoch 140: 0.5234375, total training samples: 178
Early stopping at epoch 162. Loss did not improve for 10 epochs.
Generate examples Step 162, Loss 0.30733212627516204
Epoch: 141
Negative Examples
['abccaccc', 'acbcac', 'accaaba', 'accaabb', 'bac', 'bacb', 'baccb', 'bba', 'bbbbacc', 'bbcbbb', 'caacbc', 'cabb', 'cc', 'cca', 'ccbababc', 'ccbabca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 141: No counterexamples found, skipped.
Accuracy at epoch 141: 0.509765625, total training samples: 178
Early stopping at epoch 149. Loss did not improve for 10 epochs.
Generate examples Step 149, Loss 0.3452749087413152
Epoch: 142
Negative Examples
['aabac', 'abacbab', 'b', 'babb', 'bb', 'bc', 'bcaab', 'bcb', 'caaaaa', 'caaab', 'caacccb', 'cabba', 'cac', 'cbcaa', 'cbcbc', 'ccbcaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 142: No counterexamples found, skipped.
Accuracy at epoch 142: 0.513671875, total training samples: 178
Early stopping at epoch 151. Loss did not improve for 10 epochs.
Generate examples Step 151, Loss 0.3011985915271859
Epoch: 143
Negative Examples
['a', 'abbcb', 'acaccab', 'accbbaca', 'b', 'baacacca', 'bbaaaacb', 'bbb', 'bbbbba', 'bbcaabcc', 'c', 'caa', 'caab', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 143: No counterexamples found, skipped.
Accuracy at epoch 143: 0.515625, total training samples: 178
Early stopping at epoch 150. Loss did not improve for 10 epochs.
Generate examples Step 150, Loss 0.29126940045925165
Epoch: 144
Negative Examples
['a', 'aa', 'abbbcbca', 'b', 'bbacacc', 'bbbacac', 'bbcbbaac', 'bc', 'bccbba', 'bcccbbaa', 'caccbaa', 'cbaaaca', 'ccc', 'ccccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 144: No counterexamples found, skipped.
Accuracy at epoch 144: 0.462890625, total training samples: 178
Early stopping at epoch 145. Loss did not improve for 10 epochs.
Generate examples Step 145, Loss 0.3141625960395761
Epoch: 145
Negative Examples
['aacbcbab', 'abbabbb', 'ac', 'acb', 'b', 'babbcca', 'bac', 'bb', 'bbacc', 'bbbaa', 'bcbac', 'c', 'ca', 'ccbaacbb', 'cccbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 145: No counterexamples found, skipped.
Accuracy at epoch 145: 0.50390625, total training samples: 178
Early stopping at epoch 158. Loss did not improve for 10 epochs.
Generate examples Step 158, Loss 0.31427267156307054
Epoch: 146
Negative Examples
['aacbaabb', 'aba', 'ac', 'acaa', 'acbbacbc', 'baaa', 'baabba', 'bb', 'bbabbba', 'bbbaa', 'bc', 'bcbcc', 'c', 'cabcbba', 'cacba', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 146: No counterexamples found, skipped.
Accuracy at epoch 146: 0.4765625, total training samples: 178
Early stopping at epoch 156. Loss did not improve for 10 epochs.
Generate examples Step 156, Loss 0.3186650276184082
Epoch: 147
Negative Examples
['aaabacb', 'aacbbcac', 'abbabcb', 'abbca', 'baccccac', 'bb', 'bba', 'bcacaacb', 'bcbcbb', 'bcc', 'c', 'cac', 'cbbab', 'cbbcc', 'ccbccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baccccac']
Pos
Train Epoch 0, Loss 0.8110597133636475
Train Epoch 1, Loss 0.7405726909637451
Train Epoch 2, Loss 0.6753697991371155
Accuracy at epoch 147: 0.603515625, total training samples: 179
Early stopping at epoch 160. Loss did not improve for 10 epochs.
Generate examples Step 160, Loss 0.28974032383527815
Epoch: 148
Negative Examples
['a', 'aa', 'aaacbcaa', 'aab', 'acc', 'baacaa', 'bba', 'bbaa', 'bcbb', 'cabbcc', 'cbbbbcbc', 'cca', 'ccacbbac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baacbcac', 'cbbcac']
Pos Pos
Counterexamples
['baacbcac', 'cbbcac']
Neg Neg
Train Epoch 0, Loss 0.7495869398117065
Train Epoch 1, Loss 0.6871564388275146
Train Epoch 2, Loss 0.6316579580307007
Accuracy at epoch 148: 0.482421875, total training samples: 181
Early stopping at epoch 149. Loss did not improve for 10 epochs.
Generate examples Step 149, Loss 0.2906517020861308
Epoch: 149
Negative Examples
['a', 'aa', 'aaabbca', 'abaacca', 'aca', 'acbc', 'b', 'baaccb', 'babccbab', 'bacca', 'bbacb', 'c', 'cabc', 'cbbabcbc', 'cbbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 149: No counterexamples found, skipped.
Accuracy at epoch 149: 0.494140625, total training samples: 181
Early stopping at epoch 177. Loss did not improve for 10 epochs.
Generate examples Step 177, Loss 0.2927919483921501
Epoch: 150
Negative Examples
['a', 'aaab', 'b', 'baabbbbc', 'bb', 'bbaacba', 'bcc', 'bccc', 'c', 'caacaa', 'cbbaaca', 'cbbbabbb', 'cbcbaab', 'cccaccba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 150: No counterexamples found, skipped.
Accuracy at epoch 150: 0.45703125, total training samples: 181
Early stopping at epoch 206. Loss did not improve for 10 epochs.
Generate examples Step 206, Loss 0.37351636480594025
Epoch: 151
Negative Examples
['aa', 'aabac', 'ab', 'abb', 'abbaba', 'bba', 'bbb', 'bcabac', 'bcbaab', 'bccbc', 'cacbbaac', 'cbacbbc', 'cbccab', 'ccba', 'ccbccccb', 'cccbccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 151: No counterexamples found, skipped.
Accuracy at epoch 151: 0.517578125, total training samples: 181
Early stopping at epoch 149. Loss did not improve for 10 epochs.
Generate examples Step 149, Loss 0.35981855352719627
Epoch: 152
Negative Examples
['a', 'aa', 'aabb', 'abbccacb', 'ac', 'aca', 'bacc', 'baccb', 'bbbbcbcc', 'bccbbbb', 'ca', 'cbabaa', 'cbcccccc', 'cc', 'ccbacc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 152: No counterexamples found, skipped.
Accuracy at epoch 152: 0.515625, total training samples: 181
Early stopping at epoch 155. Loss did not improve for 10 epochs.
Generate examples Step 155, Loss 0.29022968961642337
Epoch: 153
Negative Examples
['a', 'acbaab', 'b', 'baabaac', 'babbcaa', 'bacab', 'bb', 'bbaa', 'caaab', 'cabbcb', 'cbaac', 'cbb', 'ccaacaac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 153: No counterexamples found, skipped.
Accuracy at epoch 153: 0.48046875, total training samples: 181
Early stopping at epoch 148. Loss did not improve for 10 epochs.
Generate examples Step 148, Loss 0.36453029573363743
Epoch: 154
Negative Examples
['a', 'abba', 'accaba', 'b', 'babca', 'bb', 'bba', 'bbbbcaaa', 'bbca', 'bc', 'bcacbba', 'bcbc', 'caac', 'cab', 'cacc', 'cbbaca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba']
Pos
Train Epoch 0, Loss 0.8334745168685913
Train Epoch 1, Loss 0.7651358842849731
Train Epoch 2, Loss 0.7023487091064453
Accuracy at epoch 154: 0.5859375, total training samples: 182
Early stopping at epoch 175. Loss did not improve for 10 epochs.
Generate examples Step 175, Loss 0.33647729066962545
Epoch: 155
Negative Examples
['a', 'aabb', 'abaabb', 'abcbcb', 'b', 'ba', 'bbabbccb', 'bbabca', 'bbbbb', 'bcbbbca', 'c', 'cbaaa', 'cbbbbab', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaba', 'ccbbc']
Pos Pos
Counterexamples
['aaba', 'ccbbc']
Neg Neg
Train Epoch 0, Loss 0.7055352926254272
Train Epoch 1, Loss 0.6564846038818359
Train Epoch 2, Loss 0.6105226278305054
Accuracy at epoch 155: 0.5, total training samples: 184
Early stopping at epoch 143. Loss did not improve for 10 epochs.
Generate examples Step 143, Loss 0.2718013831310802
Epoch: 156
Negative Examples
['a', 'aaaabbac', 'aabc', 'acbb', 'acbbbbba', 'acc', 'b', 'bb', 'bc', 'bcbbcbc', 'bccaabc', 'c', 'cbc', 'cc', 'cca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 156: No counterexamples found, skipped.
Accuracy at epoch 156: 0.4765625, total training samples: 184
Early stopping at epoch 179. Loss did not improve for 10 epochs.
Generate examples Step 179, Loss 0.3166886293225818
Epoch: 157
Negative Examples
['a', 'aaa', 'acabcc', 'acbcbbca', 'bbbaba', 'bc', 'bcb', 'bcccba', 'caaabaaa', 'caac', 'caacbaa', 'cabbabb', 'cabccbc', 'cacca', 'cba', 'cbbbcbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 157: No counterexamples found, skipped.
Accuracy at epoch 157: 0.517578125, total training samples: 184
Early stopping at epoch 167. Loss did not improve for 10 epochs.
Generate examples Step 167, Loss 0.30382290943747475
Epoch: 158
Negative Examples
['a', 'aaacbaba', 'aac', 'abacabab', 'abbcba', 'abccaa', 'bccabcc', 'c', 'ca', 'cabab', 'cb', 'cbaaca', 'cbccbb', 'cc', 'ccabcc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 158: No counterexamples found, skipped.
Accuracy at epoch 158: 0.486328125, total training samples: 184
Early stopping at epoch 170. Loss did not improve for 10 epochs.
Generate examples Step 170, Loss 0.3249577346600984
Epoch: 159
Negative Examples
['a', 'abaa', 'acabcbcc', 'acbaba', 'acbbaaa', 'acccaa', 'babccbca', 'baccbcac', 'bbccbccc', 'bcb', 'bcc', 'c', 'cab', 'ccbc', 'cccaac', 'cccabca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 159: No counterexamples found, skipped.
Accuracy at epoch 159: 0.48828125, total training samples: 184
Early stopping at epoch 158. Loss did not improve for 10 epochs.
Generate examples Step 158, Loss 0.2896885778169212
Epoch: 160
Negative Examples
['aac', 'abcaccaa', 'b', 'ba', 'bb', 'bbb', 'bbbbccb', 'bca', 'bcacbb', 'bcbcaca', 'c', 'caacbaa', 'caba', 'ccacc', 'ccbaccc', 'ccccaac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 160: No counterexamples found, skipped.
Accuracy at epoch 160: 0.44921875, total training samples: 184
Early stopping at epoch 203. Loss did not improve for 10 epochs.
Generate examples Step 203, Loss 0.3484403007462913
Epoch: 161
Negative Examples
['a', 'aaaac', 'abacbc', 'abca', 'acaa', 'bacba', 'bb', 'bbcbab', 'bbcca', 'bcbb', 'cabc', 'cac', 'cbbba', 'ccbbcac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacba']
Pos
Train Epoch 0, Loss 0.7974529266357422
Train Epoch 1, Loss 0.7219991683959961
Train Epoch 2, Loss 0.6508405804634094
Accuracy at epoch 161: 0.623046875, total training samples: 185
Early stopping at epoch 232. Loss did not improve for 10 epochs.
Generate examples Step 232, Loss 0.2763843658528103
Epoch: 162
Negative Examples
['aab', 'aababb', 'aabcab', 'abacc', 'abbbbbba', 'abccbbcc', 'b', 'ba', 'babbacc', 'bbca', 'bcbaaa', 'c', 'caca', 'cbbab', 'cc', 'ccabcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abbbbbba']
Pos
Train Epoch 0, Loss 0.7776332497596741
Train Epoch 1, Loss 0.7185441851615906
Train Epoch 2, Loss 0.6650224924087524
Accuracy at epoch 162: 0.62109375, total training samples: 186
Early stopping at epoch 266. Loss did not improve for 10 epochs.
Generate examples Step 266, Loss 0.2794315345055155
Epoch: 163
Negative Examples
['ac', 'aca', 'acbaab', 'accbcca', 'b', 'bbabb', 'bbca', 'bcbbcb', 'cbbbcc', 'cbbccb', 'ccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bbc', 'bbcca', 'ccbbcca']
Pos Pos Pos
Counterexamples
['bbc', 'bbcca', 'ccbbcca']
Neg Neg Neg
Train Epoch 0, Loss 0.7276609539985657
Train Epoch 1, Loss 0.6816418170928955
Train Epoch 2, Loss 0.6380875110626221
Accuracy at epoch 163: 0.560546875, total training samples: 189
Early stopping at epoch 160. Loss did not improve for 10 epochs.
Generate examples Step 160, Loss 0.29993968480122013
Epoch: 164
Negative Examples
['abbaa', 'ac', 'acbcbcbc', 'babbcaac', 'bcacba', 'bcba', 'bcbbccaa', 'c', 'cababaab', 'cabbac', 'cacbb', 'cbbaa', 'cbbaca', 'cbbbca', 'cbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['babbcaac']
Pos
Train Epoch 0, Loss 0.7927069664001465
Train Epoch 1, Loss 0.7226890325546265
Train Epoch 2, Loss 0.6608200669288635
Accuracy at epoch 164: 0.625, total training samples: 190
Early stopping at epoch 229. Loss did not improve for 10 epochs.
Generate examples Step 229, Loss 0.3012749793736831
Epoch: 165
Negative Examples
['aaababa', 'aaacbcbc', 'aabb', 'abccca', 'b', 'bbabaaa', 'bccc', 'c', 'cbcaacab', 'cccbbbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bbaac', 'bbabc', 'bbbba', 'bbbbac', 'cac']
Pos Pos Pos Pos Pos
Counterexamples
['bbaac', 'bbabc', 'bbbba', 'bbbbac', 'cac']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7120611667633057
Train Epoch 1, Loss 0.6725761294364929
Train Epoch 2, Loss 0.6343070268630981
Accuracy at epoch 165: 0.55859375, total training samples: 195
Early stopping at epoch 149. Loss did not improve for 10 epochs.
Generate examples Step 149, Loss 0.28940970500310265
Epoch: 166
Negative Examples
['a', 'acba', 'acbccab', 'b', 'bacabb', 'bb', 'bbacc', 'bbbbcaa', 'c', 'cabca', 'caccbcb', 'cbacab', 'cbcbba', 'ccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 166: No counterexamples found, skipped.
Accuracy at epoch 166: 0.513671875, total training samples: 195
Early stopping at epoch 162. Loss did not improve for 10 epochs.
Generate examples Step 162, Loss 0.2704027637016554
Epoch: 167
Negative Examples
['a', 'aababbaa', 'ababca', 'abcbb', 'acc', 'accaccc', 'baaa', 'baacbc', 'bac', 'bb', 'bbabbbcb', 'bccbbba', 'cb', 'cbbbcb', 'cca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 167: No counterexamples found, skipped.
Accuracy at epoch 167: 0.53515625, total training samples: 195
Early stopping at epoch 214. Loss did not improve for 10 epochs.
Generate examples Step 214, Loss 0.27698890608410504
Epoch: 168
Negative Examples
['aab', 'abacb', 'acc', 'b', 'baabcbc', 'bacc', 'bbbbca', 'bbcbcca', 'bca', 'cabbba', 'cabbcc', 'cbabbcca', 'cbb', 'ccbcba', 'cccabbac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 168: No counterexamples found, skipped.
Accuracy at epoch 168: 0.513671875, total training samples: 195
Early stopping at epoch 185. Loss did not improve for 10 epochs.
Generate examples Step 185, Loss 0.26916998352414817
Epoch: 169
Negative Examples
['a', 'aacbbbb', 'ac', 'acc', 'b', 'babcbaba', 'bacbbaa', 'bbbbba', 'bbbcac', 'bca', 'c', 'cacbacb', 'cb', 'cba', 'cbcabac', 'cccbbacb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 169: No counterexamples found, skipped.
Accuracy at epoch 169: 0.55078125, total training samples: 195
Early stopping at epoch 151. Loss did not improve for 10 epochs.
Generate examples Step 151, Loss 0.2845028574137311
Epoch: 170
Negative Examples
['a', 'aaa', 'abbabcc', 'abbcb', 'acaaaca', 'baacb', 'babba', 'bbb', 'bcbcbacc', 'bccccbcb', 'c', 'cabcbbaa', 'cacb', 'cbaaab', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 170: No counterexamples found, skipped.
Accuracy at epoch 170: 0.5546875, total training samples: 195
Early stopping at epoch 154. Loss did not improve for 10 epochs.
Generate examples Step 154, Loss 0.2891896678555396
Epoch: 171
Negative Examples
['aabbaaaa', 'abbabab', 'abc', 'ac', 'bbabb', 'bbc', 'ca', 'cab', 'cb', 'cbbabbb', 'cbcb', 'ccbbaacb', 'ccbcca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acac']
Pos
Counterexamples
['acac']
Neg
Train Epoch 0, Loss 0.6965941190719604
Train Epoch 1, Loss 0.6398594379425049
Train Epoch 2, Loss 0.5882176756858826
Accuracy at epoch 171: 0.533203125, total training samples: 196
Early stopping at epoch 150. Loss did not improve for 10 epochs.
Generate examples Step 150, Loss 0.26611746867366187
Epoch: 172
Negative Examples
['a', 'ab', 'ababcb', 'bbccc', 'bc', 'bcabbb', 'bcbccb', 'bcc', 'bcccacbb', 'caa', 'caaccba', 'cabbbb', 'cbabbca', 'cbb', 'cbcca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 172: No counterexamples found, skipped.
Accuracy at epoch 172: 0.505859375, total training samples: 196
Early stopping at epoch 157. Loss did not improve for 10 epochs.
Generate examples Step 157, Loss 0.3244826152354856
Epoch: 173
Negative Examples
['a', 'aacbcbcc', 'abaabca', 'abacbb', 'acabb', 'acabcca', 'accbabbc', 'bcabab', 'bcacba', 'c', 'cbbcbc', 'cbc', 'cbca', 'cbcabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 173: No counterexamples found, skipped.
Accuracy at epoch 173: 0.5078125, total training samples: 196
Early stopping at epoch 154. Loss did not improve for 10 epochs.
Generate examples Step 154, Loss 0.2976119435602619
Epoch: 174
Negative Examples
['a', 'aaac', 'aaacc', 'aabaab', 'aababaa', 'abcc', 'ac', 'acac', 'bbaac', 'bbcaaccc', 'bcbcbbb', 'caba', 'cbabaaa', 'ccbabca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 174: No counterexamples found, skipped.
Accuracy at epoch 174: 0.484375, total training samples: 196
Early stopping at epoch 146. Loss did not improve for 10 epochs.
Generate examples Step 146, Loss 0.27165320272348364
Epoch: 175
Negative Examples
['a', 'aabcbc', 'ab', 'aba', 'abbbaa', 'abccbcb', 'b', 'bac', 'bbc', 'bcbbba', 'c', 'cacbcb', 'cbaabcab', 'cbabba', 'cbca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 175: No counterexamples found, skipped.
Accuracy at epoch 175: 0.515625, total training samples: 196
Early stopping at epoch 146. Loss did not improve for 10 epochs.
Generate examples Step 146, Loss 0.2856626123392663
Epoch: 176
Negative Examples
['abbbacbc', 'b', 'baac', 'bba', 'bcaba', 'c', 'caa', 'caaa', 'caaba', 'cbabcb', 'cbb', 'cbcaba', 'cbcb', 'cbcba', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baac']
Pos
Train Epoch 0, Loss 0.9242140650749207
Train Epoch 1, Loss 0.8551091551780701
Train Epoch 2, Loss 0.7941943407058716
Accuracy at epoch 176: 0.53515625, total training samples: 197
Early stopping at epoch 150. Loss did not improve for 10 epochs.
Generate examples Step 150, Loss 0.3179169545899953
Epoch: 177
Negative Examples
['a', 'aacbabca', 'abac', 'abcbaac', 'acabccc', 'accca', 'b', 'ba', 'bbbbcaca', 'bcb', 'c', 'cbcabbbc', 'ccabcaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abac']
Pos
Train Epoch 0, Loss 0.710835874080658
Train Epoch 1, Loss 0.6570322513580322
Train Epoch 2, Loss 0.6048217415809631
Accuracy at epoch 177: 0.630859375, total training samples: 198
Early stopping at epoch 162. Loss did not improve for 10 epochs.
Generate examples Step 162, Loss 0.3375801513531457
Epoch: 178
Negative Examples
['aaaab', 'aab', 'aaccbaaa', 'abcbcccb', 'abccab', 'b', 'ba', 'bccacccb', 'c', 'cccbbcac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abbabab', 'abca', 'baab', 'bcabab', 'cacbc']
Pos Pos Pos Pos Pos
Counterexamples
['abbabab', 'abca', 'baab', 'bcabab', 'cacbc']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.708807110786438
Train Epoch 1, Loss 0.6663805246353149
Train Epoch 2, Loss 0.6290721297264099
Accuracy at epoch 178: 0.513671875, total training samples: 203
Early stopping at epoch 154. Loss did not improve for 10 epochs.
Generate examples Step 154, Loss 0.2915611015212151
Epoch: 179
Negative Examples
['abacbcaa', 'abbacb', 'abbcbcac', 'abcccbb', 'acacccb', 'b', 'ba', 'baca', 'bbaabcb', 'bbacbcc', 'bccbcc', 'c', 'cacca', 'cb', 'cbbacaac', 'ccaaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 179: No counterexamples found, skipped.
Accuracy at epoch 179: 0.50390625, total training samples: 203
Early stopping at epoch 161. Loss did not improve for 10 epochs.
Generate examples Step 161, Loss 0.29822112564687375
Epoch: 180
Negative Examples
['aaab', 'aacaa', 'acba', 'baaa', 'baaaaa', 'bac', 'bacacaca', 'bc', 'bcbcc', 'bccabbbc', 'bccbbacb', 'c', 'caccabcc', 'caccba', 'ccbb', 'ccbbcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 180: No counterexamples found, skipped.
Accuracy at epoch 180: 0.50390625, total training samples: 203
Early stopping at epoch 149. Loss did not improve for 10 epochs.
Generate examples Step 149, Loss 0.278320116798083
Epoch: 181
Negative Examples
['a', 'aa', 'abc', 'aca', 'babc', 'bacb', 'bacbbb', 'bbaacbb', 'ca', 'caabbb', 'caacbaaa', 'cabac', 'cabcaca', 'cba', 'cbb', 'ccbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 181: No counterexamples found, skipped.
Accuracy at epoch 181: 0.494140625, total training samples: 203
Early stopping at epoch 156. Loss did not improve for 10 epochs.
Generate examples Step 156, Loss 0.29482444741164043
Epoch: 182
Negative Examples
['a', 'aaacabcc', 'abbaaabc', 'abc', 'ac', 'acaabaa', 'acabcca', 'acbabc', 'bbaccba', 'bc', 'bca', 'bccccb', 'cba', 'cbabc', 'ccaaa', 'ccbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 182: No counterexamples found, skipped.
Accuracy at epoch 182: 0.5390625, total training samples: 203
Early stopping at epoch 154. Loss did not improve for 10 epochs.
Generate examples Step 154, Loss 0.30713071746210896
Epoch: 183
Negative Examples
['a', 'aabcbaac', 'abacabba', 'abb', 'abccaab', 'accbabaa', 'ba', 'bab', 'babbc', 'bb', 'bccc', 'cbaabacb', 'cbacaa', 'ccb', 'cccb', 'cccba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 183: No counterexamples found, skipped.
Accuracy at epoch 183: 0.5, total training samples: 203
Early stopping at epoch 138. Loss did not improve for 10 epochs.
Generate examples Step 138, Loss 0.27924442312700287
Epoch: 184
Negative Examples
['a', 'abacaa', 'abcbb', 'b', 'bacbbacb', 'bbacc', 'bca', 'bcacb', 'c', 'cb', 'cc', 'ccaaaaca', 'ccbbabbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 184: No counterexamples found, skipped.
Accuracy at epoch 184: 0.52734375, total training samples: 203
Early stopping at epoch 153. Loss did not improve for 10 epochs.
Generate examples Step 153, Loss 0.2788605440359611
Epoch: 185
Negative Examples
['a', 'aa', 'aabcbaaa', 'aacb', 'baab', 'babcbb', 'bacacbb', 'baccaccc', 'bbcbab', 'c', 'caa', 'cac', 'cacb', 'cba', 'ccaabaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 185: No counterexamples found, skipped.
Accuracy at epoch 185: 0.51171875, total training samples: 203
Early stopping at epoch 171. Loss did not improve for 10 epochs.
Generate examples Step 171, Loss 0.38207890405211337
Epoch: 186
Negative Examples
['a', 'aa', 'aababa', 'aac', 'ba', 'baba', 'bbaba', 'bc', 'bccabbb', 'c', 'caaaac', 'cbaaa', 'cbaabc', 'ccabaa', 'ccbbbaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baba']
Pos
Train Epoch 0, Loss 0.7656662464141846
Train Epoch 1, Loss 0.7125957608222961
Train Epoch 2, Loss 0.6633891463279724
Accuracy at epoch 186: 0.625, total training samples: 204
Early stopping at epoch 164. Loss did not improve for 10 epochs.
Generate examples Step 164, Loss 0.31981769800186155
Epoch: 187
Negative Examples
['a', 'abb', 'acacac', 'bbabab', 'bbabcba', 'bbccaa', 'bcab', 'cbabab', 'cbbacaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaba', 'aac', 'baaaa', 'bbbb', 'bcba', 'cbacacac']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aaba', 'aac', 'baaaa', 'bbbb', 'bcba', 'cbacacac']
Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7282090783119202
Train Epoch 1, Loss 0.6885213255882263
Train Epoch 2, Loss 0.6509889960289001
Accuracy at epoch 187: 0.486328125, total training samples: 210
Early stopping at epoch 141. Loss did not improve for 10 epochs.
Generate examples Step 141, Loss 0.2965227566974264
Epoch: 188
Negative Examples
['a', 'aa', 'aaacbcc', 'ab', 'b', 'bbbbab', 'cab', 'cacaab', 'cb', 'cbbbb', 'cbbcbbb', 'cbcbac', 'ccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 188: No counterexamples found, skipped.
Accuracy at epoch 188: 0.5078125, total training samples: 210
Early stopping at epoch 170. Loss did not improve for 10 epochs.
Generate examples Step 170, Loss 0.30816968996622407
Epoch: 189
Negative Examples
['abaabbc', 'abaaccca', 'abababbc', 'abcca', 'acaababa', 'accacc', 'babbc', 'bacaaab', 'bacabbca', 'bbacaaca', 'bbcabb', 'bc', 'bccacbab', 'cbacb', 'cbc', 'ccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 189: No counterexamples found, skipped.
Accuracy at epoch 189: 0.490234375, total training samples: 210
Early stopping at epoch 155. Loss did not improve for 10 epochs.
Generate examples Step 155, Loss 0.3108050626439926
Epoch: 190
Negative Examples
['aaaccc', 'ba', 'baabbcac', 'bacbb', 'bacccac', 'bbaa', 'bbbbbbbb', 'bbbc', 'bccbaa', 'ca', 'cacbcb', 'cbc', 'cc', 'ccaac', 'cccac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baabbcac', 'bacccac']
Pos Pos
Train Epoch 0, Loss 0.78202223777771
Train Epoch 1, Loss 0.728313684463501
Train Epoch 2, Loss 0.6793763637542725
Accuracy at epoch 190: 0.654296875, total training samples: 212
Early stopping at epoch 150. Loss did not improve for 10 epochs.
Generate examples Step 150, Loss 0.2866688747674424
Epoch: 191
Negative Examples
['a', 'aa', 'aaaaaa', 'aabba', 'ac', 'b', 'baca', 'bbcbca', 'bc', 'cabbcbb', 'cabca', 'cb', 'cba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 191: No counterexamples found, skipped.
Accuracy at epoch 191: 0.64453125, total training samples: 212
Early stopping at epoch 207. Loss did not improve for 10 epochs.
Generate examples Step 207, Loss 0.3023882559858836
Epoch: 192
Negative Examples
['abb', 'abcbbba', 'acb', 'accb', 'acccacbb', 'b', 'ba', 'baacb', 'bababbbb', 'baca', 'bbbaabca', 'bcaabb', 'bcbbab', 'caabb', 'caacbb', 'ccbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abcbbba']
Pos
Train Epoch 0, Loss 0.7294204235076904
Train Epoch 1, Loss 0.6808210015296936
Train Epoch 2, Loss 0.6352238655090332
Accuracy at epoch 192: 0.669921875, total training samples: 213
Early stopping at epoch 173. Loss did not improve for 10 epochs.
Generate examples Step 173, Loss 0.2996569621837002
Epoch: 193
Negative Examples
['aaacbc', 'abacaa', 'abcb', 'acbbbb', 'bcaccb', 'bccbbbcc', 'cbbcbcb', 'cca']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaa', 'aaaa', 'aaaaba', 'abbbcbc', 'ac', 'acaa', 'bbccba', 'cbab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaa', 'aaaa', 'aaaaba', 'abbbcbc', 'ac', 'acaa', 'bbccba', 'cbab']
Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.754885196685791
Train Epoch 1, Loss 0.7171406149864197
Train Epoch 2, Loss 0.6805277466773987
Accuracy at epoch 193: 0.58984375, total training samples: 221
Early stopping at epoch 188. Loss did not improve for 10 epochs.
Generate examples Step 188, Loss 0.30162361484986766
Epoch: 194
Negative Examples
['aa', 'aacbcaba', 'abbccab', 'abcbcba', 'accaab', 'b', 'baaaaac', 'baaba', 'babbbb', 'bacbaaca', 'bb', 'c', 'cbcbb', 'ccbcba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ccac']
Pos
Counterexamples
['baaaaac', 'baaba', 'ccac']
Pos Pos Neg
Train Epoch 0, Loss 0.7233238816261292
Train Epoch 1, Loss 0.6979970335960388
Train Epoch 2, Loss 0.6752707362174988
Accuracy at epoch 194: 0.642578125, total training samples: 224
Early stopping at epoch 154. Loss did not improve for 10 epochs.
Generate examples Step 154, Loss 0.31196551611346585
Epoch: 195
Negative Examples
['aaccbbab', 'ab', 'abb', 'acbab', 'acbba', 'baacaba', 'bb', 'bbabcbba', 'bbc', 'bbca', 'bcacab', 'cb', 'cbab', 'ccaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabbc', 'cbbbc']
Pos Pos
Counterexamples
['baacaba', 'aabbc', 'cbbbc']
Pos Neg Neg
Train Epoch 0, Loss 0.7426913380622864
Train Epoch 1, Loss 0.7125213742256165
Train Epoch 2, Loss 0.6871540546417236
Accuracy at epoch 195: 0.65234375, total training samples: 227
Early stopping at epoch 152. Loss did not improve for 10 epochs.
Generate examples Step 152, Loss 0.3063105301919326
Epoch: 196
Negative Examples
['a', 'aaacccb', 'aacbbcac', 'abb', 'b', 'bbabb', 'bbabcccb', 'bbc', 'bcac', 'ca', 'cabbacac', 'cacc', 'cbabacb', 'ccaaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abbaba']
Pos
Counterexamples
['abbaba']
Neg
Train Epoch 0, Loss 0.733177900314331
Train Epoch 1, Loss 0.6810716986656189
Train Epoch 2, Loss 0.6337428689002991
Accuracy at epoch 196: 0.494140625, total training samples: 228
Early stopping at epoch 155. Loss did not improve for 10 epochs.
Generate examples Step 155, Loss 0.36545355503375715
Epoch: 197
Negative Examples
['a', 'acc', 'accbca', 'baa', 'bbaa', 'bc', 'bca', 'ca', 'caaca', 'cabcbaba', 'cacbaa', 'cbaaca', 'cbbcbba', 'ccabab', 'ccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 197: No counterexamples found, skipped.
Accuracy at epoch 197: 0.51171875, total training samples: 228
Early stopping at epoch 148. Loss did not improve for 10 epochs.
Generate examples Step 148, Loss 0.31301057898758244
Epoch: 198
Negative Examples
['a', 'aaa', 'aaaa', 'acabbac', 'acbbcbca', 'acbccaa', 'accc', 'b', 'ba', 'baccbac', 'bbcbbcaa', 'c', 'cabcca', 'cac', 'cccaac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 198: No counterexamples found, skipped.
Accuracy at epoch 198: 0.46875, total training samples: 228
Early stopping at epoch 152. Loss did not improve for 10 epochs.
Generate examples Step 152, Loss 0.3210219255849427
Epoch: 199
Negative Examples
['a', 'aaaaa', 'aaacaba', 'aacaabcc', 'aaccaab', 'acbaabbc', 'ba', 'bbb', 'bbbbcba', 'bbcbab', 'cab', 'cabaac', 'cbccaaa', 'cbccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 199: No counterexamples found, skipped.
Accuracy at epoch 199: 0.482421875, total training samples: 228
Early stopping at epoch 177. Loss did not improve for 10 epochs.
Generate examples Step 177, Loss 0.32487091137452073
Epoch: 200
Negative Examples
['aabbaab', 'aacbaac', 'abacc', 'abbccbbc', 'ac', 'acca', 'accaaab', 'bababbcc', 'babbbcb', 'bacbab', 'bbca', 'bcbabb', 'bccb', 'caab', 'cbaaaa', 'cbcccaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 200: No counterexamples found, skipped.
Accuracy at epoch 200: 0.4765625, total training samples: 228
Early stopping at epoch 159. Loss did not improve for 10 epochs.
Generate examples Step 159, Loss 0.31601041816174985
Epoch: 201
Negative Examples
['a', 'ab', 'aba', 'abbacc', 'b', 'bacbc', 'bbbccbbc', 'caabca', 'caacab', 'cabbbcbb', 'cacbca', 'cbaabbcb', 'cc', 'ccab', 'cccbc', 'ccccbac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 201: No counterexamples found, skipped.
Accuracy at epoch 201: 0.498046875, total training samples: 228
Early stopping at epoch 148. Loss did not improve for 10 epochs.
Generate examples Step 148, Loss 0.3488347048327427
Epoch: 202
Negative Examples
['a', 'aabcacac', 'aaccaaab', 'abcc', 'b', 'baa', 'baabb', 'bac', 'bb', 'bbbcb', 'bcbbcc', 'cababa', 'cb', 'ccaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 202: No counterexamples found, skipped.
Accuracy at epoch 202: 0.50390625, total training samples: 228
Early stopping at epoch 143. Loss did not improve for 10 epochs.
Generate examples Step 143, Loss 0.3829443274686734
Epoch: 203
Negative Examples
['aaabaa', 'b', 'baa', 'baab', 'bac', 'baca', 'bbb', 'bbbcb', 'bbbccbc', 'bc', 'bca', 'bcaccbac', 'c', 'cbab', 'ccbbcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 203: No counterexamples found, skipped.
Accuracy at epoch 203: 0.49609375, total training samples: 228
Early stopping at epoch 136. Loss did not improve for 10 epochs.
Generate examples Step 136, Loss 0.33246971568922057
Epoch: 204
Negative Examples
['aac', 'acaabbc', 'accacbc', 'b', 'bcaaba', 'bcb', 'bcbaa', 'c', 'cabb', 'cabbc', 'caccc', 'cbcbbac', 'cbcc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 204: No counterexamples found, skipped.
Accuracy at epoch 204: 0.51171875, total training samples: 228
Early stopping at epoch 158. Loss did not improve for 10 epochs.
Generate examples Step 158, Loss 0.3547324367664145
Epoch: 205
Negative Examples
['abcbccc', 'ac', 'baaacba', 'bababa', 'babb', 'bacbcbb', 'bbb', 'bcbab', 'c', 'caa', 'caaccc', 'cbacc', 'cbcacaca', 'cbcbba', 'cbcc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaacba']
Pos
Train Epoch 0, Loss 0.8497626781463623
Train Epoch 1, Loss 0.7861132621765137
Train Epoch 2, Loss 0.7321468591690063
Accuracy at epoch 205: 0.63671875, total training samples: 229
Early stopping at epoch 136. Loss did not improve for 10 epochs.
Generate examples Step 136, Loss 0.30766336739498334
Epoch: 206
Negative Examples
['a', 'ab', 'abcbbccc', 'ac', 'acacac', 'b', 'baaa', 'bbbbcba', 'bcbccc', 'cac', 'cbcca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['caaa', 'cbaa']
Pos Pos
Counterexamples
['caaa', 'cbaa']
Neg Neg
Train Epoch 0, Loss 0.7028290033340454
Train Epoch 1, Loss 0.6603174209594727
Train Epoch 2, Loss 0.6195464134216309
Accuracy at epoch 206: 0.521484375, total training samples: 231
Early stopping at epoch 147. Loss did not improve for 10 epochs.
Generate examples Step 147, Loss 0.3241148830668346
Epoch: 207
Negative Examples
['a', 'aa', 'ab', 'aba', 'aca', 'acb', 'acbbbab', 'accabb', 'accccccc', 'bcbccca', 'bccca', 'cac', 'cb', 'cbacbbba', 'ccbac', 'cccaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 207: No counterexamples found, skipped.
Accuracy at epoch 207: 0.509765625, total training samples: 231
Early stopping at epoch 159. Loss did not improve for 10 epochs.
Generate examples Step 159, Loss 0.338062347099185
Epoch: 208
Negative Examples
['aaa', 'aaaa', 'abacbcbc', 'abbbcaa', 'acbccbac', 'acc', 'baa', 'bbcbbcc', 'bcaaa', 'bccc', 'caaaca', 'cacc', 'cbbaa', 'cbca', 'cbcbac', 'ccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 208: No counterexamples found, skipped.
Accuracy at epoch 208: 0.509765625, total training samples: 231
Early stopping at epoch 159. Loss did not improve for 10 epochs.
Generate examples Step 159, Loss 0.3259919982403517
Epoch: 209
Negative Examples
['a', 'acbb', 'ba', 'bacac', 'bb', 'bbbba', 'bbbbcc', 'bbcbb', 'bcaac', 'bccac', 'c', 'cac', 'cbcac', 'ccbbbaa', 'ccbcbcba', 'cccaaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacac']
Pos
Train Epoch 0, Loss 0.8489952087402344
Train Epoch 1, Loss 0.7936426401138306
Train Epoch 2, Loss 0.7455001473426819
Accuracy at epoch 209: 0.513671875, total training samples: 232
Early stopping at epoch 136. Loss did not improve for 10 epochs.
Generate examples Step 136, Loss 0.3177611438462334
Epoch: 210
Negative Examples
['a', 'aaaabcba', 'abacbcba', 'b', 'babb', 'bac', 'bb', 'cab', 'cb', 'cbb', 'cbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ababc', 'bacaaaba', 'bbbbc', 'bc']
Pos Pos Pos Pos
Counterexamples
['ababc', 'bbbbc', 'bc']
Neg Neg Neg
Train Epoch 0, Loss 0.6976688504219055
Train Epoch 1, Loss 0.6579554080963135
Train Epoch 2, Loss 0.6193900108337402
Accuracy at epoch 210: 0.4921875, total training samples: 235
Early stopping at epoch 148. Loss did not improve for 10 epochs.
Generate examples Step 148, Loss 0.31650007091112586
Epoch: 211
Negative Examples
['a', 'abaaa', 'abbbcc', 'ac', 'acbbb', 'ba', 'bacacaaa', 'bba', 'bbac', 'bbbabb', 'bcabbaa', 'c', 'caabbbc', 'cbbbaabc', 'ccac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 211: No counterexamples found, skipped.
Accuracy at epoch 211: 0.54296875, total training samples: 235
Early stopping at epoch 146. Loss did not improve for 10 epochs.
Generate examples Step 146, Loss 0.3278694996217481
Epoch: 212
Negative Examples
['aabb', 'aacb', 'abcacc', 'ac', 'b', 'bcaacbac', 'bcc', 'bccc', 'bcccba', 'c', 'cabaaba', 'cabcbc', 'cb', 'ccac', 'cccaac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 212: No counterexamples found, skipped.
Accuracy at epoch 212: 0.482421875, total training samples: 235
Early stopping at epoch 175. Loss did not improve for 10 epochs.
Generate examples Step 175, Loss 0.3288923535834659
Epoch: 213
Negative Examples
['aabbcbaa', 'abcc', 'acabcca', 'b', 'baacacb', 'babacbac', 'bacba', 'bb', 'bcccacbc', 'cabaca', 'cacac', 'caccabc', 'cbcbab', 'ccbbc', 'ccbbcccc', 'ccbcab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['bacba']
Pos
Train Epoch 0, Loss 0.7836664915084839
Train Epoch 1, Loss 0.7314315438270569
Train Epoch 2, Loss 0.6828821301460266
Accuracy at epoch 213: 0.611328125, total training samples: 236
Early stopping at epoch 199. Loss did not improve for 10 epochs.
Generate examples Step 199, Loss 0.31957180678844455
Epoch: 214
Negative Examples
['a', 'abbc', 'abccbaaa', 'babcb', 'bacbb', 'bbbbccb', 'caaac', 'caaaccb', 'cabbccca', 'cabcbacc', 'cacbbbcb', 'cb', 'cbbcabc', 'ccabcbbb', 'ccbaba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bba']
Pos
Counterexamples
['bba']
Neg
Train Epoch 0, Loss 0.7103383541107178
Train Epoch 1, Loss 0.6669523119926453
Train Epoch 2, Loss 0.6245630383491516
Accuracy at epoch 214: 0.501953125, total training samples: 237
Early stopping at epoch 176. Loss did not improve for 10 epochs.
Generate examples Step 176, Loss 0.3953074513182128
Epoch: 215
Negative Examples
['abcaaca', 'accabb', 'baabab', 'bb', 'bbb', 'bbbbb', 'bbbcb', 'bbccc', 'bc', 'bcbb', 'cabcc', 'cabccccc', 'cbbabc', 'ccbccc', 'cccbc', 'cccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 215: No counterexamples found, skipped.
Accuracy at epoch 215: 0.52734375, total training samples: 237
Early stopping at epoch 188. Loss did not improve for 10 epochs.
Generate examples Step 188, Loss 0.38973309801369116
Epoch: 216
Negative Examples
['aab', 'aabcb', 'ab', 'abcb', 'acabb', 'accabbab', 'b', 'babacaab', 'baccaabc', 'bb', 'bbabcacb', 'bbbba', 'bbbcbaaa', 'bc', 'cbaca', 'ccacbcab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 216: No counterexamples found, skipped.
Accuracy at epoch 216: 0.482421875, total training samples: 237
Early stopping at epoch 140. Loss did not improve for 10 epochs.
Generate examples Step 140, Loss 0.30963391805371493
Epoch: 217
Negative Examples
['aabaaa', 'abaaacc', 'abbbc', 'acb', 'b', 'baaacb', 'bc', 'c', 'caaaaac', 'cb', 'cbcbcb', 'ccbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 217: No counterexamples found, skipped.
Accuracy at epoch 217: 0.51171875, total training samples: 237
Early stopping at epoch 157. Loss did not improve for 10 epochs.
Generate examples Step 157, Loss 0.32458816751649106
Epoch: 218
Negative Examples
['aaabaca', 'aaabacb', 'aaac', 'aabbbb', 'aacb', 'aacbabbc', 'ab', 'acba', 'bac', 'bacaa', 'bbb', 'bc', 'cbcabb', 'cc', 'ccabaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 218: No counterexamples found, skipped.
Accuracy at epoch 218: 0.48046875, total training samples: 237
Early stopping at epoch 164. Loss did not improve for 10 epochs.
Generate examples Step 164, Loss 0.3407544607465917
Epoch: 219
Negative Examples
['aa', 'aacbbba', 'abb', 'abba', 'accc', 'b', 'baccba', 'bbabc', 'bbba', 'bbcbbcbb', 'bcbacaa', 'c', 'caaaaabc', 'caabaaa', 'ccbab', 'ccbcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abba', 'baccba']
Pos Pos
Train Epoch 0, Loss 0.7870296239852905
Train Epoch 1, Loss 0.7403056621551514
Train Epoch 2, Loss 0.6984596848487854
Accuracy at epoch 219: 0.611328125, total training samples: 239
Early stopping at epoch 188. Loss did not improve for 10 epochs.
Generate examples Step 188, Loss 0.30955103654710076
Epoch: 220
Negative Examples
['a', 'aababcb', 'aabcc', 'aaca', 'aacbbbbc', 'abbbaca', 'aca', 'b', 'babacc', 'bcca', 'caacb', 'cbb', 'cbbcaccc', 'cbbccacc', 'ccabbbcc', 'cccababa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 220: No counterexamples found, skipped.
Accuracy at epoch 220: 0.630859375, total training samples: 239
Early stopping at epoch 152. Loss did not improve for 10 epochs.
Generate examples Step 152, Loss 0.33237458307758655
Epoch: 221
Negative Examples
['a', 'aabacb', 'ab', 'acacca', 'b', 'bb', 'c', 'cac', 'cbbaabbb', 'cbcc', 'cbccacc', 'ccbcaaca', 'ccccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acabba', 'ba']
Pos Pos
Counterexamples
['acabba', 'ba']
Neg Neg
Train Epoch 0, Loss 0.7119245529174805
Train Epoch 1, Loss 0.6723358631134033
Train Epoch 2, Loss 0.6337499618530273
Accuracy at epoch 221: 0.5078125, total training samples: 241
Early stopping at epoch 191. Loss did not improve for 10 epochs.
Generate examples Step 191, Loss 0.3278674407241245
Epoch: 222
Negative Examples
['aacb', 'accccb', 'b', 'bababacc', 'bccb', 'bccbcac', 'bcccaccc', 'c', 'caaaaacb', 'cacac', 'cacacb', 'cbacc', 'cbcbbbb', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 222: No counterexamples found, skipped.
Accuracy at epoch 222: 0.53125, total training samples: 241
Early stopping at epoch 204. Loss did not improve for 10 epochs.
Generate examples Step 204, Loss 0.31801013961070923
Epoch: 223
Negative Examples
['aacbcba', 'abccbccc', 'acabacb', 'b', 'baa', 'baacb', 'bbc', 'bbca', 'bcabca', 'bcc', 'bccb', 'ca', 'cab', 'cbabbac', 'cbbcaacb', 'cbbcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 223: No counterexamples found, skipped.
Accuracy at epoch 223: 0.4921875, total training samples: 241
Early stopping at epoch 184. Loss did not improve for 10 epochs.
Generate examples Step 184, Loss 0.41449364037127107
Epoch: 224
Negative Examples
['a', 'aba', 'acca', 'babacc', 'babbcbc', 'bcaa', 'bcca', 'c', 'cabab', 'cabbaa', 'cac', 'cbbcc', 'ccaacab', 'ccaccaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 224: No counterexamples found, skipped.
Accuracy at epoch 224: 0.453125, total training samples: 241
Early stopping at epoch 186. Loss did not improve for 10 epochs.
Generate examples Step 186, Loss 0.31382608732437706
Epoch: 225
Negative Examples
['a', 'aac', 'abcbc', 'ac', 'baaacccb', 'bacbbca', 'bbaca', 'bbca', 'bbcca', 'bcaccc', 'c', 'caabaccb', 'cbbbb', 'ccbaccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 225: No counterexamples found, skipped.
Accuracy at epoch 225: 0.517578125, total training samples: 241
Early stopping at epoch 153. Loss did not improve for 10 epochs.
Generate examples Step 153, Loss 0.4162569982664926
Epoch: 226
Negative Examples
['ab', 'abcab', 'ac', 'b', 'baaa', 'babb', 'c', 'ca', 'caa', 'caaacb', 'cabbba', 'cbaacbc', 'ccbbcaa', 'ccccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 226: No counterexamples found, skipped.
Accuracy at epoch 226: 0.474609375, total training samples: 241
Early stopping at epoch 261. Loss did not improve for 10 epochs.
Generate examples Step 261, Loss 0.31429597200783155
Epoch: 227
Negative Examples
['abacc', 'abcc', 'aca', 'b', 'bacabccc', 'baccc', 'bbaaca', 'bca', 'bcabca', 'bcc', 'bccb', 'bccc', 'c', 'cbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 227: No counterexamples found, skipped.
Accuracy at epoch 227: 0.529296875, total training samples: 241
Early stopping at epoch 271. Loss did not improve for 10 epochs.
Generate examples Step 271, Loss 0.3163189108976546
Epoch: 228
Negative Examples
['aacabca', 'aacca', 'abcbacc', 'aca', 'acc', 'b', 'baacb', 'babca', 'baca', 'bb', 'bbbacc', 'bbcc', 'ca', 'cc', 'cccbbcab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 228: No counterexamples found, skipped.
Accuracy at epoch 228: 0.494140625, total training samples: 241
Early stopping at epoch 234. Loss did not improve for 10 epochs.
Generate examples Step 234, Loss 0.31496194537649763
Epoch: 229
Negative Examples
['aabaaacc', 'aabca', 'abcbb', 'acacbaca', 'accc', 'bbbb', 'bcaacb', 'bcbbbbba', 'bcbca', 'c', 'ca', 'cabcaaca', 'cbabbbab', 'cbbbca', 'cbcca', 'cbccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 229: No counterexamples found, skipped.
Accuracy at epoch 229: 0.455078125, total training samples: 241
Early stopping at epoch 236. Loss did not improve for 10 epochs.
Generate examples Step 236, Loss 0.3486857848076881
Epoch: 230
Negative Examples
['a', 'aaaabcba', 'ababcbb', 'acbc', 'acbcabab', 'accb', 'ba', 'bacbbbb', 'bbc', 'bbcb', 'bc', 'ca', 'caabb', 'ccabcbb', 'ccbaacb', 'ccbcbbca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 230: No counterexamples found, skipped.
Accuracy at epoch 230: 0.5078125, total training samples: 241
Early stopping at epoch 180. Loss did not improve for 10 epochs.
Generate examples Step 180, Loss 0.31000210863450617
Epoch: 231
Negative Examples
['a', 'aacb', 'ababc', 'abbbcba', 'ac', 'aca', 'acb', 'b', 'babcacca', 'baccc', 'bbbcb', 'c', 'caabccb', 'cc', 'ccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abbbcba']
Pos
Train Epoch 0, Loss 0.7535772323608398
Train Epoch 1, Loss 0.7051776051521301
Train Epoch 2, Loss 0.659842848777771
Accuracy at epoch 231: 0.634765625, total training samples: 242
Early stopping at epoch 249. Loss did not improve for 10 epochs.
Generate examples Step 249, Loss 0.2904126087427139
Epoch: 232
Negative Examples
['aabcb', 'abca', 'acc', 'b', 'babacc', 'bbcb', 'bca', 'bcb', 'bcc', 'ca', 'cbbbcc', 'ccccbccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bbba', 'bbcca']
Pos Pos
Counterexamples
['bbba', 'bbcca']
Neg Neg
Train Epoch 0, Loss 0.7024683952331543
Train Epoch 1, Loss 0.6634332537651062
Train Epoch 2, Loss 0.6262298822402954
Accuracy at epoch 232: 0.513671875, total training samples: 244
Early stopping at epoch 238. Loss did not improve for 10 epochs.
Generate examples Step 238, Loss 0.31811745608202086
Epoch: 233
Negative Examples
['a', 'aabacc', 'aabcbb', 'aabcc', 'abbabaca', 'abbbbca', 'abbbbcb', 'abcc', 'acacac', 'acacbbab', 'accbbcca', 'babbbbcc', 'bc', 'bca', 'bcb', 'cabcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 233: No counterexamples found, skipped.
Accuracy at epoch 233: 0.529296875, total training samples: 244
Early stopping at epoch 237. Loss did not improve for 10 epochs.
Generate examples Step 237, Loss 0.33472003080263857
Epoch: 234
Negative Examples
['a', 'aaaccb', 'aacb', 'aba', 'abb', 'abcbcab', 'abccc', 'baaacbb', 'baabcbb', 'babccc', 'bbbabcbc', 'bcaa', 'c', 'caacb', 'cca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 234: No counterexamples found, skipped.
Accuracy at epoch 234: 0.501953125, total training samples: 244
Early stopping at epoch 182. Loss did not improve for 10 epochs.
Generate examples Step 182, Loss 0.31982629790983563
Epoch: 235
Negative Examples
['aacbcca', 'abb', 'abc', 'acbaac', 'bbbbacc', 'bbcbacc', 'bbcbccc', 'bbccc', 'bc', 'bcbbaba', 'c', 'cabbacc', 'cabccb', 'cc', 'ccbcab', 'ccbccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 235: No counterexamples found, skipped.
Accuracy at epoch 235: 0.50390625, total training samples: 244
Early stopping at epoch 244. Loss did not improve for 10 epochs.
Generate examples Step 244, Loss 0.2995570469875725
Epoch: 236
Negative Examples
['aaabcc', 'baab', 'babacc', 'bacaaccb', 'bcca', 'ca', 'cacaacc', 'cb', 'cbbbacb', 'cc', 'ccacbaca', 'ccbaca', 'cccbacc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 236: No counterexamples found, skipped.
Accuracy at epoch 236: 0.544921875, total training samples: 244
Early stopping at epoch 161. Loss did not improve for 10 epochs.
Generate examples Step 161, Loss 0.30785179671681956
Epoch: 237
Negative Examples
['a', 'aaa', 'aaaca', 'aabcba', 'acc', 'b', 'baca', 'c', 'ca', 'cb', 'cbc', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 237: No counterexamples found, skipped.
Accuracy at epoch 237: 0.47265625, total training samples: 244
Early stopping at epoch 206. Loss did not improve for 10 epochs.
Generate examples Step 206, Loss 0.3031943050271647
Epoch: 238
Negative Examples
['aaabccc', 'abcca', 'baaabaca', 'bacabca', 'bacbabcc', 'bbaca', 'bca', 'bcaabca', 'bcb', 'bcca', 'c', 'ca', 'caa', 'cb', 'cbbaca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 238: No counterexamples found, skipped.
Accuracy at epoch 238: 0.4921875, total training samples: 244
Early stopping at epoch 234. Loss did not improve for 10 epochs.
Generate examples Step 234, Loss 0.3242277376195218
Epoch: 239
Negative Examples
['a', 'aabbca', 'abbabc', 'babbbc', 'bbaacc', 'bbcbcbb', 'bccc', 'c', 'caacbcca', 'cacca', 'cbb', 'cc', 'ccb', 'ccbaccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 239: No counterexamples found, skipped.
Accuracy at epoch 239: 0.4921875, total training samples: 244
Early stopping at epoch 179. Loss did not improve for 10 epochs.
Generate examples Step 179, Loss 0.33705746945407655
Epoch: 240
Negative Examples
['a', 'aabbca', 'aacbbac', 'abbabaac', 'abc', 'acacbc', 'b', 'bbbabbb', 'bc', 'bccbaaab', 'bccc', 'c', 'cbb', 'cc', 'ccbbaacc', 'ccbbcaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 240: No counterexamples found, skipped.
Accuracy at epoch 240: 0.4921875, total training samples: 244
Early stopping at epoch 157. Loss did not improve for 10 epochs.
Generate examples Step 157, Loss 0.3184694064946114
Epoch: 241
Negative Examples
['a', 'aca', 'b', 'baccc', 'bbababa', 'bbb', 'bc', 'bcabaaca', 'bcb', 'bcc', 'bcccb', 'cb', 'cbcbc', 'cc', 'cca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 241: No counterexamples found, skipped.
Accuracy at epoch 241: 0.470703125, total training samples: 244
Early stopping at epoch 278. Loss did not improve for 10 epochs.
Generate examples Step 278, Loss 0.3270880843789774
Epoch: 242
Negative Examples
['a', 'aaacbaac', 'aacb', 'abaaaba', 'abcbb', 'abccc', 'acbccb', 'ba', 'bb', 'bbcaacca', 'bbccb', 'bcbcbc', 'cbacca', 'cbc', 'ccbccc', 'ccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaaaba']
Pos
Train Epoch 0, Loss 0.7607583999633789
Train Epoch 1, Loss 0.71287602186203
Train Epoch 2, Loss 0.6674063205718994
Accuracy at epoch 242: 0.615234375, total training samples: 245
Early stopping at epoch 275. Loss did not improve for 10 epochs.
Generate examples Step 275, Loss 0.30687975915877713
Epoch: 243
Negative Examples
['aacb', 'abbbcca', 'aca', 'acabbcca', 'babb', 'bbcca', 'bbccb', 'bcca', 'c', 'ca', 'cbbbbcca', 'cc', 'cca', 'ccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 243: No counterexamples found, skipped.
Accuracy at epoch 243: 0.6171875, total training samples: 245
Early stopping at epoch 256. Loss did not improve for 10 epochs.
Generate examples Step 256, Loss 0.2903199682903661
Epoch: 244
Negative Examples
['aaca', 'aca', 'acbcca', 'b', 'bbacb', 'bbacc', 'bcca', 'bccc', 'caaacc', 'cacbbacc', 'cc', 'cca', 'ccbcbcca', 'cccbaccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 244: No counterexamples found, skipped.
Accuracy at epoch 244: 0.62890625, total training samples: 245
Early stopping at epoch 287. Loss did not improve for 10 epochs.
Generate examples Step 287, Loss 0.2731306539434526
Epoch: 245
Negative Examples
['aaab', 'acaccc', 'acbbccb', 'b', 'bacb', 'bcabbacb', 'caacbccb', 'caccb', 'cb', 'cbaca', 'cbbaca', 'ccb', 'ccbcaaca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 245: No counterexamples found, skipped.
Accuracy at epoch 245: 0.619140625, total training samples: 245
Early stopping at epoch 235. Loss did not improve for 10 epochs.
Generate examples Step 235, Loss 0.2871204027432506
Epoch: 246
Negative Examples
['a', 'aacaacb', 'acb', 'b', 'baccb', 'bbaab', 'bccc', 'ca', 'cacbbaca', 'cb', 'cc', 'ccbcca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 246: No counterexamples found, skipped.
Accuracy at epoch 246: 0.65625, total training samples: 245
Early stopping at epoch 235. Loss did not improve for 10 epochs.
Generate examples Step 235, Loss 0.2929529247142501
Epoch: 247
Negative Examples
['aaaaaccc', 'b', 'bbabb', 'bbaca', 'bbbacb', 'bbcca', 'caacc', 'caaccb', 'cacbbaac', 'cbabaccc', 'cbbbabb', 'cca', 'ccacacca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 247: No counterexamples found, skipped.
Accuracy at epoch 247: 0.6328125, total training samples: 245
Early stopping at epoch 267. Loss did not improve for 10 epochs.
Generate examples Step 267, Loss 0.2972443276154461
Epoch: 248
Negative Examples
['a', 'abbaaa', 'abc', 'aca', 'baacca', 'baca', 'bbbabb', 'bbbaccc', 'bbbbaca', 'bbbccb', 'bbccc', 'bcabcbb', 'cbbaca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acbacac']
Pos
Counterexamples
['acbacac']
Neg
Train Epoch 0, Loss 0.7099554538726807
Train Epoch 1, Loss 0.6571890115737915
Train Epoch 2, Loss 0.6090684533119202
Accuracy at epoch 248: 0.470703125, total training samples: 246
Early stopping at epoch 181. Loss did not improve for 10 epochs.
Generate examples Step 181, Loss 0.2935953585656135
Epoch: 249
Negative Examples
['aaa', 'aaccbb', 'ab', 'ac', 'b', 'babbbaba', 'baccb', 'bbccab', 'bcbb', 'ca', 'cb', 'cbcaca', 'cca', 'ccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 249: No counterexamples found, skipped.
Accuracy at epoch 249: 0.494140625, total training samples: 246
Early stopping at epoch 203. Loss did not improve for 10 epochs.
Generate examples Step 203, Loss 0.30720796143891765
Epoch: 250
Negative Examples
['aa', 'aaab', 'aab', 'aaca', 'ab', 'abaac', 'abbbaabb', 'acaccac', 'baaacca', 'bbabc', 'bbacaca', 'c', 'cb', 'cbbccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaac']
Pos
Train Epoch 0, Loss 0.8814905285835266
Train Epoch 1, Loss 0.8226171731948853
Train Epoch 2, Loss 0.7708446979522705
Accuracy at epoch 250: 0.666015625, total training samples: 247
Early stopping at epoch 218. Loss did not improve for 10 epochs.
Generate examples Step 218, Loss 0.33330466690128796
Epoch: 251
Negative Examples
['aaaabacc', 'b', 'bbabca', 'bbacbb', 'bbbbcc', 'bbc', 'bcbbbca', 'bcbbcc', 'c', 'cabacca', 'cabbacbc', 'cac', 'cbabcc', 'cbb', 'cbcabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bba']
Pos
Counterexamples
['bba']
Neg
Train Epoch 0, Loss 0.696146547794342
Train Epoch 1, Loss 0.6550807952880859
Train Epoch 2, Loss 0.6157058477401733
Accuracy at epoch 251: 0.533203125, total training samples: 248
Early stopping at epoch 155. Loss did not improve for 10 epochs.
Generate examples Step 155, Loss 0.3188441575337679
Epoch: 252
Negative Examples
['aa', 'ab', 'abbb', 'ac', 'acbaccbb', 'b', 'baabcab', 'babbbb', 'bac', 'bacb', 'bb', 'bbaa', 'bbaacbca', 'bcbabcab', 'cccacbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 252: No counterexamples found, skipped.
Accuracy at epoch 252: 0.51953125, total training samples: 248
Early stopping at epoch 163. Loss did not improve for 10 epochs.
Generate examples Step 163, Loss 0.32142910652044343
Epoch: 253
Negative Examples
['a', 'abab', 'ababac', 'abacba', 'ac', 'accaba', 'acccca', 'b', 'bb', 'bbcabb', 'caccbb', 'cbbabaaa', 'cbbbbbab', 'ccbcaabc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abacba']
Pos
Train Epoch 0, Loss 0.7866799235343933
Train Epoch 1, Loss 0.7379200458526611
Train Epoch 2, Loss 0.6925802230834961
Accuracy at epoch 253: 0.619140625, total training samples: 249
Early stopping at epoch 198. Loss did not improve for 10 epochs.
Generate examples Step 198, Loss 0.3192698657512665
Epoch: 254
Negative Examples
['ab', 'aba', 'abaabca', 'abab', 'aca', 'acaaaca', 'acabcb', 'b', 'bacca', 'bbaca', 'bbbcccb', 'bbcbca', 'cabbacca', 'cacabb', 'cb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 254: No counterexamples found, skipped.
Accuracy at epoch 254: 0.6328125, total training samples: 249
Early stopping at epoch 211. Loss did not improve for 10 epochs.
Generate examples Step 211, Loss 0.3525088238266279
Epoch: 255
Negative Examples
['aabcca', 'abacab', 'b', 'baacaca', 'babbbb', 'bbba', 'bbcaa', 'bbcbacaa', 'bccbccb', 'c', 'ca', 'cabacac', 'cb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['acbc', 'cabcacba']
Pos Pos
Counterexamples
['acbc', 'cabcacba']
Neg Neg
Train Epoch 0, Loss 0.7181528210639954
Train Epoch 1, Loss 0.6785315275192261
Train Epoch 2, Loss 0.6406909823417664
Accuracy at epoch 255: 0.509765625, total training samples: 251
Early stopping at epoch 121. Loss did not improve for 10 epochs.
Generate examples Step 121, Loss 0.3975023825149067
Epoch: 256
Negative Examples
['a', 'aa', 'abbbbc', 'abccaabb', 'ac', 'acbc', 'bab', 'baba', 'bbac', 'bbb', 'bcb', 'c', 'caa', 'ccacbac', 'ccbc', 'cccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baba']
Pos
Train Epoch 0, Loss 0.7749972343444824
Train Epoch 1, Loss 0.7281638979911804
Train Epoch 2, Loss 0.6837966442108154
Accuracy at epoch 256: 0.662109375, total training samples: 252
Early stopping at epoch 206. Loss did not improve for 10 epochs.
Generate examples Step 206, Loss 0.31386685529768754
Epoch: 257
Negative Examples
['aabb', 'aacb', 'aacc', 'aaccacc', 'abaa', 'abaacacb', 'ababcc', 'abbcc', 'accc', 'b', 'baaacaca', 'babcb', 'baccacb', 'cc', 'ccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 257: No counterexamples found, skipped.
Accuracy at epoch 257: 0.65625, total training samples: 252
Early stopping at epoch 157. Loss did not improve for 10 epochs.
Generate examples Step 157, Loss 0.33434945627858365
Epoch: 258
Negative Examples
['a', 'aacacca', 'abaaaca', 'abbbbc', 'abc', 'accaa', 'acccaabb', 'bacbcbca', 'bc', 'c', 'ca', 'cabb', 'cbabccb', 'cbcacc', 'ccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 258: No counterexamples found, skipped.
Accuracy at epoch 258: 0.65234375, total training samples: 252
Early stopping at epoch 170. Loss did not improve for 10 epochs.
Generate examples Step 170, Loss 0.3224456998688436
Epoch: 259
Negative Examples
['acbcbacc', 'accab', 'acccbabc', 'b', 'bacca', 'bbaccb', 'bbaccc', 'bbca', 'caacaaa', 'cb', 'cbaa', 'ccbabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bacbbbba', 'bbaaaaa', 'bcba', 'bccaba']
Pos Pos Pos Pos
Counterexamples
['bacbbbba', 'bbaaaaa', 'bcba', 'bccaba']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7078486680984497
Train Epoch 1, Loss 0.6693481802940369
Train Epoch 2, Loss 0.634249746799469
Accuracy at epoch 259: 0.509765625, total training samples: 256
Early stopping at epoch 153. Loss did not improve for 10 epochs.
Generate examples Step 153, Loss 0.3807784367304344
Epoch: 260
Negative Examples
['aa', 'aaa', 'ac', 'acbc', 'ba', 'babac', 'bbbcccba', 'bcaab', 'c', 'caaaba', 'cabca', 'cacbbaac', 'cbccb', 'cc', 'ccbabaa', 'cccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 260: No counterexamples found, skipped.
Accuracy at epoch 260: 0.53125, total training samples: 256
Early stopping at epoch 141. Loss did not improve for 10 epochs.
Generate examples Step 141, Loss 0.31692941948561604
Epoch: 261
Negative Examples
['a', 'aab', 'aabacc', 'aac', 'abb', 'b', 'baabbc', 'baacc', 'bbaac', 'bbac', 'bcccc', 'cbba', 'cca', 'ccbcbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 261: No counterexamples found, skipped.
Accuracy at epoch 261: 0.501953125, total training samples: 256
Early stopping at epoch 162. Loss did not improve for 10 epochs.
Generate examples Step 162, Loss 0.41600839619987584
Epoch: 262
Negative Examples
['aa', 'abb', 'abbacba', 'acbabaac', 'bb', 'bbabaaaa', 'bbcb', 'bcbbab', 'bcc', 'caaa', 'caaaa', 'cabaccbb', 'cbcab', 'ccacb', 'ccb', 'ccbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 262: No counterexamples found, skipped.
Accuracy at epoch 262: 0.494140625, total training samples: 256
Early stopping at epoch 149. Loss did not improve for 10 epochs.
Generate examples Step 149, Loss 0.3390585438410441
Epoch: 263
Negative Examples
['aaac', 'aabcbbc', 'aabccb', 'abaabbba', 'abcbc', 'acaac', 'accbcc', 'baabbaa', 'bab', 'babaca', 'bb', 'bba', 'bc', 'cabc', 'cacaac', 'cbcaccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaabbba']
Pos
Train Epoch 0, Loss 0.8030143976211548
Train Epoch 1, Loss 0.7582192420959473
Train Epoch 2, Loss 0.7175408601760864
Accuracy at epoch 263: 0.64453125, total training samples: 257
Early stopping at epoch 173. Loss did not improve for 10 epochs.
Generate examples Step 173, Loss 0.3294838463095413
Epoch: 264
Negative Examples
['a', 'acaaacca', 'baa', 'baaacaac', 'baccc', 'bbbcbc', 'bbbccbab', 'bc', 'bccbbaca', 'c', 'caaac', 'cabca', 'caccacb', 'cbacccbb', 'ccbcc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baaacaac']
Pos
Train Epoch 0, Loss 0.7316499948501587
Train Epoch 1, Loss 0.6917480230331421
Train Epoch 2, Loss 0.6528383493423462
Accuracy at epoch 264: 0.58984375, total training samples: 258
Early stopping at epoch 227. Loss did not improve for 10 epochs.
Generate examples Step 227, Loss 0.3280989736841436
Epoch: 265
Negative Examples
['ab', 'abbcb', 'accbbcb', 'bca', 'ccbacacb']
Neg Neg Neg Neg Neg
Positive Examples
['aa', 'abacb', 'b', 'bbbcbbca', 'bcabcab', 'bcabcacc', 'bccbaaca', 'cbaaac', 'cbbb']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aa', 'abacb', 'b', 'bbbcbbca', 'bcabcab', 'bcabcacc', 'bccbaaca', 'cbaaac', 'cbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7173862457275391
Train Epoch 1, Loss 0.6839702129364014
Train Epoch 2, Loss 0.6558207869529724
Accuracy at epoch 265: 0.546875, total training samples: 267
Early stopping at epoch 162. Loss did not improve for 10 epochs.
Generate examples Step 162, Loss 0.3037845600236413
Epoch: 266
Negative Examples
['a', 'aaacaa', 'aababab', 'aaccc', 'abacbbcb', 'abcbca', 'abcca', 'acccb', 'b', 'baaacc', 'bcaa', 'c', 'cabaa', 'cc', 'ccbcc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 266: No counterexamples found, skipped.
Accuracy at epoch 266: 0.568359375, total training samples: 267
Early stopping at epoch 144. Loss did not improve for 10 epochs.
Generate examples Step 144, Loss 0.34604120871116373
Epoch: 267
Negative Examples
['aba', 'ac', 'b', 'bacacc', 'bbabba', 'bc', 'bcbcbcc', 'ca', 'cabac', 'cabbbb', 'cb', 'cbbbbaa', 'ccacbab', 'ccbcaaaa', 'cccaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 267: No counterexamples found, skipped.
Accuracy at epoch 267: 0.59765625, total training samples: 267
Early stopping at epoch 160. Loss did not improve for 10 epochs.
Generate examples Step 160, Loss 0.31802586516978576
Epoch: 268
Negative Examples
['aab', 'aabcbab', 'ababccb', 'accbcbaa', 'b', 'baab', 'babaac', 'bbba', 'bcbaab', 'bccacc', 'c', 'cabacba', 'cb', 'cbac', 'cbcab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaba']
Pos
Counterexamples
['aaaba']
Neg
Train Epoch 0, Loss 0.6939516067504883
Train Epoch 1, Loss 0.6520299315452576
Train Epoch 2, Loss 0.6107025146484375
Accuracy at epoch 268: 0.46484375, total training samples: 268
Early stopping at epoch 135. Loss did not improve for 10 epochs.
Generate examples Step 135, Loss 0.3292693241554148
Epoch: 269
Negative Examples
['a', 'aa', 'abaa', 'abbbaccc', 'abccb', 'ac', 'acba', 'ba', 'baba', 'bb', 'caa', 'cb', 'cbabaa', 'cbbccb', 'cccbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baba']
Pos
Train Epoch 0, Loss 0.7898744344711304
Train Epoch 1, Loss 0.7377880215644836
Train Epoch 2, Loss 0.6927065849304199
Accuracy at epoch 269: 0.611328125, total training samples: 269
Early stopping at epoch 157. Loss did not improve for 10 epochs.
Generate examples Step 157, Loss 0.3236006960461411
Epoch: 270
Negative Examples
['aabbcb', 'ab', 'ac', 'acbccaaa', 'acbccbb', 'babbcb', 'bc', 'bcaaab', 'bccba', 'bcccb', 'ca', 'cba', 'cbaaac', 'cbac', 'cbbac', 'cccac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 270: No counterexamples found, skipped.
Accuracy at epoch 270: 0.548828125, total training samples: 269
Early stopping at epoch 142. Loss did not improve for 10 epochs.
Generate examples Step 142, Loss 0.3407421145405803
Epoch: 271
Negative Examples
['a', 'ab', 'abcaacb', 'abcc', 'ac', 'acacbacb', 'b', 'bab', 'bbbcac', 'bcb', 'bcbbacc', 'c', 'cab', 'ccbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 271: No counterexamples found, skipped.
Accuracy at epoch 271: 0.58203125, total training samples: 269
Early stopping at epoch 176. Loss did not improve for 10 epochs.
Generate examples Step 176, Loss 0.2855186794100508
Epoch: 272
Negative Examples
['a', 'aaccbacb', 'abb', 'accbb', 'b', 'babccabb', 'bb', 'bcbbbb', 'bcc', 'bcca', 'cab', 'cacaab', 'cbbaacc', 'cca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 272: No counterexamples found, skipped.
Accuracy at epoch 272: 0.568359375, total training samples: 269
Early stopping at epoch 216. Loss did not improve for 10 epochs.
Generate examples Step 216, Loss 0.27645641730128345
Epoch: 273
Negative Examples
['aabbccb', 'aacb', 'aaccabcb', 'ab', 'abcbbca', 'acca', 'b', 'ba', 'baabba', 'babcbccb', 'bb', 'bbbca', 'ca', 'cb', 'cbaccccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 273: No counterexamples found, skipped.
Accuracy at epoch 273: 0.5390625, total training samples: 269
Early stopping at epoch 164. Loss did not improve for 10 epochs.
Generate examples Step 164, Loss 0.29879548983140425
Epoch: 274
Negative Examples
['a', 'aaaaacca', 'aacaccc', 'abbba', 'acb', 'acbbbaba', 'b', 'ba', 'bbbaacbb', 'bbcca', 'ca', 'cca', 'ccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baaa', 'bccbc', 'cccbccbc']
Pos Pos Pos
Counterexamples
['baaa', 'bccbc', 'cccbccbc']
Neg Neg Neg
Train Epoch 0, Loss 0.7058237195014954
Train Epoch 1, Loss 0.6678931713104248
Train Epoch 2, Loss 0.6337359547615051
Accuracy at epoch 274: 0.49609375, total training samples: 272
Early stopping at epoch 150. Loss did not improve for 10 epochs.
Generate examples Step 150, Loss 0.2887350863573567
Epoch: 275
Negative Examples
['a', 'aa', 'aaa', 'aacca', 'aacccaca', 'abbbbab', 'ba', 'bacc', 'bb', 'bba', 'bbbab', 'c', 'cb', 'cbbccab', 'cbcabbca', 'ccca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 275: No counterexamples found, skipped.
Accuracy at epoch 275: 0.484375, total training samples: 272
Early stopping at epoch 159. Loss did not improve for 10 epochs.
Generate examples Step 159, Loss 0.301513465680182
Epoch: 276
Negative Examples
['a', 'aaccc', 'abacbbb', 'abbaa', 'ac', 'bacaaca', 'bacbbaca', 'bbacc', 'bbc', 'bbcacc', 'bcbcaacc', 'c', 'cb', 'ccaaa', 'ccbabbcb', 'ccccbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 276: No counterexamples found, skipped.
Accuracy at epoch 276: 0.49609375, total training samples: 272
Early stopping at epoch 166. Loss did not improve for 10 epochs.
Generate examples Step 166, Loss 0.3352791699106822
Epoch: 277
Negative Examples
['a', 'aacbaaba', 'ab', 'abbbcbcb', 'acbcbacb', 'acbcca', 'babaab', 'babbbbab', 'bba', 'bbbcac', 'c', 'cacb', 'cacbbb', 'cbcaccab', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 277: No counterexamples found, skipped.
Accuracy at epoch 277: 0.490234375, total training samples: 272
Early stopping at epoch 157. Loss did not improve for 10 epochs.
Generate examples Step 157, Loss 0.3116368987137758
Epoch: 278
Negative Examples
['aaaaab', 'aab', 'abb', 'abbc', 'abcbbbc', 'acbaba', 'acbabcb', 'accc', 'baacc', 'babaaa', 'bb', 'bbbaaba', 'caabcac', 'cbcbaac', 'ccaac']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 278: No counterexamples found, skipped.
Accuracy at epoch 278: 0.521484375, total training samples: 272
Early stopping at epoch 136. Loss did not improve for 10 epochs.
Generate examples Step 136, Loss 0.30581360315754463
Epoch: 279
Negative Examples
['a', 'aaba', 'aac', 'ab', 'abbccaaa', 'acbcbcba', 'baac', 'babcac', 'bb', 'bbacacaa', 'bcaabbca', 'caa', 'cb', 'cbc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['baac']
Pos
Train Epoch 0, Loss 0.7407121658325195
Train Epoch 1, Loss 0.6935340762138367
Train Epoch 2, Loss 0.6474255919456482
Accuracy at epoch 279: 0.60546875, total training samples: 273
Early stopping at epoch 191. Loss did not improve for 10 epochs.
Generate examples Step 191, Loss 0.2789877898370226
Epoch: 280
Negative Examples
['a', 'aa', 'ab', 'acaaccca', 'acbbb', 'acbcabca', 'b', 'ba', 'bacab', 'bbbca', 'bbcbba', 'bccabca', 'cab', 'cb', 'cca', 'ccaabca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 280: No counterexamples found, skipped.
Accuracy at epoch 280: 0.6328125, total training samples: 273
Early stopping at epoch 175. Loss did not improve for 10 epochs.
Generate examples Step 175, Loss 0.2928866357965903
Epoch: 281
Negative Examples
['a', 'ab', 'aba', 'abaaca', 'abab', 'bbcaacc', 'caacbc', 'cabb', 'cbccaa', 'cbccb', 'ccaaaaab', 'ccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaac', 'acac', 'bbab', 'bcaa']
Pos Pos Pos Pos
Counterexamples
['aaaac', 'acac', 'bbab', 'bcaa']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7020500302314758
Train Epoch 1, Loss 0.6640690565109253
Train Epoch 2, Loss 0.6300532221794128
Accuracy at epoch 281: 0.505859375, total training samples: 277
Early stopping at epoch 161. Loss did not improve for 10 epochs.
Generate examples Step 161, Loss 0.39107008480731353
Epoch: 282
Negative Examples
['a', 'aaaacc', 'aacc', 'aacccbab', 'aba', 'abababca', 'abacbcba', 'acbbcac', 'acbcbacb', 'baccb', 'bbaaca', 'cb', 'cbcaaaa', 'ccaba', 'ccaccb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 282: No counterexamples found, skipped.
Accuracy at epoch 282: 0.53125, total training samples: 277
Early stopping at epoch 223. Loss did not improve for 10 epochs.
Generate examples Step 223, Loss 0.2923260114288756
Epoch: 283
Negative Examples
['a', 'aaabacaa', 'ab', 'abbb', 'acaa', 'accb', 'baaaaccb', 'bcacb', 'bccaaccc', 'caaacacb', 'cabaccaa', 'cba', 'cbab', 'cbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 283: No counterexamples found, skipped.
Accuracy at epoch 283: 0.48828125, total training samples: 277
Early stopping at epoch 172. Loss did not improve for 10 epochs.
Generate examples Step 172, Loss 0.3084461370300006
Epoch: 284
Negative Examples
['aab', 'aaccbcca', 'abcaac', 'accbaca', 'b', 'baaa', 'babbcccc', 'bbabbcba', 'bbaccbb', 'bbcab', 'bcaacaab', 'bcb', 'bcbabb', 'caacacbc', 'cbcac', 'ccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abcaac']
Pos
Train Epoch 0, Loss 0.7744195461273193
Train Epoch 1, Loss 0.7272927165031433
Train Epoch 2, Loss 0.6833468079566956
Accuracy at epoch 284: 0.56640625, total training samples: 278
Early stopping at epoch 152. Loss did not improve for 10 epochs.
Generate examples Step 152, Loss 0.3303283630243314
Epoch: 285
Negative Examples
['aa', 'aaba', 'abbbabb', 'abbbb', 'bacaba', 'bb', 'bbcbcbb', 'bbccc', 'bccaa', 'bccba', 'bccbccca', 'cbbab', 'cc', 'cca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aac', 'c']
Pos Pos
Counterexamples
['bacaba', 'aac', 'c']
Pos Neg Neg
Train Epoch 0, Loss 0.7045450806617737
Train Epoch 1, Loss 0.6833000183105469
Train Epoch 2, Loss 0.6627146601676941
Accuracy at epoch 285: 0.634765625, total training samples: 281
Early stopping at epoch 164. Loss did not improve for 10 epochs.
Generate examples Step 164, Loss 0.3594595164963693
Epoch: 286
Negative Examples
['a', 'aab', 'aabb', 'ab', 'b', 'bbbbabcb', 'bcaaa', 'bccb', 'ca', 'cababbac', 'cb', 'cbabccca', 'ccaaaaac', 'ccbcacba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aacabc', 'abbcbc']
Pos Pos
Counterexamples
['aacabc', 'abbcbc']
Neg Neg
Train Epoch 0, Loss 0.738910436630249
Train Epoch 1, Loss 0.6924445629119873
Train Epoch 2, Loss 0.650982141494751
Accuracy at epoch 286: 0.48046875, total training samples: 283
Early stopping at epoch 167. Loss did not improve for 10 epochs.
Generate examples Step 167, Loss 0.35880777789723306
Epoch: 287
Negative Examples
['aababa', 'aabcab', 'abaccac', 'ac', 'accacbb', 'bbbbbaca', 'bbbbbc', 'bbcbaa', 'bcabcaab', 'bcccaca', 'bcccba', 'c', 'caaaba', 'cbabcbc', 'cbbccabc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaccac']
Pos
Train Epoch 0, Loss 0.8038405776023865
Train Epoch 1, Loss 0.7539016604423523
Train Epoch 2, Loss 0.7082656025886536
Accuracy at epoch 287: 0.509765625, total training samples: 284
Early stopping at epoch 201. Loss did not improve for 10 epochs.
Generate examples Step 201, Loss 0.30925002782651695
Epoch: 288
Negative Examples
['aaac', 'abb', 'abbabbcc', 'abbbbaa', 'abcbcbac', 'b', 'baba', 'bbbab', 'bbccc', 'bccbb', 'caccca', 'cba', 'cbaacbc', 'cbabacaa', 'cbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['cacaccaa']
Pos
Counterexamples
['baba', 'cacaccaa']
Pos Neg
Train Epoch 0, Loss 0.7036081552505493
Train Epoch 1, Loss 0.6821622848510742
Train Epoch 2, Loss 0.6615792512893677
Accuracy at epoch 288: 0.58984375, total training samples: 286
Early stopping at epoch 170. Loss did not improve for 10 epochs.
Generate examples Step 170, Loss 0.3390071087064799
Epoch: 289
Negative Examples
['a', 'aaaaabcb', 'aacbaa', 'abbacba', 'abcacac', 'acabacaa', 'acacbbb', 'accc', 'bcbaacb', 'c', 'caab', 'cabcc', 'cac', 'cbbaaacc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabcbba']
Pos
Counterexamples
['abcacac', 'aabcbba']
Pos Neg
Train Epoch 0, Loss 0.7299326658248901
Train Epoch 1, Loss 0.7073763608932495
Train Epoch 2, Loss 0.6868343353271484
Accuracy at epoch 289: 0.626953125, total training samples: 288
Early stopping at epoch 160. Loss did not improve for 10 epochs.
Generate examples Step 160, Loss 0.3333377780751412
Epoch: 290
Negative Examples
['aaaccab', 'aabbaaba', 'acbca', 'b', 'baabaab', 'bac', 'bbab', 'bbbaccc', 'bbca', 'bccac', 'ca', 'caccca', 'cbccc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['c', 'cacc']
Pos Pos
Counterexamples
['c', 'cacc']
Neg Neg
Train Epoch 0, Loss 0.7074254155158997
Train Epoch 1, Loss 0.670974612236023
Train Epoch 2, Loss 0.6372771263122559
Accuracy at epoch 290: 0.498046875, total training samples: 290
Early stopping at epoch 150. Loss did not improve for 10 epochs.
Generate examples Step 150, Loss 0.37160390003627497
Epoch: 291
Negative Examples
['aaaaccba', 'aacabb', 'acaba', 'acca', 'accac', 'bb', 'bc', 'bcbacbaa', 'c', 'caccbb', 'cbb', 'cbcaabcc', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 291: No counterexamples found, skipped.
Accuracy at epoch 291: 0.44921875, total training samples: 290
Early stopping at epoch 182. Loss did not improve for 10 epochs.
Generate examples Step 182, Loss 0.3253855244383786
Epoch: 292
Negative Examples
['a', 'abbaab', 'acaba', 'b', 'bacbccc', 'bcab', 'bcbb', 'bccccb', 'c', 'caa', 'caaaaba', 'cabccaab', 'ccabccaa', 'ccbbb', 'cccbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 292: No counterexamples found, skipped.
Accuracy at epoch 292: 0.478515625, total training samples: 290
Early stopping at epoch 144. Loss did not improve for 10 epochs.
Generate examples Step 144, Loss 0.3679691193432644
Epoch: 293
Negative Examples
['a', 'abaaac', 'acbbcaac', 'b', 'babbac', 'bacbbb', 'bb', 'bbbaacbb', 'bbcb', 'bbccaab', 'bcccbaa', 'c', 'cb', 'cbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaaac', 'babbac']
Pos Pos
Train Epoch 0, Loss 0.757703423500061
Train Epoch 1, Loss 0.7171790599822998
Train Epoch 2, Loss 0.6776260137557983
Accuracy at epoch 293: 0.677734375, total training samples: 292
Early stopping at epoch 177. Loss did not improve for 10 epochs.
Generate examples Step 177, Loss 0.32261437026972184
Epoch: 294
Negative Examples
['a', 'aabacba', 'aca', 'b', 'babbcb', 'bacca', 'baccba', 'caaacbab', 'cab', 'cbbbabb', 'cc', 'ccbbb', 'ccccbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abcbaaba']
Pos
Counterexamples
['baccba', 'abcbaaba']
Pos Neg
Train Epoch 0, Loss 0.7069965600967407
Train Epoch 1, Loss 0.6846920251846313
Train Epoch 2, Loss 0.6629828214645386
Accuracy at epoch 294: 0.685546875, total training samples: 294
Early stopping at epoch 192. Loss did not improve for 10 epochs.
Generate examples Step 192, Loss 0.3077081744534982
Epoch: 295
Negative Examples
['aa', 'aaacb', 'ab', 'ababacb', 'aca', 'b', 'baa', 'caacb', 'cabbb', 'cb', 'cbcacb', 'cc', 'cca', 'ccbca']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 295: No counterexamples found, skipped.
Accuracy at epoch 295: 0.65625, total training samples: 294
Early stopping at epoch 176. Loss did not improve for 10 epochs.
Generate examples Step 176, Loss 0.3461210747896615
Epoch: 296
Negative Examples
['aacbbbcb', 'aacc', 'ab', 'abbabac', 'abcbbacc', 'abccccbb', 'acccaba', 'babba', 'bbbabab', 'bbcabca', 'bcaacb', 'cabcccca', 'ccbacbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['babacc', 'caa', 'cabc']
Pos Pos Pos
Counterexamples
['babacc', 'caa', 'cabc']
Neg Neg Neg
Train Epoch 0, Loss 0.7131116986274719
Train Epoch 1, Loss 0.6787741184234619
Train Epoch 2, Loss 0.6465044617652893
Accuracy at epoch 296: 0.5078125, total training samples: 297
Early stopping at epoch 188. Loss did not improve for 10 epochs.
Generate examples Step 188, Loss 0.35216512456142085
Epoch: 297
Negative Examples
['aa', 'ab', 'ababaab', 'abbbcbc', 'abccc', 'acaaaa', 'acbbccc', 'ba', 'baacb', 'bbbcacb', 'bbccbbca', 'ca', 'cac', 'cbabb', 'cbac', 'cc']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 297: No counterexamples found, skipped.
Accuracy at epoch 297: 0.521484375, total training samples: 297
Early stopping at epoch 171. Loss did not improve for 10 epochs.
Generate examples Step 171, Loss 0.3259795178161111
Epoch: 298
Negative Examples
['a', 'ab', 'acbbacca', 'b', 'bab', 'bcab', 'bcacabb', 'bcbab', 'bcbcab', 'bcca', 'bcccbb', 'c', 'ca', 'ccbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 298: No counterexamples found, skipped.
Accuracy at epoch 298: 0.478515625, total training samples: 297
Early stopping at epoch 203. Loss did not improve for 10 epochs.
Generate examples Step 203, Loss 0.3284597176254964
Epoch: 299
Negative Examples
['aabbaa', 'abb', 'acbbbcc', 'b', 'baabcbca', 'bab', 'baca', 'bacacbba', 'bb', 'bbabba', 'bcb', 'bccacab', 'cbbacbbb', 'cbcabcb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 299: No counterexamples found, skipped.
Accuracy at epoch 299: 0.517578125, total training samples: 297
Pos train / Tot train = 87 / 297
