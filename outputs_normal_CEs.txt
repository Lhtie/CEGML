Early stopping at epoch 121. Loss did not improve for 10 epochs.
Generate examples Step 121, Loss 0.2810733223059138
Epoch: 0
Negative Examples
['a', 'aa', 'b']
Neg Neg Neg
Positive Examples
['aaaba', 'aabaabab', 'aabbab', 'aabbabb', 'aabbbab', 'aba', 'abaa', 'abaaa', 'abaababa', 'ababab', 'ababbaab', 'ababbbab', 'abb', 'abbaba', 'abbbab', 'abbbb', 'baaaa', 'baaabbaa', 'babbbbaa', 'bbb', 'bbbba']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaba', 'aabbab', 'aabbabb', 'aabbbab', 'aba', 'abaa', 'abaaa', 'abaababa', 'ababbaab', 'ababbbab', 'abb', 'abbaba', 'abbbab', 'abbbb', 'baaaa', 'baaabbaa', 'babbbbaa', 'bbb', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.6998685002326965
Train Epoch 1, Loss 0.6194526553153992
Train Epoch 2, Loss 0.5441020131111145
Accuracy at epoch 0: 0.9677734375, total training samples: 19
Early stopping at epoch 110. Loss did not improve for 10 epochs.
Generate examples Step 110, Loss 0.26401336397136654
Epoch: 1
Negative Examples
['aa', 'aaaa', 'aaab', 'aab', 'aababaaa', 'ab', 'abaaaab', 'abababbb', 'abbbb', 'b', 'ba', 'baabaab', 'baabab', 'bab', 'babab', 'bababaa', 'babbbab', 'bb', 'bbaaaa', 'bbabbba', 'bbb', 'bbba', 'bbbabaa', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaab', 'aab', 'ab', 'abaaaab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.9612507820129395
Train Epoch 1, Loss 0.8516148328781128
Train Epoch 2, Loss 0.7614928483963013
Accuracy at epoch 1: 0.8681640625, total training samples: 23
Early stopping at epoch 131. Loss did not improve for 10 epochs.
Generate examples Step 131, Loss 0.2825862458257964
Epoch: 2
Negative Examples
['a', 'aa', 'aaa', 'aaaaba', 'aaabbbb', 'aabaa', 'aabbbaaa', 'ab', 'abaaa', 'ababa', 'ababbb', 'abbbbabb', 'b', 'ba', 'baaaabbb', 'baabaab', 'babaa', 'bababaa', 'bba', 'bbaa', 'bbabaaaa', 'bbba', 'bbbabbbb', 'bbbb', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabb', 'abab']
Pos Pos
Counterexamples
['ab', 'aabb']
Pos Neg
Train Epoch 0, Loss 0.7066981792449951
Train Epoch 1, Loss 0.6866334676742554
Train Epoch 2, Loss 0.6682991981506348
Accuracy at epoch 2: 0.9609375, total training samples: 25
Early stopping at epoch 107. Loss did not improve for 10 epochs.
Generate examples Step 107, Loss 0.2601246174286913
Epoch: 3
Negative Examples
['a', 'aa', 'aaaab', 'aaaba', 'aabaa', 'aba', 'abaaaaaa', 'abaabb', 'ababa', 'abb', 'abbaa', 'abbaab', 'abbb', 'abbbb', 'ba', 'baaba', 'bab', 'babbbaab', 'bba', 'bbaab', 'bbaabb', 'bbbaaaba', 'bbbaaba', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ab', 'b']
Pos Pos
Counterexamples
['aaaab', 'b']
Pos Neg
Train Epoch 0, Loss 0.7026377320289612
Train Epoch 1, Loss 0.682357668876648
Train Epoch 2, Loss 0.6647939085960388
Accuracy at epoch 3: 0.4345703125, total training samples: 27
Early stopping at epoch 179. Loss did not improve for 10 epochs.
Generate examples Step 179, Loss 0.3441138532426622
Epoch: 4
Negative Examples
['a', 'aaabbb', 'aabbabba', 'aabbabbb', 'aba', 'abaabbb', 'ababa', 'abababa', 'abbba', 'abbbb', 'b', 'ba', 'bababb', 'babbb', 'bba', 'bbabbbbb', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabb', 'ababb', 'babaabb', 'babb', 'bb']
Pos Pos Pos Pos Pos
Counterexamples
['aabb', 'ababb', 'babaabb', 'babb', 'bb']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.695833146572113
Train Epoch 1, Loss 0.6264734268188477
Train Epoch 2, Loss 0.5616286993026733
Accuracy at epoch 4: 0.96875, total training samples: 32
Early stopping at epoch 121. Loss did not improve for 10 epochs.
Generate examples Step 121, Loss 0.277897659872399
Epoch: 5
Negative Examples
['a', 'aa', 'aaa', 'aaaa', 'aaaaabaa', 'aaaabbb', 'aaab', 'aab', 'aabaaaba', 'abaa', 'ababa', 'ababaa', 'abb', 'abbaba', 'b', 'ba', 'baa', 'baaaabba', 'babaa', 'babbbb', 'bb', 'bbaaa', 'bbababbb', 'bbb', 'bbbabba', 'bbbabbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaab', 'aab']
Pos Pos
Train Epoch 0, Loss 0.8263381719589233
Train Epoch 1, Loss 0.7439528107643127
Train Epoch 2, Loss 0.6729145050048828
Accuracy at epoch 5: 0.5615234375, total training samples: 34
Early stopping at epoch 112. Loss did not improve for 10 epochs.
Generate examples Step 112, Loss 0.29527855789766905
Epoch: 6
Negative Examples
['a', 'aaaba', 'aaabb', 'aabb', 'aabbbbbb', 'ababaabb', 'abb', 'b', 'ba', 'bababa', 'bb', 'bbb', 'bbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aababbab', 'abab', 'baa', 'baaaa', 'baabaaa', 'bab', 'bbaa', 'bbabaab', 'bbbbaaa']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aababbab', 'baa', 'baaaa', 'baabaaa', 'bab', 'bbaa', 'bbabaab', 'bbbbaaa']
Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.755770206451416
Train Epoch 1, Loss 0.6882992386817932
Train Epoch 2, Loss 0.6250917911529541
Accuracy at epoch 6: 0.9609375, total training samples: 42
Early stopping at epoch 116. Loss did not improve for 10 epochs.
Generate examples Step 116, Loss 0.2581515155541591
Epoch: 7
Negative Examples
['a', 'aa', 'aaa', 'aaaaaab', 'aaaaba', 'aaabaabb', 'aaabab', 'aaba', 'aabaaab', 'aabbaba', 'ab', 'abaabba', 'abba', 'abbabb', 'b', 'ba', 'baa', 'baabba', 'baba', 'babbbb', 'bb', 'bbaa', 'bbb', 'bbbaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaaaab', 'aaabab', 'aabaaab', 'ab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.7972055077552795
Train Epoch 1, Loss 0.7205361723899841
Train Epoch 2, Loss 0.6540555953979492
Accuracy at epoch 7: 0.5830078125, total training samples: 46
Early stopping at epoch 146. Loss did not improve for 10 epochs.
Generate examples Step 146, Loss 0.292164656377974
Epoch: 8
Negative Examples
['a', 'aaababb', 'aaba', 'aabbbb', 'aba', 'ababaaa', 'abbaaaba', 'abbaabba', 'abbba', 'b', 'ba', 'baaabba', 'baaba', 'bababb', 'bba', 'bbb', 'bbbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aa', 'aaa', 'aaabaaab', 'aaabab', 'aab', 'aabaa', 'abbbbbab', 'bb', 'bbab', 'bbbaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aa', 'aaa', 'aabaa', 'abbbbbab', 'bb', 'bbab', 'bbbaaab']
Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7432059049606323
Train Epoch 1, Loss 0.6823693513870239
Train Epoch 2, Loss 0.6237520575523376
Accuracy at epoch 8: 0.9765625, total training samples: 53
Early stopping at epoch 122. Loss did not improve for 10 epochs.
Generate examples Step 122, Loss 0.25731655761478395
Epoch: 9
Negative Examples
['a', 'aa', 'aaabaa', 'aaabbb', 'aaba', 'ab', 'abaaaba', 'abaaabba', 'abab', 'abababa', 'abb', 'abba', 'abbbbbab', 'b', 'ba', 'baaabaab', 'baabbabb', 'bab', 'baba', 'babb', 'babbb', 'bbaa', 'bbabaab', 'bbabaabb', 'bbb', 'bbba', 'bbbbab', 'bbbbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['ab', 'abab']
Pos Pos
Train Epoch 0, Loss 0.7924935817718506
Train Epoch 1, Loss 0.7169990539550781
Train Epoch 2, Loss 0.6500741243362427
Accuracy at epoch 9: 0.662109375, total training samples: 55
Early stopping at epoch 158. Loss did not improve for 10 epochs.
Generate examples Step 158, Loss 0.31339588911278443
Epoch: 10
Negative Examples
['aaa', 'ababb', 'abbaaa', 'abbb', 'abbbabb', 'b', 'baa', 'baaa', 'baabaabb', 'babbbb', 'bbabba', 'bbb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aab', 'aabbab', 'ab', 'abbbabbb', 'baaaaaa', 'baaaab', 'babaa', 'bb', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabbab', 'abbbabbb', 'baaaaaa', 'baaaab', 'babaa', 'bb', 'bbab']
Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7314971089363098
Train Epoch 1, Loss 0.674281656742096
Train Epoch 2, Loss 0.6185895800590515
Accuracy at epoch 10: 0.970703125, total training samples: 62
Early stopping at epoch 129. Loss did not improve for 10 epochs.
Generate examples Step 129, Loss 0.25169433160470084
Epoch: 11
Negative Examples
['aaa', 'aaaba', 'aabaaba', 'ab', 'abaa', 'abaab', 'abaabab', 'ababbbbb', 'abb', 'abbababa', 'abbbbbab', 'b', 'ba', 'baaaaa', 'baaaaabb', 'baaaab', 'baaababa', 'baab', 'babaa', 'bababbab', 'bb', 'bbab', 'bbabbb', 'bbbaa', 'bbbb', 'bbbbaba', 'bbbbb', 'bbbbbaba', 'bbbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['ab', 'abaab', 'abaabab']
Pos Pos Pos
Train Epoch 0, Loss 0.7907094359397888
Train Epoch 1, Loss 0.7188887000083923
Train Epoch 2, Loss 0.6556498408317566
Accuracy at epoch 11: 0.7470703125, total training samples: 65
Early stopping at epoch 111. Loss did not improve for 10 epochs.
Generate examples Step 111, Loss 0.2784945177180426
Epoch: 12
Negative Examples
['a', 'aaaaa', 'aaabaaba', 'aaabbaa', 'aabaa', 'aabaaa', 'aabbbb', 'abbba', 'abbbbba', 'b', 'bababb', 'babb', 'bbba', 'bbbabaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aa', 'aaaaaaab', 'aaab', 'aabab', 'ab', 'abaabbab', 'ababab', 'baaabab', 'baab', 'bb']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aa', 'abaabbab', 'baaabab', 'baab', 'bb']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7462643384933472
Train Epoch 1, Loss 0.6875718832015991
Train Epoch 2, Loss 0.6313225626945496
Accuracy at epoch 12: 0.9677734375, total training samples: 70
Early stopping at epoch 166. Loss did not improve for 10 epochs.
Generate examples Step 166, Loss 0.30688281544668233
Epoch: 13
Negative Examples
['a', 'aa', 'aaa', 'aaaabb', 'aaaba', 'aabaaa', 'aababb', 'aabb', 'aabbb', 'abaa', 'abaababb', 'abb', 'abbb', 'abbbbbb', 'ba', 'baa', 'baaaaab', 'baaababb', 'baaabbb', 'baabaaab', 'bababb', 'babbbab', 'bb', 'bbb', 'bbbabab', 'bbbabbbb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 13: No counterexamples found, skipped.
Early stopping at epoch 113. Loss did not improve for 10 epochs.
Generate examples Step 113, Loss 0.26560917444396437
Epoch: 14
Negative Examples
['a', 'aa', 'ab', 'aba', 'ababaabb', 'ababba', 'ababbaa', 'abba', 'abbaa', 'abbaabba', 'abbabbb', 'abbbba', 'b', 'ba', 'baa', 'baabaaaa', 'bab', 'babaa', 'bbaabaa', 'bbaabab', 'bbbba', 'bbbbaaaa', 'bbbbaba', 'bbbbabab', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['ab']
Pos
Train Epoch 0, Loss 0.7764375805854797
Train Epoch 1, Loss 0.703244686126709
Train Epoch 2, Loss 0.6365071535110474
Accuracy at epoch 14: 0.75, total training samples: 71
Early stopping at epoch 116. Loss did not improve for 10 epochs.
Generate examples Step 116, Loss 0.2674392986501384
Epoch: 15
Negative Examples
['aaa', 'aabbbaa', 'aabbbba', 'aba', 'abaaaabb', 'abaaba', 'abaabbb', 'abb', 'abba', 'ba', 'baa', 'baaa', 'baaaaba', 'baabbaba', 'bbaa', 'bbaabb', 'bbabbaba', 'bbabbbb', 'bbbaaabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aa', 'aab', 'ab', 'abaaab', 'abbbaab', 'bbabbaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aa', 'abbbaab', 'bbabbaab']
Neg Neg Neg
Train Epoch 0, Loss 0.7525885105133057
Train Epoch 1, Loss 0.6899714469909668
Train Epoch 2, Loss 0.6304376125335693
Accuracy at epoch 15: 0.9599609375, total training samples: 74
Early stopping at epoch 107. Loss did not improve for 10 epochs.
Generate examples Step 107, Loss 0.246804377960938
Epoch: 16
Negative Examples
['a', 'aa', 'aaa', 'aaaab', 'aaabaaba', 'aaba', 'abba', 'abbbaab', 'b', 'ba', 'baa', 'baaa', 'baaba', 'baabbbb', 'baabbbbb', 'bab', 'babba', 'babbbb', 'bb', 'bba', 'bbaa', 'bbbaab', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaab']
Pos
Train Epoch 0, Loss 0.7760632634162903
Train Epoch 1, Loss 0.7049831748008728
Train Epoch 2, Loss 0.6429836750030518
Accuracy at epoch 16: 0.78515625, total training samples: 75
Early stopping at epoch 107. Loss did not improve for 10 epochs.
Generate examples Step 107, Loss 0.2772223632092829
Epoch: 17
Negative Examples
['a', 'aa', 'aabaaa', 'aababb', 'abaaa', 'ababaa', 'abb', 'abbba', 'abbbbaa', 'b', 'baaa', 'baaaa', 'baaaaa', 'babbaabb', 'bb', 'bbaaaba', 'bbb', 'bbba', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaaabab', 'ab', 'abaabbab', 'bbabaaab', 'bbbaab', 'bbbabaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaabbab', 'bbabaaab', 'bbbaab', 'bbbabaab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7617012858390808
Train Epoch 1, Loss 0.6967737078666687
Train Epoch 2, Loss 0.6339954137802124
Accuracy at epoch 17: 0.9765625, total training samples: 79
Early stopping at epoch 164. Loss did not improve for 10 epochs.
Generate examples Step 164, Loss 0.27350366386500274
Epoch: 18
Negative Examples
['aa', 'aaa', 'aaaabaa', 'aaabb', 'aab', 'aabaab', 'aabb', 'abaaabb', 'abaab', 'abbabb', 'abbabbb', 'abbbaab', 'abbbb', 'b', 'baabbb', 'babbbab', 'babbbb', 'bb', 'bbab', 'bbabbb', 'bbabbbb', 'bbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aab', 'aabaab', 'abaab']
Pos Pos Pos
Train Epoch 0, Loss 0.8119035363197327
Train Epoch 1, Loss 0.7396696209907532
Train Epoch 2, Loss 0.6765053868293762
Accuracy at epoch 18: 0.7783203125, total training samples: 82
Early stopping at epoch 161. Loss did not improve for 10 epochs.
Generate examples Step 161, Loss 0.2874414876655296
Epoch: 19
Negative Examples
['aa', 'aaabbbbb', 'aabbbb', 'abb', 'b', 'baabb', 'babbabb', 'babbb', 'bb', 'bbaaaabb', 'bbababbb', 'bbabb', 'bbabbabb', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aabaaabb', 'aabab', 'aababbab', 'aabbbbab', 'abbaab', 'abbbaab', 'baaaaab', 'baab', 'babab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabaaabb', 'aababbab', 'aabbbbab', 'abbaab', 'abbbaab', 'baaaaab', 'baab', 'babab']
Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7487679719924927
Train Epoch 1, Loss 0.6891717314720154
Train Epoch 2, Loss 0.6309680938720703
Accuracy at epoch 19: 0.9697265625, total training samples: 90
Early stopping at epoch 120. Loss did not improve for 10 epochs.
Generate examples Step 120, Loss 0.23694330897213015
Epoch: 20
Negative Examples
['a', 'aaa', 'aaaa', 'aaaab', 'aaaabb', 'aaaabbba', 'aaabaa', 'aababb', 'aabbaaa', 'ababaaba', 'abababb', 'abbaaaba', 'abbab', 'abbbb', 'b', 'baaa', 'baaaab', 'bab', 'babaa', 'babab', 'babbbbb', 'bb', 'bbaa', 'bbaabb', 'bbaabbba', 'bbab', 'bbabbbab', 'bbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaab']
Pos
Train Epoch 0, Loss 0.7603194117546082
Train Epoch 1, Loss 0.6915954351425171
Train Epoch 2, Loss 0.6298105120658875
Accuracy at epoch 20: 0.7587890625, total training samples: 91
Early stopping at epoch 126. Loss did not improve for 10 epochs.
Generate examples Step 126, Loss 0.2743566636964092
Epoch: 21
Negative Examples
['a', 'aaa', 'aaaa', 'aaabbb', 'aaabbbaa', 'aaba', 'aabba', 'aabbbb', 'abaa', 'b', 'ba', 'baaaa', 'babbb', 'bb', 'bbb', 'bbbaabaa', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabb', 'babaabab', 'babab', 'bbaaaaa', 'bbaab', 'bbbab', 'bbbabaab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaabb', 'babaabab', 'babab', 'bbaaaaa', 'bbaab', 'bbbab', 'bbbabaab']
Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7327426075935364
Train Epoch 1, Loss 0.6732117533683777
Train Epoch 2, Loss 0.6143766045570374
Accuracy at epoch 21: 0.9658203125, total training samples: 98
Early stopping at epoch 137. Loss did not improve for 10 epochs.
Generate examples Step 137, Loss 0.23412570486898007
Epoch: 22
Negative Examples
['a', 'aaaaaaa', 'aababbbb', 'aabbaaa', 'aabbaaba', 'aabbab', 'abaaab', 'abaaba', 'abb', 'abbbaab', 'abbbbbb', 'b', 'ba', 'baa', 'baab', 'baabbb', 'babaabaa', 'babb', 'babba', 'bb', 'bbaaa', 'bbababbb', 'bbabb', 'bbbaab', 'bbbab', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaaab']
Pos
Train Epoch 0, Loss 0.8017758727073669
Train Epoch 1, Loss 0.7248866558074951
Train Epoch 2, Loss 0.6594498157501221
Accuracy at epoch 22: 0.787109375, total training samples: 99
Early stopping at epoch 138. Loss did not improve for 10 epochs.
Generate examples Step 138, Loss 0.2707237633440992
Epoch: 23
Negative Examples
['a', 'aa', 'aaaabaaa', 'aaaabba', 'aabbb', 'ababb', 'abba', 'abbabaa', 'b', 'ba', 'baaabba', 'baabbab', 'baabbb', 'bababba', 'babbaaa', 'bb', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabb', 'aaabbaab', 'abaabb', 'abb', 'babaabab', 'bbaabb']
Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaabb', 'aaabbaab', 'abaabb', 'abb', 'babaabab', 'bbaabb']
Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7171075344085693
Train Epoch 1, Loss 0.658212423324585
Train Epoch 2, Loss 0.6009774804115295
Accuracy at epoch 23: 0.9658203125, total training samples: 105
Early stopping at epoch 125. Loss did not improve for 10 epochs.
Generate examples Step 125, Loss 0.23747937854320283
Epoch: 24
Negative Examples
['a', 'aa', 'aaa', 'aaabbab', 'aaabbbab', 'aabaaab', 'aabbaabb', 'aba', 'abaaa', 'abaaab', 'ababab', 'abb', 'abba', 'abbaba', 'b', 'ba', 'baa', 'baab', 'bab', 'babaabb', 'bb', 'bba', 'bbaa', 'bbabb', 'bbbaaabb', 'bbbaabb', 'bbbbaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aabaaab', 'abaaab', 'ababab']
Pos Pos Pos
Train Epoch 0, Loss 0.7904415130615234
Train Epoch 1, Loss 0.7151421904563904
Train Epoch 2, Loss 0.6498635411262512
Accuracy at epoch 24: 0.7822265625, total training samples: 108
Early stopping at epoch 120. Loss did not improve for 10 epochs.
Generate examples Step 120, Loss 0.2756146985637255
Epoch: 25
Negative Examples
['a', 'aa', 'aaa', 'aaaa', 'aaaabba', 'aaabb', 'aabb', 'abaa', 'abaabbb', 'ababb', 'ababbbb', 'abbababb', 'abbbaa', 'b', 'bb', 'bba', 'bbb', 'bbbaaba', 'bbbabbb', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaaa', 'ab', 'abab', 'baaaa', 'baab', 'baabab', 'bab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaaaaa', 'baaaa', 'baab', 'baabab', 'bab']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7308176755905151
Train Epoch 1, Loss 0.6771264672279358
Train Epoch 2, Loss 0.62604820728302
Accuracy at epoch 25: 0.974609375, total training samples: 113
Early stopping at epoch 174. Loss did not improve for 10 epochs.
Generate examples Step 174, Loss 0.26918175722871507
Epoch: 26
Negative Examples
['aaab', 'aaabaabb', 'aaababbb', 'aaabbb', 'aabaaabb', 'aabaabb', 'aabb', 'aabbbab', 'ab', 'abaaabbb', 'abaabb', 'abaabbab', 'b', 'bab', 'babb', 'babbb', 'bb', 'bbabb', 'bbb', 'bbbabb', 'bbbabbab', 'bbbb', 'bbbbaab', 'bbbbabab', 'bbbbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaab', 'ab']
Pos Pos
Train Epoch 0, Loss 0.7637141942977905
Train Epoch 1, Loss 0.6987415552139282
Train Epoch 2, Loss 0.6399927139282227
Accuracy at epoch 26: 0.771484375, total training samples: 115
Early stopping at epoch 156. Loss did not improve for 10 epochs.
Generate examples Step 156, Loss 0.2451282754825179
Epoch: 27
Negative Examples
['a', 'aabaaa', 'aababaaa', 'aabb', 'aba', 'ababaaa', 'abbabb', 'b', 'baa', 'baaaabbb', 'baaabbaa', 'baabbb', 'bababb', 'babb', 'babbaabb', 'babbbaba', 'bbabaaa', 'bbabbb', 'bbb', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabbbab', 'bababaab', 'bb', 'bbbab']
Pos Pos Pos Pos
Counterexamples
['aabbbab', 'bababaab', 'bb', 'bbbab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.714882493019104
Train Epoch 1, Loss 0.6537700295448303
Train Epoch 2, Loss 0.5931958556175232
Accuracy at epoch 27: 0.974609375, total training samples: 119
Early stopping at epoch 163. Loss did not improve for 10 epochs.
Generate examples Step 163, Loss 0.2539283743173611
Epoch: 28
Negative Examples
['aa', 'aaaab', 'aaabbb', 'aab', 'aabaaaa', 'ab', 'abaaaa', 'abaab', 'abbab', 'abbabb', 'abbb', 'b', 'baaaaab', 'baaaab', 'baab', 'baabaaab', 'baabb', 'bab', 'babb', 'babbb', 'bbab', 'bbbaabbb', 'bbbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaab', 'aab', 'ab', 'abaab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.7617864608764648
Train Epoch 1, Loss 0.6944464445114136
Train Epoch 2, Loss 0.6344833374023438
Accuracy at epoch 28: 0.75, total training samples: 123
Early stopping at epoch 176. Loss did not improve for 10 epochs.
Generate examples Step 176, Loss 0.26103486053350955
Epoch: 29
Negative Examples
['aabbab', 'aabbbbb', 'abb', 'abbbbbab', 'b', 'babb', 'babbb', 'bb', 'bbaabbab', 'bbab', 'bbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaab', 'aab', 'aabb', 'abaab', 'abab', 'ababaaaa', 'abbbaaab', 'bab', 'babaabab', 'bbaaab', 'bbaab', 'bbaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aabb', 'ababaaaa', 'abbbaaab', 'bab', 'babaabab', 'bbaaab', 'bbaab', 'bbaabab']
Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7652411460876465
Train Epoch 1, Loss 0.6970243453979492
Train Epoch 2, Loss 0.630919337272644
Accuracy at epoch 29: 0.96875, total training samples: 131
Early stopping at epoch 159. Loss did not improve for 10 epochs.
Generate examples Step 159, Loss 0.23186912946403027
Epoch: 30
Negative Examples
['aaababab', 'aabab', 'aabbabab', 'abab', 'ababab', 'ababb', 'ababbab', 'ababbabb', 'abb', 'abbabbbb', 'b', 'baaaab', 'baaabaab', 'baab', 'baabab', 'baabb', 'babab', 'babbb', 'bb', 'bbaaab', 'bbab', 'bbabaab', 'bbbb', 'bbbbab', 'bbbbbab', 'bbbbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaababab', 'aabab', 'abab', 'ababab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.8231974840164185
Train Epoch 1, Loss 0.7361071109771729
Train Epoch 2, Loss 0.6639423370361328
Accuracy at epoch 30: 0.7998046875, total training samples: 135
Early stopping at epoch 168. Loss did not improve for 10 epochs.
Generate examples Step 168, Loss 0.24990946539407652
Epoch: 31
Negative Examples
['a', 'aababb', 'aababbbb', 'aba', 'abaababa', 'abababa', 'abb', 'abbbaba', 'abbbabb', 'abbbbb', 'b', 'baababb', 'babb', 'bba', 'bbaba', 'bbabbbb', 'bbba', 'bbbaba', 'bbbba', 'bbbbaaba', 'bbbbaba', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 31: No counterexamples found, skipped.
Early stopping at epoch 142. Loss did not improve for 10 epochs.
Generate examples Step 142, Loss 0.2482900286054278
Epoch: 32
Negative Examples
['a', 'aa', 'aababba', 'aabbbb', 'ababb', 'abb', 'abbabb', 'abbbaa', 'abbbb', 'b', 'ba', 'baabaaa', 'baabbaaa', 'babaaabb', 'babaaba', 'babb', 'babbbabb', 'babbbb', 'bb', 'bbaaa', 'bbaaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabbab', 'baab', 'babaab', 'bbab']
Pos Pos Pos Pos
Counterexamples
['aabbab', 'baab', 'babaab', 'bbab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7170224785804749
Train Epoch 1, Loss 0.6571826934814453
Train Epoch 2, Loss 0.5984829068183899
Accuracy at epoch 32: 0.9794921875, total training samples: 139
Early stopping at epoch 131. Loss did not improve for 10 epochs.
Generate examples Step 131, Loss 0.2258489295614488
Epoch: 33
Negative Examples
['a', 'aa', 'aaba', 'aabba', 'abaabab', 'ababaab', 'abb', 'abba', 'abbabaab', 'abbba', 'abbbaaa', 'abbbab', 'abbbb', 'b', 'ba', 'baa', 'baaa', 'baababba', 'baababbb', 'baba', 'babbaabb', 'bb', 'bbaaaa', 'bbabaa', 'bbabaab', 'bbb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaabab', 'ababaab']
Pos Pos
Train Epoch 0, Loss 0.8357187509536743
Train Epoch 1, Loss 0.756190299987793
Train Epoch 2, Loss 0.6910966038703918
Accuracy at epoch 33: 0.7685546875, total training samples: 141
Early stopping at epoch 180. Loss did not improve for 10 epochs.
Generate examples Step 180, Loss 0.26447459009800167
Epoch: 34
Negative Examples
['aaabaaaa', 'abb', 'abbb', 'abbbbaab', 'abbbbbbb', 'b', 'bababbb', 'babb', 'babbbabb', 'bb', 'bbaabb', 'bbb', 'bbbaabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabb', 'aabb', 'aabbaab', 'abbaaabb', 'baabb', 'baabbab', 'bab', 'babaaabb', 'babab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabb', 'aabb', 'aabbaab', 'abbaaabb', 'baabb', 'baabbab', 'bab', 'babaaabb', 'babab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.715632975101471
Train Epoch 1, Loss 0.6622494459152222
Train Epoch 2, Loss 0.6098974347114563
Accuracy at epoch 34: 0.9736328125, total training samples: 150
Early stopping at epoch 172. Loss did not improve for 10 epochs.
Generate examples Step 172, Loss 0.26920616023802346
Epoch: 35
Negative Examples
['aaaaaab', 'aab', 'aabab', 'aababbb', 'aabbabb', 'aabbabbb', 'aabbbb', 'ab', 'abaab', 'abb', 'abbab', 'abbb', 'abbbaab', 'b', 'baaaaabb', 'baaab', 'baaabab', 'baab', 'bab', 'babab', 'babb', 'babbbb', 'bb', 'bbb', 'bbbabbb', 'bbbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaaaab', 'aab', 'aabab', 'ab', 'abaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7679879069328308
Train Epoch 1, Loss 0.7056501507759094
Train Epoch 2, Loss 0.6501361131668091
Accuracy at epoch 35: 0.806640625, total training samples: 155
Early stopping at epoch 154. Loss did not improve for 10 epochs.
Generate examples Step 154, Loss 0.25708693948484235
Epoch: 36
Negative Examples
['aa', 'aaaabb', 'aaabaa', 'aabaa', 'aabb', 'aabbb', 'abababb', 'ababbaa', 'abb', 'abbabbbb', 'abbb', 'abbbaa', 'abbbabb', 'abbbbbb', 'baabbb', 'babaaaa', 'babaabb', 'babbbbab', 'bb', 'bbabbb', 'bbb', 'bbbabbb', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabb', 'abbbabab', 'baabbab', 'bab', 'bbaab']
Pos Pos Pos Pos Pos
Counterexamples
['aaabb', 'abbbabab', 'baabbab', 'bab', 'bbaab']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7230571508407593
Train Epoch 1, Loss 0.6634453535079956
Train Epoch 2, Loss 0.6031156182289124
Accuracy at epoch 36: 0.9638671875, total training samples: 160
Early stopping at epoch 191. Loss did not improve for 10 epochs.
Generate examples Step 191, Loss 0.2955790579629441
Epoch: 37
Negative Examples
['aaaab', 'aaab', 'aaabbab', 'aaabbb', 'aabb', 'ab', 'abaaaaab', 'abab', 'abbaaab', 'abbaab', 'abbbabab', 'b', 'baab', 'baabaaab', 'baababab', 'bab', 'bb', 'bbaaab', 'bbbaaaab', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaab', 'aaab', 'ab', 'abaaaaab', 'abab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7427346110343933
Train Epoch 1, Loss 0.674992561340332
Train Epoch 2, Loss 0.615252673625946
Accuracy at epoch 37: 0.765625, total training samples: 165
Early stopping at epoch 171. Loss did not improve for 10 epochs.
Generate examples Step 171, Loss 0.24558005984439407
Epoch: 38
Negative Examples
['a', 'aababbb', 'ababb', 'abb', 'abbbb', 'abbbbaab', 'b', 'ba', 'baa', 'bababb', 'babb', 'babbaa', 'babbabb', 'babbbb', 'bb', 'bbabb', 'bbb', 'bbbaa', 'bbbaaaba', 'bbbaab', 'bbbaabbb', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 38: No counterexamples found, skipped.
Early stopping at epoch 185. Loss did not improve for 10 epochs.
Generate examples Step 185, Loss 0.23760478535006124
Epoch: 39
Negative Examples
['a', 'aabbbaba', 'aabbbba', 'abaabbbb', 'abababb', 'ababbbb', 'abbabb', 'abbbabb', 'abbbbb', 'b', 'ba', 'baabba', 'babb', 'babba', 'babbbbbb', 'bb', 'bba', 'bbb', 'bbba', 'bbbaa', 'bbbabbb', 'bbbabbbb', 'bbbb', 'bbbbaa', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 39: No counterexamples found, skipped.
Early stopping at epoch 169. Loss did not improve for 10 epochs.
Generate examples Step 169, Loss 0.2530028832309386
Epoch: 40
Negative Examples
['aaabbb', 'aabbb', 'aabbbbab', 'abb', 'abbababb', 'abbb', 'b', 'baabb', 'babbab', 'bb', 'bbababbb', 'bbb', 'bbbab', 'bbbb', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abaaabb', 'bab', 'babab']
Pos Pos Pos
Counterexamples
['abaaabb', 'bab', 'babab']
Neg Neg Neg
Train Epoch 0, Loss 0.7259767651557922
Train Epoch 1, Loss 0.6604529023170471
Train Epoch 2, Loss 0.59539395570755
Accuracy at epoch 40: 0.9736328125, total training samples: 168
Early stopping at epoch 179. Loss did not improve for 10 epochs.
Generate examples Step 179, Loss 0.2634521202908622
Epoch: 41
Negative Examples
['aaab', 'aaabab', 'aabaaabb', 'aabab', 'ab', 'abaab', 'abaabab', 'abb', 'abbaaab', 'b', 'baaaab', 'bab', 'babab', 'babbabab', 'babbbbab', 'bb', 'bbaaab', 'bbab', 'bbbaabab', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab']
Pos
Counterexamples
['aaab', 'aaabab', 'aabab', 'ab', 'abaab', 'abaabab']
Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7829203009605408
Train Epoch 1, Loss 0.7038025856018066
Train Epoch 2, Loss 0.6376269459724426
Accuracy at epoch 41: 0.7998046875, total training samples: 174
Early stopping at epoch 188. Loss did not improve for 10 epochs.
Generate examples Step 188, Loss 0.23318924672073787
Epoch: 42
Negative Examples
['aabbbb', 'ababbb', 'abb', 'abbabbb', 'abbb', 'abbbaabb', 'abbbbab', 'baababb', 'baabb', 'babb', 'babbab', 'babbb', 'babbbabb', 'bb', 'bbab', 'bbabb', 'bbb', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 42: No counterexamples found, skipped.
Early stopping at epoch 179. Loss did not improve for 10 epochs.
Generate examples Step 179, Loss 0.2606665755311648
Epoch: 43
Negative Examples
['aabb', 'abaabbb', 'abbb', 'abbbbaab', 'b', 'baababb', 'bababbb', 'babb', 'bb', 'bbaaabb', 'bbaab', 'bbab', 'bbb', 'bbbab', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaababab', 'aab', 'aabaab', 'abaaaabb', 'abab', 'abbab', 'baaaabab', 'baab', 'baabab', 'bab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaaaabb', 'abbab', 'baaaabab', 'baab', 'baabab', 'bab']
Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7486210465431213
Train Epoch 1, Loss 0.6811818480491638
Train Epoch 2, Loss 0.6158503293991089
Accuracy at epoch 43: 0.974609375, total training samples: 180
Early stopping at epoch 168. Loss did not improve for 10 epochs.
Generate examples Step 168, Loss 0.24962840842072076
Epoch: 44
Negative Examples
['aaabaaab', 'aaababb', 'aaabb', 'aab', 'aabb', 'ab', 'abaabb', 'abab', 'ababaaab', 'ababb', 'abbaab', 'abbabab', 'b', 'baaab', 'baab', 'bab', 'babb', 'babbbab', 'bb', 'bbaaabab', 'bbaabb', 'bbab', 'bbbab', 'bbbbabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaabaaab', 'aab', 'ab', 'abab', 'ababaaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7668190002441406
Train Epoch 1, Loss 0.6946576833724976
Train Epoch 2, Loss 0.6330206394195557
Accuracy at epoch 44: 0.80859375, total training samples: 185
Early stopping at epoch 166. Loss did not improve for 10 epochs.
Generate examples Step 166, Loss 0.24907789249976953
Epoch: 45
Negative Examples
['aabb', 'aabbab', 'ababb', 'ababbb', 'abb', 'abbbb', 'abbbbabb', 'b', 'baabaa', 'baabbb', 'bab', 'bababb', 'babbaabb', 'babbabbb', 'babbbab', 'babbbbb', 'bb', 'bbab', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabb', 'aabbaab', 'abaab', 'ababaab', 'baab']
Pos Pos Pos Pos Pos
Counterexamples
['aaabb', 'aabbaab', 'baab']
Neg Neg Neg
Train Epoch 0, Loss 0.7276442646980286
Train Epoch 1, Loss 0.6634489297866821
Train Epoch 2, Loss 0.6021485328674316
Accuracy at epoch 45: 0.978515625, total training samples: 188
Early stopping at epoch 188. Loss did not improve for 10 epochs.
Generate examples Step 188, Loss 0.2695463343114449
Epoch: 46
Negative Examples
['aaaab', 'aabab', 'abab', 'ababab', 'abbaaaab', 'abbaabab', 'abbab', 'abbabab', 'abbbabab', 'b', 'bab', 'babab', 'bb', 'bbaaaab', 'bbaab', 'bbabaaab', 'bbabaab', 'bbbaabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ab']
Pos
Counterexamples
['aaaab', 'aabab', 'abab', 'ababab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.7508376836776733
Train Epoch 1, Loss 0.669007420539856
Train Epoch 2, Loss 0.598093569278717
Accuracy at epoch 46: 0.73046875, total training samples: 192
Early stopping at epoch 168. Loss did not improve for 10 epochs.
Generate examples Step 168, Loss 0.23421249243282002
Epoch: 47
Negative Examples
['a', 'aababb', 'aabbaabb', 'aabbbaa', 'aabbbbb', 'abaaba', 'ababb', 'abb', 'abbabb', 'b', 'ba', 'baaaabb', 'baaabb', 'babaab', 'babaabbb', 'bababb', 'babb', 'bb', 'bbaaabb', 'bbaab', 'bbaabb', 'bbabbab', 'bbb', 'bbbb', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 47: No counterexamples found, skipped.
Early stopping at epoch 196. Loss did not improve for 10 epochs.
Generate examples Step 196, Loss 0.2494719910440106
Epoch: 48
Negative Examples
['aaaaabb', 'aaabb', 'aaabbbb', 'aababbab', 'aabb', 'aabbb', 'abaab', 'abaabbb', 'abb', 'abbab', 'b', 'baaaabbb', 'baab', 'baabb', 'baabbb', 'babb', 'babbb', 'bb', 'bbb', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aab', 'bab', 'bbaabbab', 'bbab']
Pos Pos Pos Pos
Counterexamples
['abaab', 'bab', 'bbaabbab', 'bbab']
Pos Neg Neg Neg
Train Epoch 0, Loss 0.7291018962860107
Train Epoch 1, Loss 0.6873347759246826
Train Epoch 2, Loss 0.6503876447677612
Accuracy at epoch 48: 0.6708984375, total training samples: 196
Early stopping at epoch 168. Loss did not improve for 10 epochs.
Generate examples Step 168, Loss 0.24168956764703672
Epoch: 49
Negative Examples
['aaabbbbb', 'aabbbb', 'abaabbbb', 'b', 'bab', 'babaabbb', 'babb', 'babbbbb', 'bb', 'bbaabbab', 'bbab', 'bbabb', 'bbabbabb', 'bbb', 'bbbab', 'bbbabb', 'bbbb', 'bbbbabbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 49: No counterexamples found, skipped.
Early stopping at epoch 176. Loss did not improve for 10 epochs.
Generate examples Step 176, Loss 0.3116247790344691
Epoch: 50
Negative Examples
['aababb', 'abb', 'abbbaab', 'b', 'bab', 'bb', 'bbbabab']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaaabb', 'aaabb', 'aab', 'aabaab', 'aabab', 'aabb', 'ab', 'abaaabb', 'abab', 'baaab', 'baaabaab', 'baab', 'babaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaabb', 'aaabb', 'aabb', 'abaaabb', 'baaab', 'baaabaab', 'baab', 'babaaab']
Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.8062750101089478
Train Epoch 1, Loss 0.7179132699966431
Train Epoch 2, Loss 0.6465557217597961
Accuracy at epoch 50: 0.97265625, total training samples: 204
Early stopping at epoch 185. Loss did not improve for 10 epochs.
Generate examples Step 185, Loss 0.2561322358506982
Epoch: 51
Negative Examples
['aaaabb', 'aaaabbab', 'aaabab', 'aab', 'aabab', 'aababbb', 'aabbaab', 'abab', 'ababaab', 'abbab', 'abbb', 'abbbab', 'b', 'baab', 'baabab', 'baabb', 'bab', 'babaabab', 'babab', 'babbbab', 'bb', 'bbaaaab', 'bbbaaab', 'bbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'ab']
Pos Pos
Counterexamples
['aaabab', 'aab', 'aabab', 'abab', 'ababaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7496626377105713
Train Epoch 1, Loss 0.6785128116607666
Train Epoch 2, Loss 0.6172513961791992
Accuracy at epoch 51: 0.693359375, total training samples: 209
Early stopping at epoch 255. Loss did not improve for 10 epochs.
Generate examples Step 255, Loss 0.23655349790351465
Epoch: 52
Negative Examples
['a', 'aabbbbb', 'ababbbbb', 'abba', 'abbbb', 'abbbbbb', 'b', 'ba', 'babbbaa', 'babbbb', 'bb', 'bba', 'bbaabb', 'bbaabbbb', 'bbabb', 'bbabba', 'bbb', 'bbbb', 'bbbbb', 'bbbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 52: No counterexamples found, skipped.
Early stopping at epoch 195. Loss did not improve for 10 epochs.
Generate examples Step 195, Loss 0.23348731722454635
Epoch: 53
Negative Examples
['aabbbbb', 'ababbbb', 'abbabb', 'abbbaa', 'abbbb', 'abbbbaa', 'b', 'ba', 'babb', 'babbabb', 'babbbabb', 'bb', 'bbaa', 'bbabaabb', 'bbabb', 'bbb', 'bbbabb', 'bbbb', 'bbbbbaab', 'bbbbbb', 'bbbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 53: No counterexamples found, skipped.
Early stopping at epoch 199. Loss did not improve for 10 epochs.
Generate examples Step 199, Loss 0.24458849824965
Epoch: 54
Negative Examples
['aabbb', 'ababbb', 'ababbbb', 'abbb', 'abbbbbbb', 'b', 'bababbb', 'babbb', 'bb', 'bbabb', 'bbb', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bab', 'babbab', 'bbab', 'bbbbabab']
Pos Pos Pos Pos
Counterexamples
['bab', 'babbab', 'bbab', 'bbbbabab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7349624633789062
Train Epoch 1, Loss 0.6515207886695862
Train Epoch 2, Loss 0.5699410438537598
Accuracy at epoch 54: 0.94921875, total training samples: 213
Early stopping at epoch 186. Loss did not improve for 10 epochs.
Generate examples Step 186, Loss 0.2466409306793927
Epoch: 55
Negative Examples
['aaaaabbb', 'aaabb', 'aaabbab', 'aababbab', 'aabb', 'aabbabb', 'aabbb', 'abaab', 'abb', 'abbaab', 'abbaabab', 'abbabb', 'b', 'baabb', 'bab', 'babab', 'bababab', 'bb', 'bbaaab', 'bbbaaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'abaaabab']
Pos Pos
Counterexamples
['abaab']
Pos
Train Epoch 0, Loss 0.7496786117553711
Train Epoch 1, Loss 0.6558147072792053
Train Epoch 2, Loss 0.5735660791397095
Accuracy at epoch 55: 0.552734375, total training samples: 214
Early stopping at epoch 198. Loss did not improve for 10 epochs.
Generate examples Step 198, Loss 0.25776003930137387
Epoch: 56
Negative Examples
['abababbb', 'abbbb', 'b', 'baaabbbb', 'baababb', 'babaabbb', 'babb', 'babbabbb', 'babbbb', 'bb', 'bbab', 'bbabb', 'bbabbb', 'bbb', 'bbbab', 'bbbabbab', 'bbbbab', 'bbbbbab', 'bbbbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabbabab', 'aabbbab', 'bab']
Pos Pos Pos
Counterexamples
['aabbabab', 'aabbbab', 'bab']
Neg Neg Neg
Train Epoch 0, Loss 0.7412425875663757
Train Epoch 1, Loss 0.641276478767395
Train Epoch 2, Loss 0.5526579022407532
Accuracy at epoch 56: 0.9091796875, total training samples: 217
Early stopping at epoch 182. Loss did not improve for 10 epochs.
Generate examples Step 182, Loss 0.27270797644156575
Epoch: 57
Negative Examples
['aaabbb', 'aababab', 'aabb', 'abb', 'abbaabab', 'abbbaaab', 'b', 'baaabb', 'baab', 'bab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabb', 'aaab', 'aaabb', 'aab', 'aabab', 'abaaaab', 'baaab', 'babaaab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aababab', 'aaaabb', 'aaabb', 'baaab', 'babaaab']
Pos Neg Neg Neg Neg
Train Epoch 0, Loss 0.7805030345916748
Train Epoch 1, Loss 0.7384809255599976
Train Epoch 2, Loss 0.7045092582702637
Accuracy at epoch 57: 0.82421875, total training samples: 222
Early stopping at epoch 201. Loss did not improve for 10 epochs.
Generate examples Step 201, Loss 0.273313512749011
Epoch: 58
Negative Examples
['abaabbbb', 'abb', 'abbabab', 'abbabb', 'abbb', 'abbbaab', 'abbbab', 'b', 'baabb', 'bab', 'bababab', 'babbab', 'babbabb', 'babbb', 'bb', 'bbaababb', 'bbabab', 'bbbabab', 'bbbbab', 'bbbbbab', 'bbbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ab', 'baababab']
Pos Pos
Counterexamples
['baababab']
Neg
Train Epoch 0, Loss 0.6962723135948181
Train Epoch 1, Loss 0.5903355479240417
Train Epoch 2, Loss 0.4918719530105591
Accuracy at epoch 58: 0.974609375, total training samples: 223
Early stopping at epoch 226. Loss did not improve for 10 epochs.
Generate examples Step 226, Loss 0.43163351970622194
Epoch: 59
Negative Examples
['aaab', 'aab', 'ab', 'b']
Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaaab', 'aaaaab', 'aaaab']
Pos Pos Pos Pos
Counterexamples
['aaab', 'aab', 'ab']
Pos Pos Pos
Train Epoch 0, Loss 0.7667638659477234
Train Epoch 1, Loss 0.6958234310150146
Train Epoch 2, Loss 0.6319099068641663
Accuracy at epoch 59: 0.849609375, total training samples: 226
Early stopping at epoch 171. Loss did not improve for 10 epochs.
Generate examples Step 171, Loss 0.24288393271177314
Epoch: 60
Negative Examples
['aaabbb', 'aabb', 'aabbabab', 'aabbbb', 'aabbbbab', 'abab', 'abbabb', 'abbabbb', 'b', 'baabb', 'baabbb', 'bab', 'babab', 'babbb', 'bb', 'bbaaaabb', 'bbaaab', 'bbaab', 'bbab', 'bbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abab']
Pos
Train Epoch 0, Loss 0.6948390007019043
Train Epoch 1, Loss 0.6136007308959961
Train Epoch 2, Loss 0.5414415001869202
Accuracy at epoch 60: 0.4482421875, total training samples: 227
Early stopping at epoch 208. Loss did not improve for 10 epochs.
Generate examples Step 208, Loss 0.2601544596076582
Epoch: 61
Negative Examples
['aabbbbbb', 'abb', 'abbbb', 'babbabb', 'babbabbb', 'babbbb', 'babbbbab', 'bb', 'bbabbab', 'bbbabaab', 'bbbabab', 'bbbabbab', 'bbbb', 'bbbbaaab', 'bbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abbb', 'b', 'baaabbbb', 'bab', 'babbab', 'bbaaabbb', 'bbaabbab', 'bbabab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbb', 'b', 'baaabbbb', 'bab', 'babbab', 'bbaaabbb', 'bbaabbab', 'bbabab']
Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.713912844657898
Train Epoch 1, Loss 0.6176148653030396
Train Epoch 2, Loss 0.5259541273117065
Accuracy at epoch 61: 0.8642578125, total training samples: 235
Early stopping at epoch 212. Loss did not improve for 10 epochs.
Generate examples Step 212, Loss 0.27170191607284994
Epoch: 62
Negative Examples
['aaaabbbb', 'ababab', 'abb', 'abbaaab', 'abbab', 'b', 'baaabb', 'baabb', 'babab', 'bb', 'bbaaaaab', 'bbaaab', 'bbbaaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabb', 'aab', 'aabaaab', 'aabaab', 'aabb', 'ab', 'abab', 'baaaabab', 'babaaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababab', 'aaaabb', 'aabb', 'baaaabab', 'babaaaab']
Pos Neg Neg Neg Neg
Train Epoch 0, Loss 0.7774161100387573
Train Epoch 1, Loss 0.7310739755630493
Train Epoch 2, Loss 0.692703902721405
Accuracy at epoch 62: 0.9541015625, total training samples: 240
Early stopping at epoch 208. Loss did not improve for 10 epochs.
Generate examples Step 208, Loss 0.252840523514451
Epoch: 63
Negative Examples
['aabb', 'abaabab', 'ababaaab', 'abbaabab', 'abbbaaab', 'b', 'baaabab', 'baab', 'baabaaab', 'baabaab', 'bab', 'bb', 'bbaaab', 'bbaab', 'bbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabaab', 'aaabab', 'aaabbab', 'aabab', 'ab', 'baaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abaabab', 'ababaaab', 'aaabbab', 'baaab']
Pos Pos Neg Neg
Train Epoch 0, Loss 0.714252233505249
Train Epoch 1, Loss 0.6932839155197144
Train Epoch 2, Loss 0.6790177822113037
Accuracy at epoch 63: 0.841796875, total training samples: 244
Early stopping at epoch 186. Loss did not improve for 10 epochs.
Generate examples Step 186, Loss 0.21659685456179043
Epoch: 64
Negative Examples
['ababbbbb', 'abbbbabb', 'ba', 'baaa', 'baabbbbb', 'bababbb', 'babb', 'babba', 'babbb', 'babbbbaa', 'bb', 'bbaab', 'bbaabbab', 'bbabb', 'bbabbbb', 'bbb', 'bbbaaa', 'bbbabbbb', 'bbbb', 'bbbbaaa', 'bbbbaab', 'bbbbbab', 'bbbbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 64: No counterexamples found, skipped.
Early stopping at epoch 200. Loss did not improve for 10 epochs.
Generate examples Step 200, Loss 0.24544366519546035
Epoch: 65
Negative Examples
['abababbb', 'abbbaabb', 'b', 'baab', 'babbaab', 'babbb', 'babbbbab', 'bbaab', 'bbaabb', 'bbab', 'bbb', 'bbbaab', 'bbbab', 'bbbabbb', 'bbbbaab', 'bbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ab', 'abbaab', 'bab', 'babab', 'bbaaab']
Pos Pos Pos Pos Pos
Counterexamples
['abbaab', 'bab', 'babab', 'bbaaab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7505873441696167
Train Epoch 1, Loss 0.6761977672576904
Train Epoch 2, Loss 0.6027560830116272
Accuracy at epoch 65: 0.95703125, total training samples: 248
Early stopping at epoch 187. Loss did not improve for 10 epochs.
Generate examples Step 187, Loss 0.23698759649662263
Epoch: 66
Negative Examples
['aab', 'aabaabab', 'aabbaab', 'aabbbbab', 'abaaaab', 'abaab', 'ababaaab', 'abb', 'abbaaaab', 'abbaab', 'abbaabab', 'b', 'baaaabab', 'baaab', 'baab', 'baabab', 'bab', 'bababaa', 'babbab', 'babbb', 'bb', 'bbaaaaab', 'bbaabab', 'bbab', 'bbabaa', 'bbabaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aab', 'aabaabab', 'abaaaab', 'abaab', 'ababaaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7882806062698364
Train Epoch 1, Loss 0.6955265402793884
Train Epoch 2, Loss 0.6199077367782593
Accuracy at epoch 66: 0.7421875, total training samples: 253
Early stopping at epoch 190. Loss did not improve for 10 epochs.
Generate examples Step 190, Loss 0.2385423650760301
Epoch: 67
Negative Examples
['ababbb', 'ababbbbb', 'abb', 'abbbb', 'b', 'baaababb', 'babb', 'babbb', 'bb', 'bbaababb', 'bbab', 'bbabab', 'bbabb', 'bbabbab', 'bbabbbb', 'bbb', 'bbbaaab', 'bbbaab', 'bbbabaab', 'bbbabab', 'bbbb', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 67: No counterexamples found, skipped.
Early stopping at epoch 195. Loss did not improve for 10 epochs.
Generate examples Step 195, Loss 0.23824282826817766
Epoch: 68
Negative Examples
['ababbbab', 'ababbbb', 'abbb', 'abbbb', 'b', 'baaabbbb', 'babababb', 'babb', 'bb', 'bbaabb', 'bbaabbb', 'bbab', 'bbb', 'bbbabb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 68: No counterexamples found, skipped.
Early stopping at epoch 180. Loss did not improve for 10 epochs.
Generate examples Step 180, Loss 0.27356325040535373
Epoch: 69
Negative Examples
['ababb', 'abb', 'abbbb', 'b', 'baabb', 'baabbaab', 'bab', 'bababbab', 'babbaab', 'babbb', 'bb', 'bbaab', 'bbab', 'bbbaab', 'bbbab', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabbab', 'ab', 'baaab', 'baab', 'bababab']
Pos Pos Pos Pos Pos
Counterexamples
['aabbab', 'baaab', 'baab', 'bababab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7378480434417725
Train Epoch 1, Loss 0.6627058982849121
Train Epoch 2, Loss 0.5905555486679077
Accuracy at epoch 69: 0.9423828125, total training samples: 257
Early stopping at epoch 166. Loss did not improve for 10 epochs.
Generate examples Step 166, Loss 0.26469000665370573
Epoch: 70
Negative Examples
['aaaaabbb', 'aaaabb', 'aaabb', 'aab', 'aabaabab', 'aabbaaab', 'aabbb', 'ab', 'abaaabb', 'abaab', 'ababb', 'abb', 'abbb', 'b', 'baaab', 'baab', 'bab', 'babaaab', 'babaabab', 'bb', 'bbaabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaabab']
Pos Pos
Counterexamples
['aab', 'aabaabab', 'ab', 'abaab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.7621210813522339
Train Epoch 1, Loss 0.6799111366271973
Train Epoch 2, Loss 0.6111835837364197
Accuracy at epoch 70: 0.6982421875, total training samples: 261
Early stopping at epoch 208. Loss did not improve for 10 epochs.
Generate examples Step 208, Loss 0.23965720689753026
Epoch: 71
Negative Examples
['ababbbb', 'abbb', 'abbbabb', 'abbbb', 'b', 'baababbb', 'baabbab', 'baabbbb', 'baabbbbb', 'babab', 'bababb', 'babbab', 'babbb', 'bb', 'bbaabb', 'bbaabbb', 'bbab', 'bbb', 'bbbabaab', 'bbbb', 'bbbbaab', 'bbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 71: No counterexamples found, skipped.
Early stopping at epoch 219. Loss did not improve for 10 epochs.
Generate examples Step 219, Loss 0.2215719336135821
Epoch: 72
Negative Examples
['a', 'ababbbab', 'b', 'ba', 'baabbb', 'babbbaab', 'bb', 'bbaaabbb', 'bbaabb', 'bbaabbb', 'bbabab', 'bbabb', 'bbb', 'bbbaa', 'bbbab', 'bbbabb', 'bbbb', 'bbbbaa', 'bbbbaabb', 'bbbbabbb', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 72: No counterexamples found, skipped.
Early stopping at epoch 177. Loss did not improve for 10 epochs.
Generate examples Step 177, Loss 0.24137959939040496
Epoch: 73
Negative Examples
['ababbbab', 'abbabab', 'abbb', 'abbbb', 'abbbbbab', 'b', 'baabb', 'bab', 'babab', 'bababb', 'babb', 'babbab', 'babbabbb', 'bbab', 'bbabbaab', 'bbabbbab', 'bbb', 'bbbaaabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 73: No counterexamples found, skipped.
Early stopping at epoch 188. Loss did not improve for 10 epochs.
Generate examples Step 188, Loss 0.2188073513369081
Epoch: 74
Negative Examples
['a', 'abbbbaa', 'abbbbb', 'b', 'ba', 'babb', 'babbb', 'babbbb', 'babbbbb', 'bb', 'bbabaa', 'bbabb', 'bbb', 'bbbaab', 'bbbaabb', 'bbbb', 'bbbbb', 'bbbbbb', 'bbbbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 74: No counterexamples found, skipped.
Early stopping at epoch 197. Loss did not improve for 10 epochs.
Generate examples Step 197, Loss 0.25346360731907563
Epoch: 75
Negative Examples
['aabbb', 'aabbbabb', 'ababb', 'abbabaab', 'abbabab', 'b', 'bab', 'babaabb', 'bb', 'bbaab', 'bbaabab', 'bbaababb', 'bbaabb', 'bbab', 'bbabaab', 'bbabaabb', 'bbb', 'bbbaaaab', 'bbbab', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabbaab']
Pos
Counterexamples
['aabbaab']
Neg
Train Epoch 0, Loss 0.8036606311798096
Train Epoch 1, Loss 0.6843125224113464
Train Epoch 2, Loss 0.598885715007782
Accuracy at epoch 75: 0.96875, total training samples: 262
Early stopping at epoch 209. Loss did not improve for 10 epochs.
Generate examples Step 209, Loss 0.25874426606155576
Epoch: 76
Negative Examples
['aabaaabb', 'aabbabab', 'aabbb', 'abaaabbb', 'abab', 'abb', 'b', 'baaab', 'baaabaab', 'bab', 'babaaab', 'babaaabb', 'babbaaab', 'bbaaabab', 'bbaab', 'bbab', 'bbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ab']
Pos
Counterexamples
['abab']
Pos
Train Epoch 0, Loss 0.7055190205574036
Train Epoch 1, Loss 0.6282159090042114
Train Epoch 2, Loss 0.5595611333847046
Accuracy at epoch 76: 0.8095703125, total training samples: 263
Early stopping at epoch 214. Loss did not improve for 10 epochs.
Generate examples Step 214, Loss 0.21371443105298418
Epoch: 77
Negative Examples
['abbaabb', 'abbbab', 'abbbabb', 'abbbb', 'baababbb', 'babbabb', 'bbaabb', 'bbabaa', 'bbabbbab', 'bbb', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['b', 'baabbb', 'babaabbb', 'babbabab', 'babbb', 'bb', 'bbaaabbb', 'bbab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['b', 'baabbb', 'babaabbb', 'babbabab', 'babbb', 'bb', 'bbaaabbb', 'bbab']
Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.71037757396698
Train Epoch 1, Loss 0.6255101561546326
Train Epoch 2, Loss 0.545280933380127
Accuracy at epoch 77: 0.9638671875, total training samples: 271
Early stopping at epoch 186. Loss did not improve for 10 epochs.
Generate examples Step 186, Loss 0.22362784348069664
Epoch: 78
Negative Examples
['aaaabbab', 'aaaabbb', 'aababaab', 'aababbab', 'aabbb', 'abaabab', 'abb', 'abbb', 'b', 'baaaaabb', 'baaaab', 'baaaabab', 'baaaabb', 'baaab', 'baabab', 'bab', 'bb', 'bbaaaab', 'bbab', 'bbb', 'bbbbaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabbb', 'baaaaaab']
Pos Pos
Counterexamples
['aababaab', 'abaabab', 'aaaaabbb', 'baaaaaab']
Pos Pos Neg Neg
Train Epoch 0, Loss 0.7856593132019043
Train Epoch 1, Loss 0.7378237247467041
Train Epoch 2, Loss 0.6997138261795044
Accuracy at epoch 78: 0.9736328125, total training samples: 275
Early stopping at epoch 133. Loss did not improve for 10 epochs.
Generate examples Step 133, Loss 0.20115791127752902
Epoch: 79
Negative Examples
['a', 'aaaa', 'aabbabba', 'ababaab', 'ababbba', 'abb', 'abbaa', 'abbabaa', 'abbbbbb', 'b', 'ba', 'babaa', 'babababa', 'babbaabb', 'babbbbbb', 'bb', 'bbaab', 'bbaabbbb', 'bbababab', 'bbb', 'bbba', 'bbbabbb', 'bbbb', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['ababaab']
Pos
Train Epoch 0, Loss 0.7544834017753601
Train Epoch 1, Loss 0.654781699180603
Train Epoch 2, Loss 0.5761934518814087
Accuracy at epoch 79: 0.521484375, total training samples: 276
Early stopping at epoch 145. Loss did not improve for 10 epochs.
Generate examples Step 145, Loss 0.23095641703638312
Epoch: 80
Negative Examples
['a', 'aabbbbb', 'abba', 'abbabb', 'abbb', 'abbbaaba', 'ba', 'baa', 'baaba', 'baabb', 'babaabb', 'bababaa', 'bababb', 'babb', 'bba', 'bbaaaa', 'bbaaba', 'bbaba', 'bbabb', 'bbabbb', 'bbba', 'bbbb', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['b', 'babababb']
Pos Pos
Counterexamples
['b', 'babababb']
Neg Neg
Train Epoch 0, Loss 0.7042068243026733
Train Epoch 1, Loss 0.6403317451477051
Train Epoch 2, Loss 0.5862229466438293
Accuracy at epoch 80: 0.912109375, total training samples: 278
Early stopping at epoch 194. Loss did not improve for 10 epochs.
Generate examples Step 194, Loss 0.2719800518109248
Epoch: 81
Negative Examples
['abbbaab', 'b', 'bb', 'bbaab', 'bbabbab', 'bbbaab', 'bbbabaab', 'bbbbbaab', 'bbbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aa', 'aaab', 'aab', 'aabbbbab', 'abaabab', 'abbabaab', 'abbabab', 'baaaab', 'baab', 'bab', 'babab', 'bbaaab', 'bbab', 'bbbabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aa', 'aabbbbab', 'abbabaab', 'abbabab', 'baaaab', 'baab', 'bab', 'babab', 'bbaaab', 'bbab', 'bbbabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7285932302474976
Train Epoch 1, Loss 0.6558176875114441
Train Epoch 2, Loss 0.5839729905128479
Accuracy at epoch 81: 0.9716796875, total training samples: 289
Early stopping at epoch 152. Loss did not improve for 10 epochs.
Generate examples Step 152, Loss 0.2381481412384245
Epoch: 82
Negative Examples
['a', 'aaaa', 'aab', 'aabbaa', 'aabbaaa', 'abaabaa', 'abb', 'abbb', 'abbbbbaa', 'b', 'ba', 'baaaab', 'baabaab', 'babaaa', 'bababbaa', 'babbaaa', 'babbbaa', 'bb', 'bba', 'bbaaab', 'bbbaabaa', 'bbbab', 'bbbabbab', 'bbbbaaaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aab']
Pos
Train Epoch 0, Loss 0.7646946907043457
Train Epoch 1, Loss 0.6867285966873169
Train Epoch 2, Loss 0.6147549748420715
Accuracy at epoch 82: 0.93359375, total training samples: 290
Early stopping at epoch 235. Loss did not improve for 10 epochs.
Generate examples Step 235, Loss 0.33524771148370486
Epoch: 83
Negative Examples
['b']
Neg
Positive Examples
['aaaaaabb', 'aaaaab', 'aaaab', 'aaab', 'aaabaaab', 'aab', 'aabaaab', 'aabab', 'ab', 'abaaaab', 'baaaab', 'baaab', 'baab', 'bab', 'bbaaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaaabb', 'baaaab', 'baaab', 'baab', 'bab', 'bbaaaab']
Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7480552196502686
Train Epoch 1, Loss 0.6639366149902344
Train Epoch 2, Loss 0.5832080841064453
Accuracy at epoch 83: 0.9658203125, total training samples: 296
Early stopping at epoch 196. Loss did not improve for 10 epochs.
Generate examples Step 196, Loss 0.29961118147457916
Epoch: 84
Negative Examples
['aaaaab', 'aaaab', 'aaab', 'aaabab', 'aab', 'aabaaaab', 'aabaab', 'ab', 'abab', 'b', 'baaab', 'bab', 'babbaab', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaaab']
Pos Pos
Counterexamples
['aaaaab', 'aaaab', 'aaab', 'aaabab', 'aab', 'aabaaaab', 'aabaab', 'ab', 'abab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.784726083278656
Train Epoch 1, Loss 0.6897942423820496
Train Epoch 2, Loss 0.613432765007019
Accuracy at epoch 84: 0.9150390625, total training samples: 305
Early stopping at epoch 199. Loss did not improve for 10 epochs.
Generate examples Step 199, Loss 0.2648096667230129
Epoch: 85
Negative Examples
['aababb', 'aabbabab', 'aabbb', 'ababab', 'abb', 'abbaaab', 'abbaab', 'abbab', 'b', 'baaabb', 'baabaab', 'babaaab', 'babab', 'babbbaab', 'bb', 'bbaaaab', 'bbbbabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabaab', 'ab', 'ababaab', 'baab', 'bab']
Pos Pos Pos Pos Pos
Counterexamples
['ababab', 'baab', 'bab']
Pos Neg Neg
Train Epoch 0, Loss 0.7211790680885315
Train Epoch 1, Loss 0.6879299283027649
Train Epoch 2, Loss 0.6578037142753601
Accuracy at epoch 85: 0.6103515625, total training samples: 308
Early stopping at epoch 215. Loss did not improve for 10 epochs.
Generate examples Step 215, Loss 0.2309843504594432
Epoch: 86
Negative Examples
['a', 'ba', 'bababbbb', 'babbbba', 'bb', 'bbabbaa', 'bbabbaab', 'bbb', 'bbba', 'bbbaabba', 'bbbabab', 'bbbabb', 'bbbabba', 'bbbba', 'bbbbaa', 'bbbbaaaa', 'bbbbaab', 'bbbbab', 'bbbbb', 'bbbbba', 'bbbbbabb', 'bbbbbbb', 'bbbbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 86: No counterexamples found, skipped.
Early stopping at epoch 210. Loss did not improve for 10 epochs.
Generate examples Step 210, Loss 0.24008693542525666
Epoch: 87
Negative Examples
['a', 'aabbbbbb', 'b', 'ba', 'bababb', 'babbabb', 'babbbbb', 'bb', 'bba', 'bbabbb', 'bbba', 'bbbaaab', 'bbbaab', 'bbbabba', 'bbbabbaa', 'bbbb', 'bbbba', 'bbbbabab', 'bbbbb', 'bbbbbaa', 'bbbbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 87: No counterexamples found, skipped.
Early stopping at epoch 224. Loss did not improve for 10 epochs.
Generate examples Step 224, Loss 0.27529809051089815
Epoch: 88
Negative Examples
['ababbbab', 'abbabbab', 'abbabbb', 'abbbab', 'abbbbaab', 'abbbbab', 'b', 'bababbab', 'babbab', 'babbbb', 'bb', 'bbaab', 'bbab', 'bbabab', 'bbabbaab', 'bbabbb', 'bbb', 'bbbaaab', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bab']
Pos
Counterexamples
['bab']
Neg
Train Epoch 0, Loss 0.7304767370223999
Train Epoch 1, Loss 0.6627495884895325
Train Epoch 2, Loss 0.5991281270980835
Accuracy at epoch 88: 0.7861328125, total training samples: 309
Early stopping at epoch 171. Loss did not improve for 10 epochs.
Generate examples Step 171, Loss 0.27218081854110543
Epoch: 89
Negative Examples
['aaabbbb', 'aabbab', 'aabbb', 'abab', 'abb', 'abbb', 'abbbab', 'b', 'baaabbb', 'baabab', 'baabbbab', 'bab', 'babab', 'babbb', 'bb', 'bbaaabab', 'bbb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abab']
Pos
Train Epoch 0, Loss 0.7227752208709717
Train Epoch 1, Loss 0.6404202580451965
Train Epoch 2, Loss 0.5642085075378418
Accuracy at epoch 89: 0.4921875, total training samples: 310
Early stopping at epoch 218. Loss did not improve for 10 epochs.
Generate examples Step 218, Loss 0.25576143334173174
Epoch: 90
Negative Examples
['abbabbb', 'abbbbb', 'b', 'bababb', 'babbbb', 'babbbbb', 'bb', 'bbaabb', 'bbab', 'bbabb', 'bbb', 'bbbab', 'bbbabbbb', 'bbbb', 'bbbbabb', 'bbbbb', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 90: No counterexamples found, skipped.
Early stopping at epoch 235. Loss did not improve for 10 epochs.
Generate examples Step 235, Loss 0.2607380510386774
Epoch: 91
Negative Examples
['abbababb', 'abbabb', 'abbabbb', 'abbbbbb', 'b', 'babababb', 'babbabab', 'babbbab', 'bb', 'bbabb', 'bbabbb', 'bbabbbab', 'bbb', 'bbbb', 'bbbbabbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 91: No counterexamples found, skipped.
Early stopping at epoch 202. Loss did not improve for 10 epochs.
Generate examples Step 202, Loss 0.275594820676766
Epoch: 92
Negative Examples
['aabbbb', 'ababbbbb', 'abbbabb', 'abbbb', 'b', 'bab', 'babbbbb', 'bb', 'bbab', 'bbabab', 'bbabbb', 'bbabbbbb', 'bbb', 'bbbab', 'bbbb', 'bbbbab', 'bbbbabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abbaabbb']
Pos
Counterexamples
['abbaabbb']
Neg
Train Epoch 0, Loss 0.7349303960800171
Train Epoch 1, Loss 0.5861034393310547
Train Epoch 2, Loss 0.4804885983467102
Accuracy at epoch 92: 0.849609375, total training samples: 311
Early stopping at epoch 188. Loss did not improve for 10 epochs.
Generate examples Step 188, Loss 0.23818622947369933
Epoch: 93
Negative Examples
['aabbb', 'aabbbaab', 'abb', 'abba', 'b', 'baaabb', 'baaabbb', 'baababbb', 'baabb', 'babaab', 'babb', 'babbabab', 'bb', 'bbaa', 'bbaaaa', 'bbaab', 'bbab', 'bbabaaaa', 'bbabaaab', 'bbabab', 'bbb', 'bbbaaaab', 'bbbabaa', 'bbbbabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 93: No counterexamples found, skipped.
Early stopping at epoch 171. Loss did not improve for 10 epochs.
Generate examples Step 171, Loss 0.24515261111217876
Epoch: 94
Negative Examples
['aabb', 'ababaabb', 'abbaa', 'abbaaaa', 'abbaabab', 'abbab', 'abbabaab', 'b', 'baaabaab', 'baab', 'bab', 'babb', 'bb', 'bbaa', 'bbaaaa', 'bbaaaaa', 'bbaab', 'bbab', 'bbabaa', 'bbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baaabab']
Pos
Counterexamples
['baaabab']
Neg
Train Epoch 0, Loss 0.6962493062019348
Train Epoch 1, Loss 0.5576887130737305
Train Epoch 2, Loss 0.439162015914917
Accuracy at epoch 94: 0.943359375, total training samples: 312
Early stopping at epoch 192. Loss did not improve for 10 epochs.
Generate examples Step 192, Loss 0.34815938948349634
Epoch: 95
Negative Examples
['a', 'aa', 'aaa', 'aaab', 'aab', 'b', 'bbbbbaab']
Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaa', 'aaaaa', 'aaaaaaa', 'aaaaaab', 'aaaaaabb', 'aaaaab', 'aaaaabab', 'aaaab', 'aaaabab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaab', 'aab', 'aaaa', 'aaaaa', 'aaaaaaa', 'aaaaaabb']
Pos Pos Neg Neg Neg Neg
Train Epoch 0, Loss 0.7819840312004089
Train Epoch 1, Loss 0.7250166535377502
Train Epoch 2, Loss 0.6843798756599426
Accuracy at epoch 95: 0.96875, total training samples: 318
Early stopping at epoch 170. Loss did not improve for 10 epochs.
Generate examples Step 170, Loss 0.2315034731264003
Epoch: 96
Negative Examples
['a', 'aa', 'aaa', 'aaaa', 'aaaaab', 'aaabaaab', 'aabaa', 'aabbaa', 'aabbabab', 'ab', 'abaa', 'abab', 'ababaaa', 'ba', 'baaaa', 'baaab', 'baabaaa', 'bab', 'babaaa', 'babaaaa', 'bb', 'bbaaa', 'bbab', 'bbbbaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaaaab', 'aaabaaab', 'ab', 'abab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.8151131868362427
Train Epoch 1, Loss 0.7204752564430237
Train Epoch 2, Loss 0.6468426585197449
Accuracy at epoch 96: 0.98046875, total training samples: 322
Early stopping at epoch 175. Loss did not improve for 10 epochs.
Generate examples Step 175, Loss 0.22513602961870757
Epoch: 97
Negative Examples
['aaa', 'aababbb', 'aabb', 'aabbaab', 'aabbab', 'abaa', 'abb', 'abbaaaa', 'abbb', 'abbbaab', 'b', 'baaa', 'baabb', 'bababaa', 'babbab', 'babbbaab', 'bb', 'bbaab', 'bbaabaa', 'bbabaaab', 'bbabbaab', 'bbb', 'bbbaaab', 'bbbab', 'bbbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aab', 'ababab', 'baab']
Pos Pos Pos
Counterexamples
['baab']
Neg
Train Epoch 0, Loss 0.7052764892578125
Train Epoch 1, Loss 0.6283688545227051
Train Epoch 2, Loss 0.5549778938293457
Accuracy at epoch 97: 0.9814453125, total training samples: 323
Early stopping at epoch 164. Loss did not improve for 10 epochs.
Generate examples Step 164, Loss 0.21093534222154905
Epoch: 98
Negative Examples
['a', 'aaaaba', 'aaabbab', 'aaba', 'aabbaa', 'aabbab', 'aabbbaab', 'aabbbbab', 'abaaa', 'abaab', 'abaabab', 'abb', 'abbab', 'abbbaab', 'b', 'ba', 'baa', 'baaaaa', 'bab', 'babaa', 'bababaab', 'bb', 'bbaa', 'bbbab', 'bbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaab', 'abaabab']
Pos Pos
Train Epoch 0, Loss 0.9843512177467346
Train Epoch 1, Loss 0.8581948280334473
Train Epoch 2, Loss 0.7601847648620605
Accuracy at epoch 98: 0.96484375, total training samples: 325
Early stopping at epoch 210. Loss did not improve for 10 epochs.
Generate examples Step 210, Loss 0.2807678461639802
Epoch: 99
Negative Examples
['abbab', 'b', 'baab', 'baabaab', 'babaab', 'bababaab', 'babbaaab', 'bb', 'bbabab', 'bbbaaab', 'bbbbabab', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aab', 'ab', 'abaabab', 'abab', 'ababab', 'baaaaab', 'baaaab', 'bab', 'babab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['baaaaab', 'baaaab', 'bab', 'babab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7169946432113647
Train Epoch 1, Loss 0.6495559811592102
Train Epoch 2, Loss 0.5787486433982849
Accuracy at epoch 99: 0.962890625, total training samples: 329
Early stopping at epoch 142. Loss did not improve for 10 epochs.
Generate examples Step 142, Loss 0.23459138403405677
Epoch: 100
Negative Examples
['a', 'aaaaaabb', 'aabaaa', 'aabbbaaa', 'aabbbb', 'abaab', 'abb', 'abbaaa', 'abbaaaa', 'abbb', 'b', 'ba', 'baaaaaa', 'baaabbab', 'baabbaa', 'baabbab', 'babaa', 'babb', 'bb', 'bbaa', 'bbaabaa', 'bbab', 'bbbaaa', 'bbbbaa', 'bbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['abaab']
Pos
Train Epoch 0, Loss 0.9357922077178955
Train Epoch 1, Loss 0.8330994248390198
Train Epoch 2, Loss 0.7515382170677185
Accuracy at epoch 100: 0.9794921875, total training samples: 330
Early stopping at epoch 181. Loss did not improve for 10 epochs.
Generate examples Step 181, Loss 0.23483035795308732
Epoch: 101
Negative Examples
['aabbabb', 'ababb', 'abb', 'abbbaaa', 'b', 'baaabbb', 'baabb', 'bababbab', 'babb', 'babbb', 'bb', 'bbaaab', 'bbaaabb', 'bbaab', 'bbabbaa', 'bbabbb', 'bbb', 'bbbaa', 'bbbab', 'bbbb', 'bbbbab', 'bbbbabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 101: No counterexamples found, skipped.
Early stopping at epoch 178. Loss did not improve for 10 epochs.
Generate examples Step 178, Loss 0.23734016258623347
Epoch: 102
Negative Examples
['aababbbb', 'abababbb', 'abb', 'abbaab', 'abbabbab', 'abbbab', 'abbbbb', 'b', 'baab', 'baabb', 'baabbbb', 'bab', 'bababb', 'babbab', 'babbb', 'bb', 'bbaaaabb', 'bbaaab', 'bbaab', 'bbab', 'bbababab', 'bbabb', 'bbb', 'bbbabb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 102: No counterexamples found, skipped.
Early stopping at epoch 181. Loss did not improve for 10 epochs.
Generate examples Step 181, Loss 0.25770077453209805
Epoch: 103
Negative Examples
['aaabbabb', 'aababbbb', 'aabb', 'aabbab', 'aabbbab', 'abb', 'abbab', 'b', 'baab', 'baabab', 'baababab', 'baabbb', 'bab', 'babaab', 'babaabb', 'babbab', 'babbb', 'bb', 'bbaaaabb', 'bbaaab', 'bbab', 'bbabab', 'bbabbab', 'bbb', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ab']
Pos
Counterexamples
[]

Round 103: No counterexamples found, skipped.
Early stopping at epoch 173. Loss did not improve for 10 epochs.
Generate examples Step 173, Loss 0.2365450314406691
Epoch: 104
Negative Examples
['abb', 'abbab', 'abbbbabb', 'b', 'baab', 'baabaa', 'baabab', 'baababb', 'baabbabb', 'bab', 'bababaab', 'bababb', 'babb', 'babbbaa', 'bb', 'bbaaaa', 'bbaabb', 'bbbaa', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 104: No counterexamples found, skipped.
Early stopping at epoch 192. Loss did not improve for 10 epochs.
Generate examples Step 192, Loss 0.25950827329887627
Epoch: 105
Negative Examples
['abb', 'abbab', 'abbb', 'abbbbaab', 'b', 'baaab', 'baabab', 'bab', 'babab', 'babb', 'bb', 'bbaaabab', 'bbab', 'bbabaab', 'bbb', 'bbbaaab', 'bbbaab', 'bbbbaab', 'bbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaababab', 'aab', 'ab']
Pos Pos Pos
Counterexamples
[]

Round 105: No counterexamples found, skipped.
Early stopping at epoch 216. Loss did not improve for 10 epochs.
Generate examples Step 216, Loss 0.24982613524258962
Epoch: 106
Negative Examples
['ababbab', 'abb', 'abbab', 'abbabb', 'abbbaaab', 'abbbab', 'b', 'baaabaab', 'baaabb', 'baabab', 'baabbab', 'bab', 'babaab', 'bababab', 'babb', 'babbab', 'babbb', 'bbaab', 'bbaabab', 'bbab', 'bbb', 'bbbabbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ab', 'abab']
Pos Pos
Counterexamples
[]

Round 106: No counterexamples found, skipped.
Early stopping at epoch 181. Loss did not improve for 10 epochs.
Generate examples Step 181, Loss 0.23341828756607497
Epoch: 107
Negative Examples
['abaaabbb', 'ababbbab', 'abb', 'abbaaab', 'abbab', 'b', 'baababb', 'baabbabb', 'bab', 'bababbbb', 'babb', 'babbb', 'babbbbab', 'bb', 'bbaaaab', 'bbaabbab', 'bbabaab', 'bbababab', 'bbbabbaa', 'bbbb', 'bbbbaaa', 'bbbbabb', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 107: No counterexamples found, skipped.
Early stopping at epoch 152. Loss did not improve for 10 epochs.
Generate examples Step 152, Loss 0.22410832326007046
Epoch: 108
Negative Examples
['a', 'aaabbbaa', 'ababba', 'ababbaab', 'abb', 'abbba', 'abbbaab', 'abbbbaaa', 'ba', 'baa', 'baaa', 'baabba', 'baabbaab', 'babaab', 'babaaba', 'babaabab', 'babba', 'babbaba', 'babbbb', 'bb', 'bbaaaaab', 'bbaabaab', 'bbaba', 'bbabb', 'bbb', 'bbbaa', 'bbbab', 'bbbaba', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 108: No counterexamples found, skipped.
Early stopping at epoch 166. Loss did not improve for 10 epochs.
Generate examples Step 166, Loss 0.2329194776847691
Epoch: 109
Negative Examples
['aabababb', 'aababbbb', 'ababb', 'abb', 'abbabbab', 'abbbabaa', 'abbbabb', 'abbbbba', 'b', 'baabaaa', 'baabbbbb', 'bab', 'babaa', 'babaabb', 'bababb', 'babb', 'bb', 'bbaaab', 'bbab', 'bbbabb', 'bbbb', 'bbbbabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 109: No counterexamples found, skipped.
Early stopping at epoch 153. Loss did not improve for 10 epochs.
Generate examples Step 153, Loss 0.22904843420951398
Epoch: 110
Negative Examples
['a', 'aabbabb', 'aabbbbb', 'abaaaba', 'abb', 'b', 'ba', 'baaa', 'baabb', 'baabbb', 'babb', 'babbbba', 'bb', 'bbaa', 'bbaaaaa', 'bbaabb', 'bbabbaaa', 'bbabbba', 'bbabbbaa', 'bbb', 'bbbaa', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 110: No counterexamples found, skipped.
Early stopping at epoch 171. Loss did not improve for 10 epochs.
Generate examples Step 171, Loss 0.25082125090235885
Epoch: 111
Negative Examples
['ababbb', 'abb', 'abbaaaab', 'abbb', 'abbbb', 'abbbbab', 'b', 'baaaabb', 'baaab', 'baaabab', 'baab', 'baabaaab', 'baabaabb', 'bab', 'babb', 'bb', 'bbaaab', 'bbaab', 'bbab', 'bbabbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 111: No counterexamples found, skipped.
Early stopping at epoch 188. Loss did not improve for 10 epochs.
Generate examples Step 188, Loss 0.25086428430029956
Epoch: 112
Negative Examples
['aabbb', 'ababaab', 'ababab', 'ababbab', 'abbaabb', 'abbab', 'b', 'baaabab', 'baaababb', 'baab', 'baababb', 'baabbab', 'bab', 'babaabb', 'bababab', 'babb', 'bb', 'bbaabaab', 'bbab', 'bbababab', 'bbabbaab', 'bbbab', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ab']
Pos
Counterexamples
['ababaab', 'ababab']
Pos Pos
Train Epoch 0, Loss 0.7244065999984741
Train Epoch 1, Loss 0.6632291674613953
Train Epoch 2, Loss 0.6191239953041077
Accuracy at epoch 112: 0.55078125, total training samples: 332
Early stopping at epoch 186. Loss did not improve for 10 epochs.
Generate examples Step 186, Loss 0.21855348691264576
Epoch: 113
Negative Examples
['abbbbb', 'b', 'babb', 'bb', 'bbaa', 'bbabb', 'bbabbb', 'bbbaaabb', 'bbbaabb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ababbbb', 'bab', 'babab', 'bababbaa', 'babbaab', 'babbabaa', 'babbbaa', 'bbaabbab', 'bbab', 'bbabbab', 'bbbb', 'bbbbaa', 'bbbbaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['ababbbb', 'bab', 'babab', 'bababbaa', 'babbaab', 'babbabaa', 'babbbaa', 'bbaabbab', 'bbab', 'bbabbab', 'bbbb', 'bbbbaa', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.774640679359436
Train Epoch 1, Loss 0.6890431642532349
Train Epoch 2, Loss 0.6219794750213623
Accuracy at epoch 113: 0.86328125, total training samples: 345
Early stopping at epoch 166. Loss did not improve for 10 epochs.
Generate examples Step 166, Loss 0.25631769492240725
Epoch: 114
Negative Examples
['a', 'abb', 'abbb', 'abbbbaa', 'b', 'ba', 'baa', 'baabb', 'baabbbab', 'babb', 'babbb', 'babbbabb', 'babbbbb', 'bb', 'bbaabbb', 'bbabab', 'bbabb', 'bbabbbb', 'bbb', 'bbbaaaa', 'bbbabaa', 'bbbabab', 'bbbbaaa', 'bbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 114: No counterexamples found, skipped.
Early stopping at epoch 198. Loss did not improve for 10 epochs.
Generate examples Step 198, Loss 0.2957930335746938
Epoch: 115
Negative Examples
['aaaaaaba', 'aabbbab', 'abaabbab', 'abababb', 'ababbab', 'abbabbab', 'b', 'baaabb', 'baabb', 'bab', 'babab', 'bbaab', 'bbab', 'bbabab', 'bbbaaaab', 'bbbaaab', 'bbbab', 'bbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaa', 'aab', 'ab', 'baaab', 'baabab']
Pos Pos Pos Pos Pos
Counterexamples
['aaaa', 'baaab', 'baabab']
Neg Neg Neg
Train Epoch 0, Loss 0.7315929532051086
Train Epoch 1, Loss 0.6749681830406189
Train Epoch 2, Loss 0.629523515701294
Accuracy at epoch 115: 0.95703125, total training samples: 348
Early stopping at epoch 190. Loss did not improve for 10 epochs.
Generate examples Step 190, Loss 0.27058901454453693
Epoch: 116
Negative Examples
['aab', 'aabbab', 'ab', 'ababab', 'abbb', 'abbbb', 'b', 'baaabb', 'baab', 'babaab', 'babb', 'babbab', 'bb', 'bbaab', 'bbab', 'bbabaaab', 'bbabb', 'bbb', 'bbbaaaab', 'bbbab', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aab', 'ab', 'ababab']
Pos Pos Pos
Train Epoch 0, Loss 0.7476205825805664
Train Epoch 1, Loss 0.6958122253417969
Train Epoch 2, Loss 0.6472215056419373
Accuracy at epoch 116: 0.8671875, total training samples: 351
Early stopping at epoch 172. Loss did not improve for 10 epochs.
Generate examples Step 172, Loss 0.2495359557040165
Epoch: 117
Negative Examples
['ababbbb', 'abb', 'abbbb', 'abbbbb', 'b', 'baaababb', 'babb', 'bb', 'bbaabb', 'bbabb', 'bbbabbb', 'bbbb', 'bbbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abbbaaab', 'baab', 'bab', 'babab', 'bbaab']
Pos Pos Pos Pos Pos
Counterexamples
['abbbaaab', 'baab', 'bab', 'babab', 'bbaab']
Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7606183290481567
Train Epoch 1, Loss 0.6892277002334595
Train Epoch 2, Loss 0.62410968542099
Accuracy at epoch 117: 0.9755859375, total training samples: 356
Early stopping at epoch 187. Loss did not improve for 10 epochs.
Generate examples Step 187, Loss 0.2591033776865361
Epoch: 118
Negative Examples
['aabb', 'aabbbbab', 'abaab', 'abab', 'ababb', 'abbab', 'abbbaab', 'abbbbab', 'b', 'baaaaab', 'baaab', 'baab', 'bab', 'babab', 'babbab', 'bb', 'bbaaaaab', 'bbaaab', 'bbaaabab', 'bbabaab', 'bbabab', 'bbabbaab', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aab', 'ab']
Pos Pos
Counterexamples
['abaab', 'abab']
Pos Pos
Train Epoch 0, Loss 0.7572137117385864
Train Epoch 1, Loss 0.6936259269714355
Train Epoch 2, Loss 0.6406984329223633
Accuracy at epoch 118: 0.8486328125, total training samples: 358
Early stopping at epoch 191. Loss did not improve for 10 epochs.
Generate examples Step 191, Loss 0.23486369693030915
Epoch: 119
Negative Examples
['ababbbab', 'abbaabbb', 'abbbb', 'b', 'babbbb', 'bb', 'bbaabbb', 'bbabb', 'bbabbb', 'bbb', 'bbbaaaab', 'bbbaab', 'bbbab', 'bbbabbab', 'bbbb', 'bbbbab', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['baabbab', 'bab', 'babab', 'bbab']
Pos Pos Pos Pos
Counterexamples
['baabbab', 'bab', 'babab', 'bbab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.7243764400482178
Train Epoch 1, Loss 0.6499723196029663
Train Epoch 2, Loss 0.5770702958106995
Accuracy at epoch 119: 0.962890625, total training samples: 362
Early stopping at epoch 184. Loss did not improve for 10 epochs.
Generate examples Step 184, Loss 0.2723413199991793
Epoch: 120
Negative Examples
['aaabbb', 'aabb', 'aabbaaab', 'abaab', 'abaabaab', 'abaabb', 'ababb', 'abb', 'abbaaab', 'abbab', 'b', 'baaabaab', 'baab', 'bab', 'babab', 'bb', 'bbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabb', 'aaabaaab', 'aabab', 'aababaab', 'ab', 'abaaaab']
Pos Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'abaabaab', 'aaaabb']
Pos Pos Neg
Train Epoch 0, Loss 0.7223338484764099
Train Epoch 1, Loss 0.6684474349021912
Train Epoch 2, Loss 0.6226410269737244
Accuracy at epoch 120: 0.927734375, total training samples: 365
Early stopping at epoch 189. Loss did not improve for 10 epochs.
Generate examples Step 189, Loss 0.26575599665704525
Epoch: 121
Negative Examples
['ababb', 'abb', 'abbbb', 'b', 'bababb', 'babb', 'babbabb', 'bb', 'bbaaab', 'bbaaabab', 'bbaabab', 'bbaabbab', 'bbab', 'bbbaa', 'bbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aa', 'baab', 'bababaab', 'bbaab']
Pos Pos Pos Pos
Counterexamples
['aa', 'baab', 'bababaab', 'bbaab']
Neg Neg Neg Neg
Train Epoch 0, Loss 0.717934787273407
Train Epoch 1, Loss 0.6476281881332397
Train Epoch 2, Loss 0.583276629447937
Accuracy at epoch 121: 0.9658203125, total training samples: 369
Early stopping at epoch 192. Loss did not improve for 10 epochs.
Generate examples Step 192, Loss 0.2755366302833656
Epoch: 122
Negative Examples
['aaabb', 'aaabbbab', 'aabab', 'aabbbaab', 'ab', 'ababb', 'ababbab', 'abbaab', 'b', 'baaaab', 'baaab', 'baab', 'bab', 'babab', 'babbaaab', 'babbabab', 'bb', 'bbaaab', 'bbaab', 'bbaabab', 'bbbaaaab', 'bbbbaaab', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aabab', 'ab']
Pos Pos
Train Epoch 0, Loss 0.7311300039291382
Train Epoch 1, Loss 0.6635100841522217
Train Epoch 2, Loss 0.601046621799469
Accuracy at epoch 122: 0.8515625, total training samples: 371
Early stopping at epoch 197. Loss did not improve for 10 epochs.
Generate examples Step 197, Loss 0.28705463761633093
Epoch: 123
Negative Examples
['aabbbbab', 'abaaabbb', 'ababab', 'ababbbab', 'abbbaaab', 'b', 'baab', 'bab', 'babaab', 'babaabab', 'babab', 'babbab', 'bb', 'bbaabab', 'bbab', 'bbabaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ab', 'abaaab', 'abaabbab', 'abab', 'abbab']
Pos Pos Pos Pos Pos
Counterexamples
['ababab', 'abaabbab', 'abbab']
Pos Neg Neg
Train Epoch 0, Loss 0.7368637919425964
Train Epoch 1, Loss 0.6968533992767334
Train Epoch 2, Loss 0.6698231101036072
Accuracy at epoch 123: 0.9375, total training samples: 374
Early stopping at epoch 192. Loss did not improve for 10 epochs.
Generate examples Step 192, Loss 0.26407071222295414
Epoch: 124
Negative Examples
['aaababb', 'aaabb', 'aaabbb', 'aabb', 'aabbab', 'abbaaab', 'abbaab', 'abbb', 'b', 'baaabab', 'baab', 'babab', 'bb', 'bbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabbab', 'abaabab', 'abab']
Pos Pos Pos
Counterexamples
['aaabbab']
Neg
Train Epoch 0, Loss 0.7347567081451416
Train Epoch 1, Loss 0.5994160771369934
Train Epoch 2, Loss 0.4901908040046692
Accuracy at epoch 124: 0.9580078125, total training samples: 375
Early stopping at epoch 145. Loss did not improve for 10 epochs.
Generate examples Step 145, Loss 0.2713414079522433
Epoch: 125
Negative Examples
['a', 'aa', 'aaabaab', 'aaabb', 'aabaaa', 'aabaab', 'aabab', 'aabb', 'ab', 'abaaaaa', 'abb', 'b', 'baa', 'baaa', 'bab', 'babaa', 'bb', 'bbaaaab', 'bbbaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaa', 'aaaaaa', 'aaab']
Pos Pos Pos
Counterexamples
['aaabaab', 'aabaab', 'aabab', 'ab', 'aaaaa', 'aaaaaa']
Pos Pos Pos Pos Neg Neg
Train Epoch 0, Loss 0.7961094379425049
Train Epoch 1, Loss 0.7535552382469177
Train Epoch 2, Loss 0.7170090675354004
Accuracy at epoch 125: 0.9775390625, total training samples: 381
Early stopping at epoch 223. Loss did not improve for 10 epochs.
Generate examples Step 223, Loss 0.36056218123329536
Epoch: 126
Negative Examples
['aab', 'aabaaab', 'aabab', 'aababab', 'abaabab', 'abab', 'ababab', 'abbaaaab', 'abbaaab', 'b', 'baaaab', 'baaab', 'baaabab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaab', 'aaaab', 'aaab', 'ab']
Pos Pos Pos Pos
Counterexamples
['aab', 'aabaaab', 'aabab', 'aababab', 'abaabab', 'abab', 'ababab']
Pos Pos Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7542363405227661
Train Epoch 1, Loss 0.6788926124572754
Train Epoch 2, Loss 0.6156948804855347
Accuracy at epoch 126: 0.8759765625, total training samples: 388
Early stopping at epoch 186. Loss did not improve for 10 epochs.
Generate examples Step 186, Loss 0.2281125323338942
Epoch: 127
Negative Examples
['abbb', 'abbbaa', 'abbbbaa', 'abbbbb', 'b', 'babaaabb', 'babaabb', 'babb', 'babbb', 'bb', 'bbaa', 'bbaaaabb', 'bbabaa', 'bbabaabb', 'bbababb', 'bbabb', 'bbb', 'bbbaaa', 'bbbb', 'bbbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 127: No counterexamples found, skipped.
Early stopping at epoch 186. Loss did not improve for 10 epochs.
Generate examples Step 186, Loss 0.23417966067790985
Epoch: 128
Negative Examples
['a', 'aabbbbb', 'abb', 'abba', 'abbb', 'abbbbbb', 'b', 'ba', 'baa', 'baabb', 'baabbaab', 'baabbbbb', 'bababbb', 'babba', 'babbbaab', 'bb', 'bbaaa', 'bbb', 'bbbabaab', 'bbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['abbaab']
Pos
Counterexamples
['abbaab']
Neg
Train Epoch 0, Loss 0.6992999911308289
Train Epoch 1, Loss 0.6278771162033081
Train Epoch 2, Loss 0.5591011047363281
Accuracy at epoch 128: 0.986328125, total training samples: 389
Early stopping at epoch 158. Loss did not improve for 10 epochs.
Generate examples Step 158, Loss 0.2383905235888823
Epoch: 129
Negative Examples
['a', 'aaaabbaa', 'aaabaab', 'aabaabb', 'abaa', 'abaaab', 'ababaa', 'ababbbb', 'abb', 'abbaab', 'abbb', 'b', 'baaabb', 'baabaa', 'baabb', 'bab', 'babaabbb', 'babb', 'babbbaa', 'babbbb', 'bb', 'bbabbb', 'bbb', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['aaabaab', 'abaaab']
Pos Pos
Train Epoch 0, Loss 0.758522629737854
Train Epoch 1, Loss 0.6813302040100098
Train Epoch 2, Loss 0.6157739162445068
Accuracy at epoch 129: 0.875, total training samples: 391
Early stopping at epoch 211. Loss did not improve for 10 epochs.
Generate examples Step 211, Loss 0.27357104041104047
Epoch: 130
Negative Examples
['aabbbbb', 'ababbaab', 'abbb', 'abbbaaab', 'abbbb', 'b', 'baaabbb', 'babbab', 'babbb', 'bb', 'bbb', 'bbbaaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaabbb', 'ababab', 'abbabab', 'baaabbab', 'bab', 'babbaaab', 'bbaabab', 'bbabab']
Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaabbb', 'abbabab', 'baaabbab', 'bab', 'babbaaab', 'bbaabab', 'bbabab']
Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7315519452095032
Train Epoch 1, Loss 0.6435642838478088
Train Epoch 2, Loss 0.5585948824882507
Accuracy at epoch 130: 0.9755859375, total training samples: 398
Early stopping at epoch 189. Loss did not improve for 10 epochs.
Generate examples Step 189, Loss 0.23280177869294819
Epoch: 131
Negative Examples
['aabaabaa', 'aabb', 'aabbb', 'abaaaabb', 'abaaabb', 'abaabb', 'ababaab', 'ababbb', 'abbaabb', 'abbabbb', 'b', 'baaabb', 'baabb', 'bab', 'babab', 'bababab', 'bb', 'bbaaabb', 'bbababab', 'bbb', 'bbbaabab', 'bbbabaab', 'bbbabab', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
['ababaab']
Pos
Train Epoch 0, Loss 0.9335235357284546
Train Epoch 1, Loss 0.7902389764785767
Train Epoch 2, Loss 0.6789364814758301
Accuracy at epoch 131: 0.8916015625, total training samples: 399
Early stopping at epoch 217. Loss did not improve for 10 epochs.
Generate examples Step 217, Loss 0.3514931509527591
Epoch: 132
Negative Examples
['b', 'bbaab']
Neg Neg
Positive Examples
['aaaab', 'aaaabb', 'aaab', 'aab', 'aababab', 'aabbaaab', 'aabbabab', 'ab', 'abaaaaab', 'abaaaab', 'abaaab', 'abab', 'ababab', 'abbaabab', 'baab', 'baabab', 'babaaaab', 'bbaaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaabb', 'aabbaaab', 'aabbabab', 'abbaabab', 'baab', 'baabab', 'babaaaab', 'bbaaaab']
Neg Neg Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.7709838151931763
Train Epoch 1, Loss 0.6921798586845398
Train Epoch 2, Loss 0.6186911463737488
Accuracy at epoch 132: 0.9794921875, total training samples: 407
Early stopping at epoch 219. Loss did not improve for 10 epochs.
Generate examples Step 219, Loss 0.32423444430936466
Epoch: 133
Negative Examples
['aaababab', 'abaaabab', 'abaab', 'abab', 'b', 'baaab', 'bab', 'bbbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaaaabab', 'aaaab', 'aaaabab', 'aaab', 'aaabaaab', 'aaabaab', 'ab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaababab', 'abaaabab', 'abaab', 'abab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.7649016380310059
Train Epoch 1, Loss 0.6766691207885742
Train Epoch 2, Loss 0.6105638742446899
Accuracy at epoch 133: 0.8408203125, total training samples: 411
Early stopping at epoch 214. Loss did not improve for 10 epochs.
Generate examples Step 214, Loss 0.23375915982002435
Epoch: 134
Negative Examples
['aabbbbb', 'abbabb', 'abbbaaab', 'abbbb', 'b', 'baabbbab', 'babb', 'babbaab', 'babbb', 'babbbab', 'bb', 'bbaabab', 'bbaabb', 'bbaabbb', 'bbabab', 'bbabb', 'bbb', 'bbbaab', 'bbbabbab', 'bbbb', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bbab']
Pos
Counterexamples
['bbab']
Neg
Train Epoch 0, Loss 0.7009078860282898
Train Epoch 1, Loss 0.6180325150489807
Train Epoch 2, Loss 0.5404166579246521
Accuracy at epoch 134: 0.9306640625, total training samples: 412
Early stopping at epoch 191. Loss did not improve for 10 epochs.
Generate examples Step 191, Loss 0.36676353697354597
Epoch: 135
Negative Examples
['aaabb', 'abaaabbb', 'abaab', 'b', 'baaab']
Neg Neg Neg Neg Neg
Positive Examples
['aaaaaab', 'aaaaab', 'aaaaabab', 'aaaaabb', 'aaaab', 'aaab', 'aabaaaab', 'aabaaab', 'ab', 'abaaab', 'baaaaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaab', 'aaaaabb', 'baaaaab']
Pos Neg Neg
Train Epoch 0, Loss 0.7577941417694092
Train Epoch 1, Loss 0.7123374938964844
Train Epoch 2, Loss 0.6722095608711243
Accuracy at epoch 135: 0.984375, total training samples: 415
Early stopping at epoch 209. Loss did not improve for 10 epochs.
Generate examples Step 209, Loss 0.23876233029933203
Epoch: 136
Negative Examples
['aabb', 'abaabaab', 'abab', 'abababab', 'ababbab', 'abbbab', 'b', 'baaabb', 'baab', 'bab', 'babab', 'bb', 'bbaab', 'bbabab', 'bbbaaaab', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aabaab']
Pos
Counterexamples
['abaabaab', 'abab', 'abababab']
Pos Pos Pos
Train Epoch 0, Loss 0.7857802510261536
Train Epoch 1, Loss 0.7002297043800354
Train Epoch 2, Loss 0.6333065032958984
Accuracy at epoch 136: 0.79296875, total training samples: 418
Early stopping at epoch 211. Loss did not improve for 10 epochs.
Generate examples Step 211, Loss 0.2291141834883195
Epoch: 137
Negative Examples
['aabbabb', 'ababbb', 'b', 'bababbb', 'babbb', 'babbbb', 'bb', 'bbaa', 'bbaabbbb', 'bbabaa', 'bbabb', 'bbabbba', 'bbb', 'bbbaaaa', 'bbbb', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 137: No counterexamples found, skipped.
Early stopping at epoch 191. Loss did not improve for 10 epochs.
Generate examples Step 191, Loss 0.22686689284940562
Epoch: 138
Negative Examples
['a', 'aabbbaba', 'abbabb', 'abbbabb', 'abbbbabb', 'b', 'ba', 'baa', 'baabb', 'babaa', 'bababb', 'babb', 'babbaabb', 'babbba', 'bbababaa', 'bbabb', 'bbabbabb', 'bbabbbaa', 'bbb', 'bbba', 'bbbabb', 'bbbb', 'bbbba']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
[]

Counterexamples
[]

Round 138: No counterexamples found, skipped.
Early stopping at epoch 184. Loss did not improve for 10 epochs.
Generate examples Step 184, Loss 0.22627770127476873
Epoch: 139
Negative Examples
['abbabbb', 'b', 'baabbbb', 'bababaaa', 'babbb', 'babbbb', 'bb', 'bbaaaabb', 'bbabaaab', 'bbabbb', 'bbbaa', 'bbbaaa', 'bbbaaab', 'bbbb', 'bbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bab', 'bbbbaab']
Pos Pos
Counterexamples
['bab', 'bbbbaab']
Neg Neg
Train Epoch 0, Loss 0.7557588815689087
Train Epoch 1, Loss 0.6857914924621582
Train Epoch 2, Loss 0.6223154067993164
Accuracy at epoch 139: 0.953125, total training samples: 420
Early stopping at epoch 225. Loss did not improve for 10 epochs.
Generate examples Step 225, Loss 0.3013060479301267
Epoch: 140
Negative Examples
['aab', 'aabbb', 'ababab', 'b', 'baab', 'baabaab', 'baabab', 'baabbab', 'bab', 'babab', 'bababbab', 'bb', 'bbaaabab', 'bbaabab', 'bbab', 'bbbaaaab', 'bbbabaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aababaab', 'ab', 'abab', 'abbab', 'baaaabab']
Pos Pos Pos Pos Pos
Counterexamples
['aab', 'ababab', 'abbab', 'baaaabab']
Pos Pos Neg Neg
Train Epoch 0, Loss 0.7462530136108398
Train Epoch 1, Loss 0.7111539840698242
Train Epoch 2, Loss 0.6823182702064514
Accuracy at epoch 140: 0.982421875, total training samples: 424
Early stopping at epoch 212. Loss did not improve for 10 epochs.
Generate examples Step 212, Loss 0.30028517173489494
Epoch: 141
Negative Examples
['aaaaabb', 'aaaabbab', 'aabaab', 'aabab', 'abaaaab', 'abaab', 'b', 'baab', 'bab', 'babab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaab', 'aaab', 'aaabaab', 'aab', 'ab']
Pos Pos Pos Pos Pos
Counterexamples
['aabaab', 'aabab', 'abaaaab', 'abaab']
Pos Pos Pos Pos
Train Epoch 0, Loss 0.7329296469688416
Train Epoch 1, Loss 0.6494958400726318
Train Epoch 2, Loss 0.5829501748085022
Accuracy at epoch 141: 0.8720703125, total training samples: 428
Early stopping at epoch 182. Loss did not improve for 10 epochs.
Generate examples Step 182, Loss 0.23899462322394052
Epoch: 142
Negative Examples
['aaabbbbb', 'aabbbab', 'abbaabb', 'abbb', 'b', 'baaabb', 'baabbab', 'babab', 'bababaab', 'bababbbb', 'babbabbb', 'babbbbb', 'bb', 'bbaaaab', 'bbaab', 'bbab', 'bbb', 'bbbbabab', 'bbbbbb']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['bab']
Pos
Counterexamples
['bab']
Neg
Train Epoch 0, Loss 0.7028060555458069
Train Epoch 1, Loss 0.6397311687469482
Train Epoch 2, Loss 0.5786624550819397
Accuracy at epoch 142: 0.95703125, total training samples: 429
Early stopping at epoch 189. Loss did not improve for 10 epochs.
Generate examples Step 189, Loss 0.3399442495484101
Epoch: 143
Negative Examples
['b', 'baaab']
Neg Neg
Positive Examples
['aaaaaaab', 'aaaaab', 'aaaaabab', 'aaaaabb', 'aaaab', 'aaaabab', 'aaab', 'aaabab', 'aab', 'aabaaab', 'ab', 'abaaaaab', 'abaaaab', 'abaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['aaaaabb']
Neg
Train Epoch 0, Loss 0.8026741147041321
Train Epoch 1, Loss 0.664247453212738
Train Epoch 2, Loss 0.5535622239112854
Accuracy at epoch 143: 0.96875, total training samples: 430
Early stopping at epoch 246. Loss did not improve for 10 epochs.
Generate examples Step 246, Loss 0.3375439398925797
Epoch: 144
Negative Examples
['aaab', 'aaabaab', 'aab', 'abaaaab', 'abaab', 'b', 'baab', 'babaab', 'bbaaaab', 'bbbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaaaaab', 'aaaaab', 'aaaab', 'ab']
Pos Pos Pos Pos Pos
Counterexamples
['aaab', 'aaabaab', 'aab', 'abaaaab', 'abaab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7530568838119507
Train Epoch 1, Loss 0.6764297485351562
Train Epoch 2, Loss 0.6109434366226196
Accuracy at epoch 144: 0.9013671875, total training samples: 435
Early stopping at epoch 177. Loss did not improve for 10 epochs.
Generate examples Step 177, Loss 0.2681360405482603
Epoch: 145
Negative Examples
['aaabbbb', 'aabaabb', 'aabb', 'aabbabab', 'abaabb', 'abab', 'abb', 'abbaaaaa', 'b', 'baaa', 'baababb', 'bab', 'bababab', 'babb', 'bb', 'bbabab', 'bbbaa']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aab', 'abbab', 'bbaabaab']
Pos Pos Pos
Counterexamples
['abab', 'abbab', 'bbaabaab']
Pos Neg Neg
Train Epoch 0, Loss 0.7420610785484314
Train Epoch 1, Loss 0.689932107925415
Train Epoch 2, Loss 0.6477310657501221
Accuracy at epoch 145: 0.96875, total training samples: 438
Early stopping at epoch 193. Loss did not improve for 10 epochs.
Generate examples Step 193, Loss 0.2975614484130722
Epoch: 146
Negative Examples
['aaaabb', 'aaabb', 'aabb', 'abaabab', 'b', 'baaaaabb', 'bb', 'bbaaaab']
Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaabb', 'aaabab', 'aab', 'aabaaaab', 'ab', 'abaaaaab', 'abaaaab', 'abab', 'baaaab', 'baaab']
Pos Pos Pos Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abaabab', 'aaaaabb', 'baaaab', 'baaab']
Pos Neg Neg Neg
Train Epoch 0, Loss 0.7319903373718262
Train Epoch 1, Loss 0.6875517964363098
Train Epoch 2, Loss 0.646090030670166
Accuracy at epoch 146: 0.8896484375, total training samples: 442
Early stopping at epoch 202. Loss did not improve for 10 epochs.
Generate examples Step 202, Loss 0.2866064829779376
Epoch: 147
Negative Examples
['abbb', 'abbbab', 'b', 'baabbaab', 'bab', 'bababbab', 'bbaab', 'bbbaaab', 'bbbab', 'bbbb', 'bbbbbab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['ab', 'abbaabab', 'babaabab', 'babab', 'bbaaaab', 'bbaabab', 'bbab']
Pos Pos Pos Pos Pos Pos Pos
Counterexamples
['abbaabab', 'babaabab', 'babab', 'bbaaaab', 'bbaabab', 'bbab']
Neg Neg Neg Neg Neg Neg
Train Epoch 0, Loss 0.750819742679596
Train Epoch 1, Loss 0.6496492028236389
Train Epoch 2, Loss 0.5580467581748962
Accuracy at epoch 147: 0.935546875, total training samples: 448
Early stopping at epoch 159. Loss did not improve for 10 epochs.
Generate examples Step 159, Loss 0.3070321111008525
Epoch: 148
Negative Examples
['aaabbab', 'aab', 'aabaabb', 'aababab', 'abaab', 'abaabab', 'abab', 'b', 'baaab', 'baabab', 'babab', 'bababbaa', 'bb', 'bbbbbaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaabab', 'aaaabbab', 'aaabab', 'aabaaab', 'ab']
Pos Pos Pos Pos Pos
Counterexamples
['aab', 'aababab', 'abaab', 'abaabab', 'abab', 'aaaabbab']
Pos Pos Pos Pos Pos Neg
Train Epoch 0, Loss 0.7661139369010925
Train Epoch 1, Loss 0.7091495990753174
Train Epoch 2, Loss 0.6744718551635742
Accuracy at epoch 148: 0.9775390625, total training samples: 454
Early stopping at epoch 172. Loss did not improve for 10 epochs.
Generate examples Step 172, Loss 0.29007144522115674
Epoch: 149
Negative Examples
['aaabaaab', 'aab', 'aabaaaab', 'aabaaab', 'aabb', 'abaaabab', 'abbb', 'b', 'baaaaab', 'baaab', 'baab', 'baabaaab', 'bab', 'bababab', 'babbaaab', 'bb', 'bbaaaab', 'bbbabaa', 'bbbbaaab']
Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg Neg
Positive Examples
['aaaaaaab', 'aaab', 'ab', 'abab']
Pos Pos Pos Pos
Counterexamples
['aaabaaab', 'aab', 'aabaaaab', 'aabaaab', 'abaaabab']
Pos Pos Pos Pos Pos
Train Epoch 0, Loss 0.7431564927101135
Train Epoch 1, Loss 0.6677116751670837
Train Epoch 2, Loss 0.61025470495224
Accuracy at epoch 149: 0.8681640625, total training samples: 459
